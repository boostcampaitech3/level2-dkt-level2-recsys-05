{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "torch.set_printoptions(sci_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNMakeDataset():\n",
    "    def __init__(self, DATA_PATH):\n",
    "        self.preporcessing(DATA_PATH)\n",
    "        self.oof_user_set = self.split_data()\n",
    "    \n",
    "    def get_oof_data(self, oof):\n",
    "        val_user_list = self.oof_user_set[oof]\n",
    "        group_df = self.all_df.groupby('userID')\n",
    "        \n",
    "        train = []\n",
    "        valid = []\n",
    "\n",
    "        for userID, df in group_df:\n",
    "            if userID in val_user_list:\n",
    "                trn_df = df.iloc[:-1, :]\n",
    "                val_df = df.iloc[-1:, :]\n",
    "                train.append(trn_df)\n",
    "                valid.append(val_df)\n",
    "            else:\n",
    "                train.append(df)\n",
    "\n",
    "        train = pd.concat(train).reset_index(drop = True)\n",
    "        valid = pd.concat(valid).reset_index(drop = True)\n",
    "        \n",
    "        return train, valid\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        return self.test_df\n",
    "\n",
    "    def split_data(self):\n",
    "        user_list = self.all_df['userID'].unique().tolist()\n",
    "        oof_user_set = {}\n",
    "        kf = KFold(n_splits = 5, random_state = 22, shuffle = True)\n",
    "        for idx, (train_user, valid_user) in enumerate(kf.split(user_list)):\n",
    "            oof_user_set[idx] = valid_user.tolist()\n",
    "        \n",
    "        return oof_user_set\n",
    "\n",
    "    def preporcessing(self, DATA_PATH):\n",
    "        dtype = {\n",
    "                'userID': 'int16',\n",
    "                'answerCode': 'int8',\n",
    "                'KnowledgeTag': 'int16'\n",
    "        }\n",
    "            \n",
    "        train_df = pd.read_csv(os.path.join(DATA_PATH, 'train_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "        train_df = train_df.sort_values(by=['userID', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "        test_df = pd.read_csv(os.path.join(DATA_PATH, 'test_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "        test_df = test_df.sort_values(by=['userID', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "        all_df = pd.concat([train_df, test_df]).sort_values(by=['userID', 'Timestamp']).reset_index(drop=True)\n",
    "        all_df['userID-assessmentItemID'] = all_df['userID'].astype(str) + '-' + all_df['assessmentItemID'].astype(str)\n",
    "        all_df = all_df[~(all_df.duplicated('userID-assessmentItemID', keep='last'))].reset_index(drop=True)\n",
    "        \n",
    "        assessmentItemID2idx = {}\n",
    "        idx2assessmentItemID = {}\n",
    "\n",
    "        for idx, assessmentItemID in enumerate(all_df['assessmentItemID'].unique().tolist()):\n",
    "            assessmentItemID2idx[assessmentItemID] = idx\n",
    "            idx2assessmentItemID[idx] = assessmentItemID\n",
    "        \n",
    "        all_df['assessmentItemID2idx'] = all_df['assessmentItemID'].apply(lambda x : assessmentItemID2idx[x])\n",
    "\n",
    "        self.num_user, self.num_item = all_df['userID'].nunique(), len(assessmentItemID2idx)\n",
    "        self.assessmentItemID2idx, self.idx2assessmentItemID = assessmentItemID2idx, idx2assessmentItemID\n",
    "        self.adj_mat = self.generate_adj_matrix(self.generate_dok_matrix(all_df))\n",
    "        self.all_df = all_df[all_df['answerCode'] != -1].reset_index(drop=True)\n",
    "        self.test_df = all_df[all_df['answerCode'] == -1].reset_index(drop=True)\n",
    "\n",
    "    def generate_dok_matrix(self, df):\n",
    "        R = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
    "        group_df = df.groupby('userID')\n",
    "        for userID, g_df in group_df:\n",
    "            items = g_df['assessmentItemID2idx'].tolist()\n",
    "            R[userID, items] = 1.0\n",
    "        \n",
    "        return R\n",
    "    \n",
    "    def generate_adj_matrix(self, R):\n",
    "        adj_mat = sp.dok_matrix((self.num_user + self.num_item, self.num_user + self.num_item), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil() # to_list\n",
    "        R = R.tolil()\n",
    "\n",
    "        adj_mat[:self.num_user, self.num_user:] = R\n",
    "        adj_mat[self.num_user:, :self.num_user] = R.T\n",
    "        adj_mat = adj_mat.todok() # to_dok_matrix\n",
    "\n",
    "        def normalized_adj_single(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "            d_inv = np.power(rowsum, -.5).flatten()  \n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "            norm_adj = d_mat_inv.dot(adj).dot(d_mat_inv)\n",
    "\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        adj_mat = normalized_adj_single(adj_mat)\n",
    "        return adj_mat.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNCustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.users = df['userID'].tolist()\n",
    "        self.items = df['assessmentItemID2idx'].tolist()\n",
    "        self.targets = df['answerCode'].tolist()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.users[idx]\n",
    "        item = self.items[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        return {\n",
    "            'user' : user, \n",
    "            'item' : item, \n",
    "            'target' : target,\n",
    "            }\n",
    "\n",
    "def make_collate_fn(samples):\n",
    "    \n",
    "    users = []\n",
    "    items = []\n",
    "    targets = []\n",
    "\n",
    "    for sample in samples:\n",
    "        users += [sample['user']]\n",
    "        items += [sample['item']]\n",
    "        targets += [sample['target']]\n",
    "\n",
    "    return {\n",
    "        'users' : torch.tensor(users, dtype = torch.long),\n",
    "        'items' : torch.tensor(items, dtype = torch.long),\n",
    "        'targets' : torch.tensor(targets, dtype = torch.float32),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim, n_layers, node_dropout, adj_mtx):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize Class attributes\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.graph = self._convert_sp_mat_to_sp_tensor(adj_mtx)\n",
    "        self.n_layers = n_layers\n",
    "        self.node_dropout = node_dropout\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weight_dict = self._init_weights()\n",
    "        print(\"Weights initialized.\")\n",
    "\n",
    "    # initialize weights\n",
    "    def _init_weights(self):\n",
    "        print(\"Initializing weights...\")\n",
    "        weight_dict = nn.ParameterDict()\n",
    "\n",
    "        initializer = torch.nn.init.xavier_uniform_\n",
    "        \n",
    "        weight_dict['user_embedding'] = nn.Parameter(initializer(torch.empty(self.n_users, self.emb_dim).to(self.device)))\n",
    "        weight_dict['item_embedding'] = nn.Parameter(initializer(torch.empty(self.n_items, self.emb_dim).to(self.device)))\n",
    "           \n",
    "        return weight_dict\n",
    "\n",
    "    # convert sparse matrix into sparse PyTorch tensor\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        \"\"\"\n",
    "        Convert scipy sparse matrix to PyTorch sparse matrix\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        X = Adjacency matrix, scipy sparse matrix\n",
    "        \"\"\"\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        i = torch.LongTensor(np.mat([coo.row, coo.col]))\n",
    "        v = torch.FloatTensor(coo.data)\n",
    "        res = torch.sparse.FloatTensor(i, v, coo.shape).to(self.device)\n",
    "        return res\n",
    "\n",
    "    # apply node_dropout\n",
    "    def _droupout_sparse(self, X):\n",
    "        \"\"\"\n",
    "        Drop individual locations in X\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        X = adjacency matrix (PyTorch sparse tensor)\n",
    "        dropout = fraction of nodes to drop\n",
    "        noise_shape = number of non non-zero entries of X\n",
    "        \"\"\"\n",
    "        node_dropout_mask = ((self.node_dropout) + torch.rand(X._nnz())).floor().bool().to(self.device)\n",
    "        i = X.coalesce().indices()\n",
    "        v = X.coalesce()._values()\n",
    "        i[:,node_dropout_mask] = 0\n",
    "        v[node_dropout_mask] = 0\n",
    "        X_dropout = torch.sparse.FloatTensor(i, v, X.shape).to(X.device)\n",
    "\n",
    "        return  X_dropout.mul(1/(1-self.node_dropout))\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Computes the forward pass\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        u = user\n",
    "        i = item (user interacted with item)\n",
    "        \"\"\"\n",
    "        # apply drop-out mask\n",
    "        graph = self._droupout_sparse(self.graph) if self.node_dropout > 0 else self.graph\n",
    "        ego_embeddings = torch.cat([self.weight_dict['user_embedding'], self.weight_dict['item_embedding']], 0)\n",
    "        final_embeddings = [ego_embeddings]\n",
    "\n",
    "        for k in range(self.n_layers):\n",
    "            ego_embeddings = torch.sparse.mm(graph, final_embeddings[k])\n",
    "            final_embeddings.append(ego_embeddings)                                       \n",
    "\n",
    "        final_embeddings = torch.stack(final_embeddings, dim=1)\n",
    "        final_embeddings = torch.mean(final_embeddings, dim=1)\n",
    "        \n",
    "        u_final_embeddings, i_final_embeddings = final_embeddings.split([self.n_users, self.n_items], 0)\n",
    "\n",
    "        self.u_final_embeddings = nn.Parameter(u_final_embeddings)\n",
    "        self.i_final_embeddings = nn.Parameter(i_final_embeddings)\n",
    "        \n",
    "        u_emb = u_final_embeddings[input['users']] # user embeddings\n",
    "        i_emb = i_final_embeddings[input['items']] # item embeddings\n",
    "        \n",
    "        output = torch.sum(torch.mul(u_emb, i_emb), dim = 1).sigmoid()\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def predict(self, input):\n",
    "        u_emb = self.u_final_embeddings[input['users']]\n",
    "        i_emb = self.i_final_embeddings[input['items']]\n",
    "\n",
    "        output = torch.sum(torch.mul(u_emb, i_emb), dim = 1).sigmoid()\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, data_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "\n",
    "    target = []\n",
    "    pred = []\n",
    "\n",
    "    for batch, input in enumerate(data_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input)\n",
    "        loss = criterion(output, input['targets'].to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val += loss.item()\n",
    "\n",
    "        target.extend(input['targets'].cpu().numpy().tolist())\n",
    "        pred.extend(output.detach().cpu().numpy().tolist())\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f'{batch + 1}/{len(data_loader)} Loss : {loss_val / (batch + 1):.5f} Roc-Auc: {roc_auc_score(target, pred):.5f}')\n",
    "\n",
    "    loss_val /= len(data_loader)\n",
    "    roc_auc = roc_auc_score(target, pred)\n",
    "\n",
    "    return loss_val, roc_auc\n",
    "\n",
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    loss_val = 0\n",
    "\n",
    "    target = []\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input in data_loader:\n",
    "            \n",
    "            output = model.predict(input)\n",
    "            \n",
    "            loss = criterion(output, input['targets'].to(device))\n",
    "            loss_val += loss.item()\n",
    "\n",
    "            target.extend(input['targets'].cpu().numpy().tolist())\n",
    "            pred.extend(output.cpu().numpy().tolist())\n",
    "\n",
    "    loss_val /= len(data_loader)\n",
    "    roc_auc = roc_auc_score(target, pred)\n",
    "\n",
    "    return loss_val, roc_auc\n",
    "\n",
    "\n",
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input in data_loader:\n",
    "            output = model.predict(input)\n",
    "            pred.extend(output.cpu().numpy().tolist())\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "epochs = 5\n",
    "lr = 0.001\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "emb_dim = 64\n",
    "n_layers = 2\n",
    "node_dropout = 0.2\n",
    "\n",
    "num_workers = 8\n",
    "\n",
    "DATA_PATH = '/opt/ml/input/data'\n",
    "MODEL_PATH = '/opt/ml/model'\n",
    "SUBMISSION_PATH = '/opt/ml/submission'\n",
    "\n",
    "model_name = 'LightGCN'\n",
    "submission_name = 'LightGCN.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(MODEL_PATH):\n",
    "    os.mkdir(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(SUBMISSION_PATH):\n",
    "    os.mkdir(SUBMISSION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dataset = GNNMakeDataset(DATA_PATH = DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = 0\n",
    "\n",
    "train_df, valid_df = make_dataset.get_oof_data(oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing weights...\n",
      "Weights initialized.\n"
     ]
    }
   ],
   "source": [
    "seed_everything(22 + oof)\n",
    "\n",
    "train_dataset = GNNCustomDataset(df = train_df)\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = batch_size, \n",
    "    shuffle = True, \n",
    "    drop_last = False,\n",
    "    collate_fn = make_collate_fn,\n",
    "    num_workers = num_workers)\n",
    "\n",
    "\n",
    "valid_dataset = GNNCustomDataset(df = valid_df)\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False, \n",
    "    drop_last = False,\n",
    "    collate_fn = make_collate_fn,\n",
    "    num_workers = num_workers)\n",
    "\n",
    "model = LightGCN(\n",
    "    n_users = make_dataset.num_user,\n",
    "    n_items = make_dataset.num_item,\n",
    "    emb_dim = emb_dim,\n",
    "    n_layers = n_layers,\n",
    "    node_dropout = node_dropout,\n",
    "    adj_mtx = make_dataset.adj_mat,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/495 Loss : 0.69315 Roc-Auc: 0.49050\n",
      "101/495 Loss : 0.68088 Roc-Auc: 0.53704\n",
      "201/495 Loss : 0.64748 Roc-Auc: 0.56541\n",
      "301/495 Loss : 0.62851 Roc-Auc: 0.60278\n",
      "401/495 Loss : 0.61600 Roc-Auc: 0.63276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.60748| Train Roc-Auc: 0.65418| Valid loss: 0.66664| Valid Roc-Auc: 0.72425|: 100%|██████████| 1/1 [13:01<00:00, 781.11s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/495 Loss : 0.55804 Roc-Auc: 0.77182\n",
      "101/495 Loss : 0.56215 Roc-Auc: 0.76582\n",
      "201/495 Loss : 0.55972 Roc-Auc: 0.76757\n",
      "301/495 Loss : 0.55787 Roc-Auc: 0.76906\n",
      "401/495 Loss : 0.55587 Roc-Auc: 0.77051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   2| Train loss: 0.55443| Train Roc-Auc: 0.77150| Valid loss: 0.64266| Valid Roc-Auc: 0.73710|: 100%|██████████| 1/1 [12:59<00:00, 779.34s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/495 Loss : 0.53916 Roc-Auc: 0.78395\n",
      "101/495 Loss : 0.54373 Roc-Auc: 0.78050\n",
      "201/495 Loss : 0.54325 Roc-Auc: 0.77990\n",
      "301/495 Loss : 0.54228 Roc-Auc: 0.78048\n",
      "401/495 Loss : 0.54207 Roc-Auc: 0.78028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   3| Train loss: 0.54151| Train Roc-Auc: 0.78032| Valid loss: 0.63223| Valid Roc-Auc: 0.73704|: 100%|██████████| 1/1 [12:59<00:00, 779.78s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/495 Loss : 0.54683 Roc-Auc: 0.78058\n",
      "101/495 Loss : 0.53670 Roc-Auc: 0.78430\n",
      "201/495 Loss : 0.53644 Roc-Auc: 0.78403\n",
      "301/495 Loss : 0.53616 Roc-Auc: 0.78405\n",
      "401/495 Loss : 0.53570 Roc-Auc: 0.78427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   4| Train loss: 0.53533| Train Roc-Auc: 0.78449| Valid loss: 0.62659| Valid Roc-Auc: 0.73761|: 100%|██████████| 1/1 [13:02<00:00, 782.92s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/495 Loss : 0.53251 Roc-Auc: 0.79488\n",
      "101/495 Loss : 0.53144 Roc-Auc: 0.78847\n",
      "201/495 Loss : 0.53137 Roc-Auc: 0.78859\n",
      "301/495 Loss : 0.53119 Roc-Auc: 0.78864\n",
      "401/495 Loss : 0.53067 Roc-Auc: 0.78911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   5| Train loss: 0.53049| Train Roc-Auc: 0.78923| Valid loss: 0.62267| Valid Roc-Auc: 0.73849|: 100%|██████████| 1/1 [12:58<00:00, 778.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST OOF-0| Epoch:   5| Train loss: 0.53049| Train Roc-Auc: 0.78923| Valid loss: 0.62267| Valid Roc-Auc: 0.73849|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_epoch = 0\n",
    "best_train_loss = 0\n",
    "best_train_roc_auc = 0\n",
    "best_valid_loss = 0\n",
    "best_valid_roc_auc = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    tbar = tqdm(range(1))\n",
    "    for _ in tbar:\n",
    "        train_loss, train_roc_auc = train(model = model, data_loader = train_data_loader, criterion = criterion, optimizer = optimizer)\n",
    "        valid_loss, valid_roc_auc = evaluate(model = model, data_loader = valid_data_loader, criterion = criterion)\n",
    "        if best_valid_roc_auc < valid_roc_auc:\n",
    "            best_epoch = epoch\n",
    "            best_train_loss = train_loss\n",
    "            best_train_roc_auc = train_roc_auc\n",
    "            best_valid_loss = valid_loss\n",
    "            best_valid_roc_auc = valid_roc_auc\n",
    "            torch.save(model.state_dict(), os.path.join(MODEL_PATH, f'oof_{oof}_' + model_name + '.pt'))\n",
    "\n",
    "        tbar.set_description(f'OOF-{oof}| Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| Train Roc-Auc: {train_roc_auc:.5f}| Valid loss: {valid_loss:.5f}| Valid Roc-Auc: {valid_roc_auc:.5f}|')\n",
    "\n",
    "print(f'BEST OOF-{oof}| Epoch: {best_epoch:3d}| Train loss: {best_train_loss:.5f}| Train Roc-Auc: {best_train_roc_auc:.5f}| Valid loss: {best_valid_loss:.5f}| Valid Roc-Auc: {best_valid_roc_auc:.5f}|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
