{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "torch.set_printoptions(sci_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDataset():\n",
    "\n",
    "    def __init__(self, DATA_PATH):\n",
    "        self.preporcessing(DATA_PATH)\n",
    "        self.oof_user_set = self.split_data()\n",
    "    \n",
    "    def split_data(self):\n",
    "        user_list = self.all_df['userID'].unique().tolist()\n",
    "        oof_user_set = {}\n",
    "        kf = KFold(n_splits = 5, random_state = 22, shuffle = True)\n",
    "        for idx, (train_user, valid_user) in enumerate(kf.split(user_list)):\n",
    "            oof_user_set[idx] = valid_user.tolist()\n",
    "        \n",
    "        return oof_user_set\n",
    "\n",
    "    def preporcessing(self, DATA_PATH):\n",
    "\n",
    "        dtype = {\n",
    "            'userID': 'int16',\n",
    "            'answerCode': 'int8',\n",
    "            'KnowledgeTag': 'int16'\n",
    "        }\n",
    "        \n",
    "        train_df = pd.read_csv(os.path.join(DATA_PATH, 'train_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "        train_df = train_df.sort_values(by=['userID', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "        test_df = pd.read_csv(os.path.join(DATA_PATH, 'test_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "        test_df = test_df.sort_values(by=['userID', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "        def get_paper_number(x):\n",
    "            return x[1:-3]\n",
    "\n",
    "        def get_paper_question_number(x):\n",
    "            return x[-3:]\n",
    "\n",
    "        def get_large_paper_number(x):\n",
    "            return x[1:4]\n",
    "        \n",
    "        train_df['paper_number'] = train_df['assessmentItemID'].apply(lambda x : get_paper_number(x))\n",
    "        train_df['paper_question_number'] = train_df['assessmentItemID'].apply(lambda x : get_paper_question_number(x))\n",
    "        train_df['large_paper_number'] = train_df['assessmentItemID'].apply(lambda x : get_large_paper_number(x))\n",
    "\n",
    "        test_df['paper_number'] = test_df['assessmentItemID'].apply(lambda x : get_paper_number(x))\n",
    "        test_df['paper_question_number'] = test_df['assessmentItemID'].apply(lambda x : get_paper_question_number(x))\n",
    "        test_df['large_paper_number'] = test_df['assessmentItemID'].apply(lambda x : get_large_paper_number(x))\n",
    "\n",
    "        # 문제 푸는데 걸린 시간\n",
    "        def get_now_elapsed(df):\n",
    "            \n",
    "            diff = df.loc[:, ['userID','Timestamp']].groupby('userID').diff().fillna(pd.Timedelta(seconds=0))\n",
    "            diff = diff.fillna(pd.Timedelta(seconds=0))\n",
    "            diff = diff['Timestamp'].apply(lambda x: x.total_seconds())\n",
    "            df['now_elapsed'] = diff\n",
    "            df['now_elapsed'] = df['now_elapsed'].apply(lambda x : x if x < 650 and x >=0 else 0)\n",
    "            df['now_elapsed'] = df['now_elapsed'] / 650\n",
    "\n",
    "            return df\n",
    "\n",
    "        train_df = get_now_elapsed(df = train_df)\n",
    "        test_df = get_now_elapsed(df = test_df)\n",
    "\n",
    "        all_df = pd.concat([train_df, test_df])\n",
    "        all_df = all_df[all_df['answerCode'] != -1].reset_index(drop = True)\n",
    "\n",
    "        # 문항별 정답률\n",
    "        train_df = train_df.set_index('assessmentItemID')\n",
    "        train_df['assessmentItemID_mean_answerCode'] = all_df.groupby('assessmentItemID').mean()['answerCode']\n",
    "        train_df = train_df.reset_index(drop = False)\n",
    "\n",
    "        test_df = test_df.set_index('assessmentItemID')\n",
    "        test_df['assessmentItemID_mean_answerCode'] = all_df.groupby('assessmentItemID').mean()['answerCode']\n",
    "        test_df = test_df.reset_index(drop = False)\n",
    "\n",
    "        # 시험지별 정답률\n",
    "        train_df = train_df.set_index('testId')\n",
    "        train_df['testId_mean_answerCode'] = all_df.groupby('testId').mean()['answerCode']\n",
    "        train_df = train_df.reset_index(drop = False)\n",
    "\n",
    "        test_df = test_df.set_index('testId')\n",
    "        test_df['testId_mean_answerCode'] = all_df.groupby('testId').mean()['answerCode']\n",
    "        test_df = test_df.reset_index(drop = False)\n",
    "\n",
    "        # 태그별 정답률 \n",
    "        train_df = train_df.set_index('KnowledgeTag')\n",
    "        train_df['KnowledgeTag_mean_answerCode'] = all_df.groupby('KnowledgeTag').mean()['answerCode']\n",
    "        train_df = train_df.reset_index(drop = False)\n",
    "\n",
    "        test_df = test_df.set_index('KnowledgeTag')\n",
    "        test_df['KnowledgeTag_mean_answerCode'] = all_df.groupby('KnowledgeTag').mean()['answerCode']\n",
    "        test_df = test_df.reset_index(drop = False)\n",
    "\n",
    "        # 문제 푼 시간\n",
    "        train_df['hour'] = train_df['Timestamp'].dt.hour\n",
    "        test_df['hour'] = test_df['Timestamp'].dt.hour\n",
    "\n",
    "        # 문제 푼 요일\n",
    "        train_df['dayofweek'] = train_df['Timestamp'].dt.dayofweek\n",
    "        test_df['dayofweek'] = test_df['Timestamp'].dt.dayofweek\n",
    "\n",
    "        # index 로 변환\n",
    "\n",
    "        def get_val2idx(val_list : list) -> dict:\n",
    "            val2idx = {}\n",
    "            for idx, val in enumerate(val_list):\n",
    "                val2idx[val] = idx\n",
    "            \n",
    "            return val2idx\n",
    "\n",
    "        assessmentItemID2idx = get_val2idx(all_df['assessmentItemID'].unique().tolist())\n",
    "        testId2idx = get_val2idx(all_df['testId'].unique().tolist())\n",
    "        KnowledgeTag2idx = get_val2idx(all_df['KnowledgeTag'].unique().tolist())\n",
    "        large_paper_number2idx = get_val2idx(all_df['large_paper_number'].unique().tolist())\n",
    "\n",
    "        train_df['assessmentItemID2idx'] = train_df['assessmentItemID'].apply(lambda x : assessmentItemID2idx[x])\n",
    "        train_df['testId2idx'] = train_df['testId'].apply(lambda x : testId2idx[x])\n",
    "        train_df['KnowledgeTag2idx'] = train_df['KnowledgeTag'].apply(lambda x : KnowledgeTag2idx[x])\n",
    "        train_df['large_paper_number2idx'] = train_df['large_paper_number'].apply(lambda x : large_paper_number2idx[x])\n",
    "\n",
    "        test_df['assessmentItemID2idx'] = test_df['assessmentItemID'].apply(lambda x : assessmentItemID2idx[x])\n",
    "        test_df['testId2idx'] = test_df['testId'].apply(lambda x : testId2idx[x])\n",
    "        test_df['KnowledgeTag2idx'] = test_df['KnowledgeTag'].apply(lambda x : KnowledgeTag2idx[x])\n",
    "        test_df['large_paper_number2idx'] = test_df['large_paper_number'].apply(lambda x : large_paper_number2idx[x])\n",
    "\n",
    "        # 최신 문제 풀이 시간 - 각 문제 풀이 시간\n",
    "        def get_relevance_elapsed(df):\n",
    "            return_df = []\n",
    "            df = df.copy()\n",
    "            group_df = df.groupby('userID')\n",
    "\n",
    "            for userID, g_df in group_df:\n",
    "                last_df = g_df.iloc[-1:, :]\n",
    "                g_df['relevance_elapsed'] = last_df['Timestamp'].values[0] - g_df['Timestamp']\n",
    "                g_df['relevance_elapsed'] = g_df['relevance_elapsed'].dt.days\n",
    "                return_df.append(g_df)\n",
    "\n",
    "            return_df = pd.concat(return_df).sort_values(by=['userID', 'Timestamp']).reset_index(drop=True)\n",
    "            return return_df\n",
    "            \n",
    "        train_df = get_relevance_elapsed(df = train_df)\n",
    "        test_df = get_relevance_elapsed(df = test_df)\n",
    "\n",
    "        self.train_df, self.test_df = train_df, test_df\n",
    "        self.all_df = pd.concat([train_df, test_df[test_df['answerCode'] != -1]]).reset_index(drop=True)\n",
    "        self.num_userID = all_df['userID'].nunique()\n",
    "        self.num_assessmentItemID = len(assessmentItemID2idx)\n",
    "        self.num_testId = len(testId2idx)\n",
    "        self.num_KnowledgeTag = len(KnowledgeTag2idx)\n",
    "        self.num_large_paper_number = len(large_paper_number2idx)\n",
    "        self.num_hour = 24\n",
    "        self.num_dayofweek = 7\n",
    "        self.num_relevance_elapsed = self.all_df['relevance_elapsed'].max() + 1\n",
    "\n",
    "    def get_oof_data(self, oof):\n",
    "\n",
    "        val_user_list = self.oof_user_set[oof]\n",
    "\n",
    "        train = []\n",
    "        valid = []\n",
    "\n",
    "        group_df = self.all_df.groupby('userID')\n",
    "\n",
    "        for userID, df in group_df:\n",
    "            if userID in val_user_list:\n",
    "                trn_df = df.iloc[:-1, :]\n",
    "                val_df = df.copy()\n",
    "                train.append(trn_df)\n",
    "                valid.append(val_df)\n",
    "            else:\n",
    "                train.append(df)\n",
    "\n",
    "        train = pd.concat(train)\n",
    "        valid = pd.concat(valid)\n",
    "        \n",
    "        return train, valid\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        return self.test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        df,\n",
    "        cat_cols = ['userID', 'assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx', 'hour', 'dayofweek', 'relevance_elapsed'],\n",
    "        num_cols = ['now_elapsed', 'assessmentItemID_mean_answerCode', 'testId_mean_answerCode', 'KnowledgeTag_mean_answerCode'],\n",
    "        test = False,):\n",
    "\n",
    "        self.cat_cols = cat_cols\n",
    "        self.num_cols = num_cols\n",
    "        self.user_list = df['userID'].unique().tolist()\n",
    "        self.get_df = df.groupby('userID')\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.user_list[idx]\n",
    "        get_df = self.get_df.get_group(user)\n",
    "\n",
    "        now_df = get_df.iloc[1:, :]\n",
    "        now_cat_feature = now_df[self.cat_cols].values\n",
    "        now_num_feature = now_df[self.num_cols].values\n",
    "        now_answerCode = now_df['answerCode'].values\n",
    "\n",
    "        past_df = get_df.iloc[:-1, :]\n",
    "        past_cat_feature = past_df[self.cat_cols].values\n",
    "        past_num_feature = past_df[self.num_cols].values\n",
    "        past_answerCode = past_df['answerCode'].values\n",
    "\n",
    "        return {\n",
    "            'past_cat_feature' : past_cat_feature, \n",
    "            'past_num_feature' : past_num_feature, \n",
    "            'past_answerCode' : past_answerCode, \n",
    "            'now_cat_feature' : now_cat_feature, \n",
    "            'now_num_feature' : now_num_feature, \n",
    "            'now_answerCode' : now_answerCode\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(seq, max_len, padding_value = 0):\n",
    "    try:\n",
    "        seq_len, col = seq.shape\n",
    "        padding = np.zeros((max_len - seq_len, col)) + padding_value\n",
    "    except:\n",
    "        seq_len = seq.shape[0]\n",
    "        padding = np.zeros((max_len - seq_len, )) + padding_value\n",
    "\n",
    "    padding_seq = np.concatenate([padding, seq])\n",
    "\n",
    "    return padding_seq\n",
    "\n",
    "def train_make_batch(samples):\n",
    "    max_len = 0\n",
    "    for sample in samples:\n",
    "        seq_len, col = sample['past_cat_feature'].shape\n",
    "        if max_len < seq_len:\n",
    "            max_len = seq_len\n",
    "    \n",
    "    past_cat_feature = []\n",
    "    past_num_feature = []\n",
    "    past_answerCode = []\n",
    "    now_cat_feature = []\n",
    "    now_num_feature = []\n",
    "    now_answerCode = []\n",
    "\n",
    "    for sample in samples:\n",
    "        past_cat_feature += [pad_sequence(sample['past_cat_feature'] + 1, max_len = max_len, padding_value = 0)]\n",
    "        past_num_feature += [pad_sequence(sample['past_num_feature'], max_len = max_len, padding_value = 0)]\n",
    "        past_answerCode += [pad_sequence(sample['past_answerCode'] + 1, max_len = max_len, padding_value = 0)]\n",
    "        now_cat_feature += [pad_sequence(sample['now_cat_feature'] + 1, max_len = max_len, padding_value = 0)]\n",
    "        now_num_feature += [pad_sequence(sample['now_num_feature'], max_len = max_len, padding_value = 0)]\n",
    "        now_answerCode += [pad_sequence(sample['now_answerCode'], max_len = max_len, padding_value = -1)]\n",
    "\n",
    "    return torch.tensor(past_cat_feature, dtype = torch.long), torch.tensor(past_num_feature, dtype = torch.float32), torch.tensor(past_answerCode, dtype = torch.long), torch.tensor(now_cat_feature, dtype = torch.long), torch.tensor(now_num_feature, dtype = torch.float32), torch.tensor(now_answerCode, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        \"\"\"\n",
    "        Q, K, V : (batch_size, num_heads, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \"\"\"\n",
    "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units) # (batch_size, num_heads, max_len, max_len)\n",
    "        attn_score = attn_score.masked_fill(mask == 0, -1e9)  # 유사도가 0인 지점은 -infinity로 보내 softmax 결과가 0이 되도록 함\n",
    "        attn_dist = self.dropout(F.softmax(attn_score, dim=-1))  # attention distribution\n",
    "        output = torch.matmul(attn_dist, V)  # (batch_size, num_heads, max_len, hidden_units) / # dim of output : batchSize x num_head x seqLen x hidden_units\n",
    "        return output, attn_dist\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads # head의 수\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        # query, key, value, output 생성을 위해 Linear 모델 생성\n",
    "        self.W_Q = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
    "        self.W_K = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
    "        self.W_V = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
    "        self.W_O = nn.Linear(hidden_units * num_heads, hidden_units, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(hidden_units, dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, enc, mask):\n",
    "        \"\"\"\n",
    "        enc : (batch_size, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \n",
    "        \"\"\"\n",
    "        residual = enc # residual connection을 위해 residual 부분을 저장\n",
    "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
    "\n",
    "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
    "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units) # (batch_size, max_len, num_heads, hidden_units)\n",
    "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units) # (batch_size, max_len, num_heads, hidden_units)\n",
    "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units) # (batch_size, max_len, num_heads, hidden_units)\n",
    "\n",
    "        # Head별로 각기 다른 attention이 가능하도록 Transpose 후 각각 attention에 통과시킴\n",
    "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2) # (batch_size, num_heads, max_len, hidden_units)\n",
    "        output, attn_dist = self.attention(Q, K, V, mask) # output : (batch_size, num_heads, max_len, hidden_units) / attn_dist : (batch_size, num_heads, max_len, max_len)\n",
    "\n",
    "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n",
    "        output = output.transpose(1, 2).contiguous() # (batch_size, max_len, num_heads, hidden_units) / contiguous() : 가변적 메모리 할당\n",
    "        output = output.view(batch_size, seqlen, -1) # (batch_size, max_len, hidden_units * num_heads)\n",
    "\n",
    "        # Linear Projection, Dropout, Residual sum, and Layer Normalization\n",
    "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual) # (batch_size, max_len, hidden_units)\n",
    "        return output, attn_dist\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "\n",
    "        self.W_1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.W_2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = self.W_2(F.relu(self.dropout(self.W_1(x))))\n",
    "        output = self.layerNorm(self.dropout(output) + residual)\n",
    "        return output\n",
    "\n",
    "\n",
    "class SASRecBlock(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(SASRecBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads, hidden_units, dropout_rate)\n",
    "        self.pointwise_feedforward = PositionwiseFeedForward(hidden_units, dropout_rate)\n",
    "\n",
    "    def forward(self, input_enc, mask):\n",
    "        \"\"\"\n",
    "        input_enc : (batch_size, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \"\"\"\n",
    "        output_enc, attn_dist = self.attention(input_enc, mask)\n",
    "        output_enc = self.pointwise_feedforward(output_enc)\n",
    "        return output_enc, attn_dist\n",
    "\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_userID, \n",
    "        num_assessmentItemID, \n",
    "        num_testId,\n",
    "        num_KnowledgeTag,\n",
    "        num_large_paper_number,\n",
    "        num_hour,\n",
    "        num_dayofweek,\n",
    "        num_relevance_elapsed,\n",
    "        num_cols,\n",
    "        cat_cols,\n",
    "        emb_size, \n",
    "        hidden_units, \n",
    "        num_heads, \n",
    "        num_layers, \n",
    "        dropout_rate, \n",
    "        device):\n",
    "        super(SASRec, self).__init__()\n",
    "\n",
    "        self.userID_emb = nn.Embedding(num_userID + 1, emb_size, padding_idx = 0) # 유저에 대한 정보\n",
    "        self.assessmentItemID_emb = nn.Embedding(num_assessmentItemID + 1, emb_size, padding_idx = 0) # 문항에 대한 정보\n",
    "        self.testId_emb = nn.Embedding(num_testId + 1, emb_size, padding_idx = 0) # 시험지에 대한 정보\n",
    "        self.KnowledgeTag_emb = nn.Embedding(num_KnowledgeTag + 1, emb_size, padding_idx = 0) # 지식 태그에 대한 정보\n",
    "        self.large_paper_number_emb = nn.Embedding(num_large_paper_number + 1, emb_size, padding_idx = 0) # 핫년에 대한 정보\n",
    "        self.hour_emb = nn.Embedding(num_hour + 1, emb_size, padding_idx = 0) # 문제 풀이 시간에 대한 정보\n",
    "        self.dayofweek_emb = nn.Embedding(num_dayofweek + 1, emb_size, padding_idx = 0) # 문제 풀이 요일에 대항 정보\n",
    "\n",
    "        self.relevance_elapsed_emb = nn.Embedding(num_relevance_elapsed + 1, hidden_units, padding_idx = 0) # 문제를 푼 시기에 대한 정보\n",
    "        self.answerCode_emb = nn.Embedding(3, hidden_units, padding_idx = 0) # 문제 정답 여부에 대한 정보\n",
    "\n",
    "        self.cat_emb = nn.Sequential(\n",
    "            nn.Linear((len(cat_cols) - 1) * emb_size, hidden_units // 2),\n",
    "            nn.LayerNorm(hidden_units // 2, eps=1e-6)\n",
    "        )\n",
    "\n",
    "        self.num_emb = nn.Sequential(\n",
    "            nn.Linear(len(num_cols), hidden_units // 2),\n",
    "            nn.LayerNorm(hidden_units // 2, eps=1e-6)\n",
    "        )\n",
    "\n",
    "        self.predict_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_units * 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.cat_cols = cat_cols\n",
    "        self.num_cols = num_cols\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "\n",
    "        self.emb_layernorm = nn.LayerNorm(hidden_units, eps=1e-6)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([SASRecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
    "    \n",
    "    \n",
    "    def forward(self, past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature):\n",
    "        \"\"\"\n",
    "        past_cat_feature : (batch_size, max_len, cat_cols)\n",
    "        past_num_feature : (batch_size, max_len, num_cols)\n",
    "        past_answerCode : (batch_size, max_len)\n",
    "\n",
    "        now_cat_feature : (batch_size, max_len, cat_cols)\n",
    "        now_num_feature : (batch_size, max_len, num_cols)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        past_cat_emb_list = []\n",
    "        for idx in range(len(self.cat_cols)):\n",
    "            if self.cat_cols[idx] == 'userID':\n",
    "                past_cat_emb_list.append(self.userID_emb(past_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'assessmentItemID2idx':\n",
    "                past_cat_emb_list.append(self.assessmentItemID_emb(past_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'testId2idx':\n",
    "                past_cat_emb_list.append(self.testId_emb(past_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'KnowledgeTag2idx':\n",
    "                past_cat_emb_list.append(self.KnowledgeTag_emb(past_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'large_paper_number2idx':\n",
    "                past_cat_emb_list.append(self.large_paper_number_emb(past_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'hour':\n",
    "                past_cat_emb_list.append(self.hour_emb(past_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'dayofweek':\n",
    "                past_cat_emb_list.append(self.dayofweek_emb(past_cat_feature[:, :, idx]))\n",
    "\n",
    "        past_cat_emb = torch.concat(past_cat_emb_list, dim = -1)\n",
    "        past_cat_emb = self.cat_emb(past_cat_emb)\n",
    "        past_num_emb = self.num_emb(past_num_feature)\n",
    "\n",
    "        past_emb = torch.concat([past_cat_emb, past_num_emb], dim = -1)\n",
    "        idx = self.cat_cols.index('relevance_elapsed')\n",
    "        past_emb += self.relevance_elapsed_emb(past_cat_feature[:, :, idx])\n",
    "        past_emb += self.answerCode_emb(past_answerCode.to(self.device))\n",
    "        past_emb = self.emb_layernorm(self.dropout(past_emb)) # LayerNorm\n",
    "\n",
    "        # masking \n",
    "        mask_pad = torch.BoolTensor(past_answerCode > 0).unsqueeze(1).unsqueeze(1) # (batch_size, 1, 1, max_len)\n",
    "        mask_time = (1 - torch.triu(torch.ones((1, 1, past_answerCode.size(1), past_answerCode.size(1))), diagonal=1)).bool() # (batch_size, 1, max_len, max_len)\n",
    "        mask = (mask_pad & mask_time).to(self.device) # (batch_size, 1, max_len, max_len)\n",
    "        for block in self.blocks:\n",
    "            past_emb, attn_dist = block(past_emb, mask)\n",
    "\n",
    "        now_cat_emb_list = []\n",
    "        for idx in range(len(self.cat_cols)):\n",
    "            if self.cat_cols[idx] == 'userID':\n",
    "                now_cat_emb_list.append(self.userID_emb(now_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'assessmentItemID2idx':\n",
    "                now_cat_emb_list.append(self.assessmentItemID_emb(now_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'testId2idx':\n",
    "                now_cat_emb_list.append(self.testId_emb(now_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'KnowledgeTag2idx':\n",
    "                now_cat_emb_list.append(self.KnowledgeTag_emb(now_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'large_paper_number2idx':\n",
    "                now_cat_emb_list.append(self.large_paper_number_emb(now_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'hour':\n",
    "                now_cat_emb_list.append(self.hour_emb(now_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'dayofweek':\n",
    "                now_cat_emb_list.append(self.dayofweek_emb(now_cat_feature[:, :, idx]))\n",
    "\n",
    "        now_cat_emb = torch.concat(now_cat_emb_list, dim = -1)\n",
    "        now_cat_emb = self.cat_emb(now_cat_emb)\n",
    "        now_num_emb = self.num_emb(now_num_feature)\n",
    "\n",
    "        now_emb = torch.concat([now_cat_emb, now_num_emb], dim = -1)\n",
    "        idx = self.cat_cols.index('relevance_elapsed')\n",
    "        now_emb += self.relevance_elapsed_emb(now_cat_feature[:, :, idx])\n",
    "        now_emb = self.emb_layernorm(now_emb) # LayerNorm\n",
    "\n",
    "        emb = torch.concat([past_emb, now_emb], dim = -1)\n",
    "        output = self.predict_layer(emb)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, data_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "\n",
    "    for past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature, now_answerCode in data_loader:\n",
    "\n",
    "        past_cat_feature, past_num_feature, past_answerCode = past_cat_feature.to(device), past_num_feature.to(device), past_answerCode\n",
    "        now_cat_feature, now_num_feature, now_answerCode = now_cat_feature.to(device), now_num_feature.to(device), now_answerCode.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature).squeeze(2)\n",
    "        loss = criterion(output[now_answerCode != -1], now_answerCode[now_answerCode != -1])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val += loss.item()\n",
    "\n",
    "    loss_val /= len(data_loader)\n",
    "\n",
    "    return loss_val\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    target = []\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature, now_answerCode in data_loader:\n",
    "            past_cat_feature, past_num_feature, past_answerCode = past_cat_feature.to(device), past_num_feature.to(device), past_answerCode\n",
    "            now_cat_feature, now_num_feature, now_answerCode = now_cat_feature.to(device), now_num_feature.to(device), now_answerCode.to(device)\n",
    "            \n",
    "            output = model(past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature).squeeze(2)\n",
    "\n",
    "            target.extend(now_answerCode[:, -1].cpu().numpy().tolist())\n",
    "            pred.extend(output[:, -1].cpu().numpy().tolist())\n",
    "\n",
    "    roc_auc = roc_auc_score(target, pred)\n",
    "\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature, now_answerCode in data_loader:\n",
    "            past_cat_feature, past_num_feature, past_answerCode = past_cat_feature.to(device), past_num_feature.to(device), past_answerCode\n",
    "            now_cat_feature, now_num_feature = now_cat_feature.to(device), now_num_feature.to(device)\n",
    "            \n",
    "            output = model(past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature).squeeze(2)\n",
    "            pred.extend(output[:, -1].cpu().numpy().tolist())\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "lr = 0.001\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "emb_size = 50\n",
    "hidden_units = 256\n",
    "num_heads = 1 \n",
    "num_layers = 2\n",
    "dropout_rate = 0.5\n",
    "num_workers = 8\n",
    "\n",
    "DATA_PATH = '/opt/ml/input/data'\n",
    "MODEL_PATH = '/opt/ml/model'\n",
    "SUBMISSION_PATH = '/opt/ml/submission'\n",
    "\n",
    "model_name = 'Transformer-num-faeture.pt'\n",
    "submission_name = 'Transformer-num-faeture.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(MODEL_PATH):\n",
    "    os.mkdir(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(SUBMISSION_PATH):\n",
    "    os.mkdir(SUBMISSION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dataset = MakeDataset(DATA_PATH = DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOF Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_roc_auc = 0\n",
    "\n",
    "for oof in make_dataset.oof_user_set.keys():\n",
    "    train_df, valid_df = make_dataset.get_oof_data(oof)\n",
    "    \n",
    "    seed_everything(22 + oof)\n",
    "    \n",
    "    train_dataset = CustomDataset(df = train_df)\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size = batch_size, \n",
    "        shuffle = True, \n",
    "        drop_last = False,\n",
    "        collate_fn = train_make_batch,\n",
    "        num_workers = num_workers)\n",
    "\n",
    "    valid_dataset = CustomDataset(df = valid_df)\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = 1, \n",
    "        shuffle = False, \n",
    "        drop_last = False,\n",
    "        collate_fn = train_make_batch,\n",
    "        num_workers = num_workers)\n",
    "\n",
    "    model = SASRec(\n",
    "        num_userID = make_dataset.num_userID,\n",
    "        num_assessmentItemID = make_dataset.num_assessmentItemID, \n",
    "        num_testId = make_dataset.num_testId,\n",
    "        num_KnowledgeTag = make_dataset.num_KnowledgeTag,\n",
    "        num_large_paper_number = make_dataset.num_large_paper_number,\n",
    "        num_hour = make_dataset.num_hour,\n",
    "        num_dayofweek = make_dataset.num_dayofweek,\n",
    "        num_relevance_elapsed = make_dataset.num_relevance_elapsed,\n",
    "        num_cols = train_dataset.num_cols,\n",
    "        cat_cols = train_dataset.cat_cols,\n",
    "        emb_size = emb_size,\n",
    "        hidden_units = hidden_units,\n",
    "        num_heads = num_heads,\n",
    "        num_layers = num_layers,\n",
    "        dropout_rate = dropout_rate,\n",
    "        device = device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    best_epoch = 0\n",
    "    best_train_loss = 0\n",
    "    best_roc_auc = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tbar = tqdm(range(1))\n",
    "        for _ in tbar:\n",
    "            train_loss = train(model = model, data_loader = train_data_loader, criterion = criterion, optimizer = optimizer)\n",
    "            roc_auc = evaluate(model = model, data_loader = valid_data_loader)\n",
    "            if best_roc_auc < roc_auc:\n",
    "                best_epoch = epoch\n",
    "                best_train_loss = train_loss\n",
    "                best_roc_auc = roc_auc\n",
    "                torch.save(model.state_dict(), os.path.join(MODEL_PATH, f'oof_{oof}_' + model_name))\n",
    "\n",
    "            tbar.set_description(f'OOF-{oof}| Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| roc_auc: {roc_auc:.5f}')\n",
    "    \n",
    "    print(f'BEST OOF-{oof}| Epoch: {best_epoch:3d}| Train loss: {best_train_loss:.5f}| roc_auc: {best_roc_auc:.5f}')\n",
    "\n",
    "    oof_roc_auc += best_roc_auc\n",
    "\n",
    "print(f'Total roc_auc: {oof_roc_auc / len(make_dataset.oof_user_set.keys()):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch:   1| Train loss: 0.56627| roc_auc: 0.75166\n",
    "Epoch:   2| Train loss: 0.51862| roc_auc: 0.78404\n",
    "Epoch:   3| Train loss: 0.49322| roc_auc: 0.79445\n",
    "Epoch:   4| Train loss: 0.48389| roc_auc: 0.80225\n",
    "Epoch:   5| Train loss: 0.48177| roc_auc: 0.80515\n",
    "Epoch:   6| Train loss: 0.47953| roc_auc: 0.80476\n",
    "Epoch:   7| Train loss: 0.47707| roc_auc: 0.80723\n",
    "Epoch:   8| Train loss: 0.47718| roc_auc: 0.80823\n",
    "Epoch:   9| Train loss: 0.47595| roc_auc: 0.80879\n",
    "Epoch:  10| Train loss: 0.47404| roc_auc: 0.81498\n",
    "Epoch:  11| Train loss: 0.47311| roc_auc: 0.81593\n",
    "Epoch:  12| Train loss: 0.47249| roc_auc: 0.80996\n",
    "Epoch:  13| Train loss: 0.47182| roc_auc: 0.81465\n",
    "Epoch:  14| Train loss: 0.47140| roc_auc: 0.81538\n",
    "Epoch:  15| Train loss: 0.47009| roc_auc: 0.81111\n",
    "Epoch:  16| Train loss: 0.46932| roc_auc: 0.81620\n",
    "Epoch:  17| Train loss: 0.46816| roc_auc: 0.81794\n",
    "Epoch:  18| Train loss: 0.46769| roc_auc: 0.81369\n",
    "Epoch:  19| Train loss: 0.46661| roc_auc: 0.81862\n",
    "Epoch:  20| Train loss: 0.46604| roc_auc: 0.82006 <- best\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = make_dataset.get_test_data()\n",
    "test_dataset = CustomDataset(df = test_df)\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 1, \n",
    "    shuffle = False, \n",
    "    drop_last = False,\n",
    "    collate_fn = train_make_batch,\n",
    "    num_workers = num_workers)\n",
    "\n",
    "pred_list = []\n",
    "\n",
    "model = SASRec(\n",
    "    num_userID = make_dataset.num_userID, \n",
    "    num_assessmentItemID = make_dataset.num_assessmentItemID, \n",
    "    num_testId = make_dataset.num_testId,\n",
    "    num_KnowledgeTag = make_dataset.num_KnowledgeTag,\n",
    "    num_large_paper_number = make_dataset.num_large_paper_number,\n",
    "    num_hour = make_dataset.num_hour,\n",
    "    num_dayofweek = make_dataset.num_dayofweek,\n",
    "    num_relevance_elapsed = make_dataset.num_relevance_elapsed,\n",
    "    num_cols = train_dataset.num_cols,\n",
    "    cat_cols = train_dataset.cat_cols,\n",
    "    emb_size = emb_size, \n",
    "    hidden_units = hidden_units, \n",
    "    num_heads = num_heads, \n",
    "    num_layers = num_layers, \n",
    "    dropout_rate = dropout_rate, \n",
    "    device = device).to(device)\n",
    "\n",
    "for oof in make_dataset.oof_user_set.keys():\n",
    "    model.load_state_dict(torch.load(os.path.join(MODEL_PATH, f'oof_{oof}_' + model_name)))\n",
    "    pred = predict(model = model, data_loader = test_data_loader)\n",
    "    pred_list.append(pred)\n",
    "\n",
    "pred_list = np.array(pred_list).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data = np.array(pred_list), columns = ['prediction'])\n",
    "submission['id'] = submission.index\n",
    "submission = submission[['id', 'prediction']]\n",
    "submission.to_csv(os.path.join(SUBMISSION_PATH, submission_name), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
