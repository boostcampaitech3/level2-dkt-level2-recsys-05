{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "torch.set_printoptions(sci_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDataset():\n",
    "\n",
    "    def __init__(self, DATA_PATH):\n",
    "        self.preporcessing(DATA_PATH)\n",
    "        self.oof_user_set = self.split_data()\n",
    "    \n",
    "    def split_data(self):\n",
    "        user_list = self.all_df['userID'].unique().tolist()\n",
    "        oof_user_set = {}\n",
    "        kf = KFold(n_splits = 5, random_state = 22, shuffle = True)\n",
    "        for idx, (train_user, valid_user) in enumerate(kf.split(user_list)):\n",
    "            oof_user_set[idx] = valid_user.tolist()\n",
    "        \n",
    "        return oof_user_set\n",
    "\n",
    "    def preporcessing(self, DATA_PATH):\n",
    "\n",
    "        dtype = {\n",
    "            'userID': 'int16',\n",
    "            'answerCode': 'int8',\n",
    "            'KnowledgeTag': 'int16'\n",
    "        }\n",
    "        \n",
    "        train_df = pd.read_csv(os.path.join(DATA_PATH, 'train_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "        train_df = train_df.sort_values(by=['userID', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "        test_df = pd.read_csv(os.path.join(DATA_PATH, 'test_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "        test_df = test_df.sort_values(by=['userID', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        def get_large_paper_number(x):\n",
    "            return x[1:4]\n",
    "        \n",
    "        train_df['large_paper_number'] = train_df['assessmentItemID'].apply(lambda x : get_large_paper_number(x))\n",
    "        test_df['large_paper_number'] = test_df['assessmentItemID'].apply(lambda x : get_large_paper_number(x))\n",
    "\n",
    "        # 문제 푸는데 걸린 시간\n",
    "        def get_now_elapsed(df):\n",
    "            \n",
    "            diff = df.loc[:, ['userID','Timestamp']].groupby('userID').diff().fillna(pd.Timedelta(seconds=0))\n",
    "            diff = diff.fillna(pd.Timedelta(seconds=0))\n",
    "            diff = diff['Timestamp'].apply(lambda x: x.total_seconds())\n",
    "            df['now_elapsed'] = diff\n",
    "            df['now_elapsed'] = df['now_elapsed'].apply(lambda x : x if x < 650 and x >=0 else 0)\n",
    "            df['now_elapsed'] = df['now_elapsed']\n",
    "\n",
    "            return df\n",
    "\n",
    "        train_df = get_now_elapsed(df = train_df)\n",
    "        test_df = get_now_elapsed(df = test_df)\n",
    "\n",
    "        all_df = pd.concat([train_df, test_df])\n",
    "        all_df = all_df[all_df['answerCode'] != -1].reset_index(drop = True)\n",
    "\n",
    "        # normalize_score\n",
    "        def get_normalize_score(df, all_df):\n",
    "            ret_df = []\n",
    "\n",
    "            group_df = df.groupby('userID')\n",
    "            mean_answerCode_df = all_df.groupby('testId').mean()['answerCode']\n",
    "            std_answerCode_df = all_df.groupby('testId').std()['answerCode']\n",
    "            for userID, get_df in group_df:\n",
    "                normalize_score_df = (get_df[get_df['answerCode'] != -1].groupby('testId').mean()['answerCode'] - mean_answerCode_df) / std_answerCode_df\n",
    "                get_df = get_df.copy().set_index('testId')\n",
    "                get_df['normalize_score'] = normalize_score_df\n",
    "                ret_df.append(get_df.reset_index(drop = False))\n",
    "\n",
    "            ret_df = pd.concat(ret_df).reset_index(drop = True)\n",
    "            \n",
    "            return ret_df\n",
    "        \n",
    "        # train_df = get_normalize_score(df = train_df, all_df = all_df)\n",
    "        # test_df = get_normalize_score(df = test_df, all_df = all_df)\n",
    "\n",
    "        # 문항별 정답률\n",
    "        train_df = train_df.set_index('assessmentItemID')\n",
    "        train_df['assessmentItemID_mean_answerCode'] = all_df.groupby('assessmentItemID').mean()['answerCode']\n",
    "        train_df = train_df.reset_index(drop = False)\n",
    "\n",
    "        test_df = test_df.set_index('assessmentItemID')\n",
    "        test_df['assessmentItemID_mean_answerCode'] = all_df.groupby('assessmentItemID').mean()['answerCode']\n",
    "        test_df = test_df.reset_index(drop = False)\n",
    "\n",
    "        # 문항별 정답률 표준편차\n",
    "        train_df = train_df.set_index('assessmentItemID')\n",
    "        train_df['assessmentItemID_std_answerCode'] = all_df.groupby('assessmentItemID').std()['answerCode']\n",
    "        train_df = train_df.reset_index(drop = False)\n",
    "\n",
    "        test_df = test_df.set_index('assessmentItemID')\n",
    "        test_df['assessmentItemID_std_answerCode'] = all_df.groupby('assessmentItemID').std()['answerCode']\n",
    "        test_df = test_df.reset_index(drop = False)\n",
    "\n",
    "        # 올바르게 푼 사람들의 문항별 풀이 시간 평균\n",
    "        train_df = train_df.set_index('assessmentItemID')\n",
    "        train_df['assessmentItemID_mean_now_elapsed'] = all_df[all_df['answerCode'] == 1].groupby('assessmentItemID').mean()['now_elapsed']\n",
    "        train_df = train_df.reset_index(drop = False)\n",
    "\n",
    "        test_df = test_df.set_index('assessmentItemID')\n",
    "        test_df['assessmentItemID_mean_now_elapsed'] = all_df[all_df['answerCode'] == 1].groupby('assessmentItemID').mean()['now_elapsed']\n",
    "        test_df = test_df.reset_index(drop = False)\n",
    "\n",
    "        # 올바르게 푼 사람들의 문항별 풀이 시간 표준 편차\n",
    "        train_df = train_df.set_index('assessmentItemID')\n",
    "        train_df['assessmentItemID_std_now_elapsed'] = all_df[all_df['answerCode'] == 1].groupby('assessmentItemID').std()['now_elapsed']\n",
    "        train_df = train_df.reset_index(drop = False)\n",
    "\n",
    "        test_df = test_df.set_index('assessmentItemID')\n",
    "        test_df['assessmentItemID_std_now_elapsed'] = all_df[all_df['answerCode'] == 1].groupby('assessmentItemID').std()['now_elapsed']\n",
    "        test_df = test_df.reset_index(drop = False)\n",
    "\n",
    "        # train_df['now_elapsed'] = train_df['now_elapsed'] + 1\n",
    "        # test_df['now_elapsed'] = test_df['now_elapsed'] + 1\n",
    "\n",
    "        # 0을 패딩으로 줄것이기 때문에 평행 이동 시켜야 함\n",
    "        # 해당 년도의 몇번째 주인지 판단\n",
    "        # https://github.com/tidyverse/lubridate/issues/731\n",
    "        sin_val = np.sin(2 * np.pi * np.array([i for i in range(1, 54)]) / 53)\n",
    "        cos_val = np.cos(2 * np.pi * np.array([i for i in range(1, 54)]) / 53)\n",
    "\n",
    "        train_df['week_number'] = train_df['Timestamp'].apply(lambda x:x.isocalendar()[1]) - 1\n",
    "        train_df['num_week_number'] = 2 * np.pi * (train_df['week_number'] + 1) / 53\n",
    "        train_df['sin_num_week_number'] = np.sin(train_df['num_week_number']) + 2 * abs(sin_val.min())\n",
    "        train_df['cos_num_week_number'] = np.cos(train_df['num_week_number']) + 2 * abs(cos_val.min())\n",
    "\n",
    "        test_df['week_number'] = test_df['Timestamp'].apply(lambda x:x.isocalendar()[1]) - 1\n",
    "        test_df['num_week_number'] = 2 * np.pi * (test_df['week_number'] + 1) / 53\n",
    "        test_df['sin_num_week_number'] = np.sin(test_df['num_week_number']) + 2 * abs(sin_val.min())\n",
    "        test_df['cos_num_week_number'] = np.cos(test_df['num_week_number']) + 2 * abs(cos_val.min())\n",
    "\n",
    "        # 문제 푼 시간\n",
    "        sin_val = np.sin(2 * np.pi * np.array([i for i in range(1, 25)]) / 24)\n",
    "        cos_val = np.cos(2 * np.pi * np.array([i for i in range(1, 25)]) / 24)\n",
    "\n",
    "        train_df['hour'] = train_df['Timestamp'].dt.hour\n",
    "        train_df['num_hour'] = 2 * np.pi * (train_df['hour'] + 1) / 24\n",
    "        train_df['sin_num_hour'] = np.sin(train_df['num_hour']) + 2 * abs(sin_val.min())\n",
    "        train_df['cos_num_hour'] = np.cos(train_df['num_hour']) + 2 * abs(cos_val.min())\n",
    "\n",
    "        test_df['hour'] = test_df['Timestamp'].dt.hour\n",
    "        test_df['num_hour'] = 2 * np.pi * (test_df['hour'] + 1) / 24\n",
    "        test_df['sin_num_hour'] = np.sin(test_df['num_hour']) + 2 * abs(sin_val.min())\n",
    "        test_df['cos_num_hour'] = np.cos(test_df['num_hour']) + 2 * abs(cos_val.min())\n",
    "\n",
    "        # 문제 푼 요일\n",
    "        sin_val = np.sin(2 * np.pi * np.array([i for i in range(1, 8)]) / 7)\n",
    "        cos_val = np.cos(2 * np.pi * np.array([i for i in range(1, 8)]) / 7)\n",
    "\n",
    "        train_df['dayofweek'] = train_df['Timestamp'].dt.dayofweek\n",
    "        train_df['num_dayofweek'] = 2 * np.pi * (train_df['dayofweek'] + 1) / 7\n",
    "        train_df['sin_num_dayofweek'] = np.sin(train_df['num_dayofweek']) + 2 * abs(sin_val.min())\n",
    "        train_df['cos_num_dayofweek'] = np.cos(train_df['num_dayofweek']) + 2 * abs(cos_val.min())\n",
    "\n",
    "        test_df['dayofweek'] = test_df['Timestamp'].dt.dayofweek\n",
    "        test_df['num_dayofweek'] = 2 * np.pi * (test_df['dayofweek'] + 1) / 7\n",
    "        test_df['sin_num_dayofweek'] = np.sin(test_df['num_dayofweek']) + 2 * abs(sin_val.min())\n",
    "        test_df['cos_num_dayofweek'] = np.cos(test_df['num_dayofweek']) + 2 * abs(cos_val.min())\n",
    "\n",
    "        # 해당 대분류 시험지를 푼 기간 (주 단위)\n",
    "        def get_now_week(df):\n",
    "            userID2large_paper_number2week_number2now_week = {}\n",
    "            group_df = df.groupby('userID')\n",
    "\n",
    "            for userID, g_df in group_df:\n",
    "                large_paper_number2week_number = {}\n",
    "                gg_df = g_df.groupby('large_paper_number')\n",
    "                for large_paper_number, ggg_df in gg_df:\n",
    "                    week_number2now_week = {}\n",
    "                    for idx, week_number in enumerate(sorted(ggg_df['week_number'].unique())):\n",
    "                        week_number2now_week[week_number] = idx\n",
    "                    \n",
    "                    large_paper_number2week_number[large_paper_number] = week_number2now_week\n",
    "\n",
    "                userID2large_paper_number2week_number2now_week[userID] = large_paper_number2week_number\n",
    "\n",
    "            def get_now_week_val(x):\n",
    "                return userID2large_paper_number2week_number2now_week[x['userID']][x['large_paper_number']][x['week_number']]\n",
    "\n",
    "            df['now_week'] = df.apply(lambda x : get_now_week_val(x), axis = 1)\n",
    "\n",
    "            return df\n",
    "\n",
    "        train_df = get_now_week(df = train_df)\n",
    "        test_df = get_now_week(df = test_df)\n",
    "    \n",
    "        # index 로 변환\n",
    "\n",
    "        def get_val2idx(val_list : list) -> dict:\n",
    "            val2idx = {}\n",
    "            for idx, val in enumerate(val_list):\n",
    "                val2idx[val] = idx\n",
    "            \n",
    "            return val2idx\n",
    "\n",
    "        assessmentItemID2idx = get_val2idx(all_df['assessmentItemID'].unique().tolist())\n",
    "        testId2idx = get_val2idx(all_df['testId'].unique().tolist())\n",
    "        KnowledgeTag2idx = get_val2idx(all_df['KnowledgeTag'].unique().tolist())\n",
    "        large_paper_number2idx = get_val2idx(all_df['large_paper_number'].unique().tolist())\n",
    "\n",
    "        train_df['assessmentItemID2idx'] = train_df['assessmentItemID'].apply(lambda x : assessmentItemID2idx[x])\n",
    "        train_df['testId2idx'] = train_df['testId'].apply(lambda x : testId2idx[x])\n",
    "        train_df['KnowledgeTag2idx'] = train_df['KnowledgeTag'].apply(lambda x : KnowledgeTag2idx[x])\n",
    "        train_df['large_paper_number2idx'] = train_df['large_paper_number'].apply(lambda x : large_paper_number2idx[x])\n",
    "\n",
    "        test_df['assessmentItemID2idx'] = test_df['assessmentItemID'].apply(lambda x : assessmentItemID2idx[x])\n",
    "        test_df['testId2idx'] = test_df['testId'].apply(lambda x : testId2idx[x])\n",
    "        test_df['KnowledgeTag2idx'] = test_df['KnowledgeTag'].apply(lambda x : KnowledgeTag2idx[x])\n",
    "        test_df['large_paper_number2idx'] = test_df['large_paper_number'].apply(lambda x : large_paper_number2idx[x])\n",
    "\n",
    "        self.assessmentItemID2idx = assessmentItemID2idx\n",
    "        self.train_df, self.test_df = train_df, test_df\n",
    "        self.all_df = pd.concat([train_df, test_df[test_df['answerCode'] != -1]]).reset_index(drop=True)\n",
    "        self.num_assessmentItemID = len(assessmentItemID2idx)\n",
    "        self.num_testId = len(testId2idx)\n",
    "        self.num_KnowledgeTag = len(KnowledgeTag2idx)\n",
    "        self.num_large_paper_number = len(large_paper_number2idx)\n",
    "        self.num_hour = 24\n",
    "        self.num_dayofweek = 7\n",
    "        self.num_week_number = 53\n",
    "\n",
    "    def get_oof_data(self, oof):\n",
    "\n",
    "        val_user_list = self.oof_user_set[oof]\n",
    "\n",
    "        train = []\n",
    "        valid = []\n",
    "\n",
    "        group_df = self.all_df.groupby('userID')\n",
    "\n",
    "        for userID, df in group_df:\n",
    "            if userID in val_user_list:\n",
    "                trn_df = df.iloc[:-1, :]\n",
    "                val_df = df.copy()\n",
    "                train.append(trn_df)\n",
    "                valid.append(val_df)\n",
    "            else:\n",
    "                train.append(df)\n",
    "\n",
    "        # normalize_score\n",
    "        def get_normalize_score(df, all_df, vailid = False):\n",
    "            ret_df = []\n",
    "\n",
    "            group_df = df.groupby('userID')\n",
    "            mean_answerCode_df = all_df.groupby('testId').mean()['answerCode']\n",
    "            std_answerCode_df = all_df.groupby('testId').std()['answerCode']\n",
    "            for userID, get_df in group_df:\n",
    "                if vailid:\n",
    "                    normalize_score_df = (get_df.iloc[:-1, :].groupby('testId').mean()['answerCode'] - mean_answerCode_df) / std_answerCode_df\n",
    "                else:\n",
    "                    normalize_score_df = (get_df.groupby('testId').mean()['answerCode'] - mean_answerCode_df) / std_answerCode_df\n",
    "                    \n",
    "                get_df = get_df.copy().set_index('testId')\n",
    "                get_df['normalize_score'] = normalize_score_df\n",
    "                ret_df.append(get_df.reset_index(drop = False))\n",
    "\n",
    "            ret_df = pd.concat(ret_df).reset_index(drop = True)\n",
    "            \n",
    "            return ret_df\n",
    "\n",
    "        train = pd.concat(train).reset_index(drop = True)\n",
    "        valid = pd.concat(valid).reset_index(drop = True)\n",
    "\n",
    "        # train = get_normalize_score(df = train, all_df = train)\n",
    "        # valid = get_normalize_score(df = valid, all_df = train, vailid = True)\n",
    "        \n",
    "        return train, valid\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        return self.test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        df,\n",
    "        cat_cols = ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx', 'hour', 'dayofweek'],\n",
    "        num_cols = ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode', 'now_week'],\n",
    "        max_len = None,\n",
    "        window = None,\n",
    "        data_augmentation = False,\n",
    "        ):\n",
    "\n",
    "        self.cat_cols = cat_cols\n",
    "        self.num_cols = num_cols\n",
    "        self.get_df = df.groupby('userID')\n",
    "        self.user_list = df['userID'].unique().tolist()\n",
    "        self.max_len = max_len\n",
    "        self.window = window\n",
    "        self.data_augmentation = data_augmentation\n",
    "        if self.data_augmentation:\n",
    "            self.cat_feature_list, self.num_feature_list, self.answerCode_list = self._data_augmentation()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.data_augmentation:\n",
    "            return len(self.cat_feature_list)\n",
    "        return len(self.user_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.data_augmentation:\n",
    "            cat_feature = self.cat_feature_list[idx]\n",
    "            num_feature = self.num_feature_list[idx]\n",
    "            answerCode = self.answerCode_list[idx]\n",
    "\n",
    "            now_cat_feature = cat_feature[1:, :]\n",
    "            now_num_feature = num_feature[1:, :]\n",
    "            now_answerCode = answerCode[1:]\n",
    "            \n",
    "            past_cat_feature = cat_feature[:-1, :]\n",
    "            past_num_feature = num_feature[:-1, :]\n",
    "            past_answerCode = answerCode[:-1]\n",
    "            \n",
    "        else:\n",
    "            user = self.user_list[idx]\n",
    "            if self.max_len:\n",
    "                get_df = self.get_df.get_group(user).iloc[-self.max_len:, :]\n",
    "            else:\n",
    "                get_df = self.get_df.get_group(user)\n",
    "\n",
    "            now_df = get_df.iloc[1:, :]\n",
    "            now_cat_feature = now_df[self.cat_cols].values\n",
    "            now_num_feature = now_df[self.num_cols].values\n",
    "            now_answerCode = now_df['answerCode'].values\n",
    "\n",
    "            past_df = get_df.iloc[:-1, :]\n",
    "            past_cat_feature = past_df[self.cat_cols].values\n",
    "            past_num_feature = past_df[self.num_cols].values\n",
    "            past_answerCode = past_df['answerCode'].values\n",
    "\n",
    "        return {\n",
    "            'past_cat_feature' : past_cat_feature, \n",
    "            'past_num_feature' : past_num_feature, \n",
    "            'past_answerCode' : past_answerCode, \n",
    "            'now_cat_feature' : now_cat_feature, \n",
    "            'now_num_feature' : now_num_feature, \n",
    "            'now_answerCode' : now_answerCode\n",
    "            }\n",
    "    \n",
    "\n",
    "    def _data_augmentation(self):\n",
    "        cat_feature_list = []\n",
    "        num_feature_list = []\n",
    "        answerCode_list = []\n",
    "        for userID, get_df in tqdm(self.get_df):\n",
    "            cat_feature = get_df[self.cat_cols].values[::-1]\n",
    "            num_feature = get_df[self.num_cols].values[::-1]\n",
    "            answerCode = get_df['answerCode'].values[::-1]\n",
    "\n",
    "            start_idx = 0\n",
    "\n",
    "            if len(get_df) <= self.max_len:\n",
    "                cat_feature_list.append(cat_feature[::-1])\n",
    "                num_feature_list.append(num_feature[::-1])\n",
    "                answerCode_list.append(answerCode[::-1])\n",
    "            else:\n",
    "                while True:\n",
    "                    if len(cat_feature[start_idx: start_idx + self.max_len, :]) < self.max_len:\n",
    "                        cat_feature_list.append(cat_feature[start_idx: start_idx + self.max_len, :][::-1])\n",
    "                        num_feature_list.append(num_feature[start_idx: start_idx + self.max_len, :][::-1])\n",
    "                        answerCode_list.append(answerCode[start_idx: start_idx + self.max_len][::-1])\n",
    "                        break\n",
    "                    cat_feature_list.append(cat_feature[start_idx: start_idx + self.max_len, :][::-1])\n",
    "                    num_feature_list.append(num_feature[start_idx: start_idx + self.max_len, :][::-1])\n",
    "                    answerCode_list.append(answerCode[start_idx: start_idx + self.max_len][::-1])\n",
    "                    start_idx += self.window\n",
    "            \n",
    "        return cat_feature_list, num_feature_list, answerCode_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(seq, max_len, padding_value = 0):\n",
    "    try:\n",
    "        seq_len, col = seq.shape\n",
    "        padding = np.zeros((max_len - seq_len, col)) + padding_value\n",
    "    except:\n",
    "        seq_len = seq.shape[0]\n",
    "        padding = np.zeros((max_len - seq_len, )) + padding_value\n",
    "\n",
    "    padding_seq = np.concatenate([padding, seq])\n",
    "\n",
    "    return padding_seq\n",
    "\n",
    "def train_make_batch(samples):\n",
    "    max_len = 0\n",
    "    for sample in samples:\n",
    "        seq_len, col = sample['past_cat_feature'].shape\n",
    "        if max_len < seq_len:\n",
    "            max_len = seq_len\n",
    "    \n",
    "    past_cat_feature = []\n",
    "    past_num_feature = []\n",
    "    past_answerCode = []\n",
    "    now_cat_feature = []\n",
    "    now_num_feature = []\n",
    "    now_answerCode = []\n",
    "\n",
    "    for sample in samples:\n",
    "        past_cat_feature += [pad_sequence(sample['past_cat_feature'] + 1, max_len = max_len, padding_value = 0)]\n",
    "        past_num_feature += [pad_sequence(sample['past_num_feature'], max_len = max_len, padding_value = 0)]\n",
    "        past_answerCode += [pad_sequence(sample['past_answerCode'] + 1, max_len = max_len, padding_value = 0)]\n",
    "        now_cat_feature += [pad_sequence(sample['now_cat_feature'] + 1, max_len = max_len, padding_value = 0)]\n",
    "        now_num_feature += [pad_sequence(sample['now_num_feature'], max_len = max_len, padding_value = 0)]\n",
    "        now_answerCode += [pad_sequence(sample['now_answerCode'], max_len = max_len, padding_value = -1)]\n",
    "\n",
    "    return torch.tensor(past_cat_feature, dtype = torch.long), torch.tensor(past_num_feature, dtype = torch.float32), torch.tensor(past_answerCode, dtype = torch.long), torch.tensor(now_cat_feature, dtype = torch.long), torch.tensor(now_num_feature, dtype = torch.float32), torch.tensor(now_answerCode, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        \"\"\"\n",
    "        Q, K, V : (batch_size, num_heads, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \"\"\"\n",
    "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units) # (batch_size, num_heads, max_len, max_len)\n",
    "        attn_score = attn_score.masked_fill(mask == 0, -1e9)  # 유사도가 0인 지점은 -infinity로 보내 softmax 결과가 0이 되도록 함\n",
    "        attn_dist = self.dropout(F.softmax(attn_score, dim=-1))  # attention distribution\n",
    "        output = torch.matmul(attn_dist, V)  # (batch_size, num_heads, max_len, hidden_units) / # dim of output : batchSize x num_head x seqLen x hidden_units\n",
    "        return output, attn_dist\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads # head의 수\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        # query, key, value, output 생성을 위해 Linear 모델 생성\n",
    "        self.W_Q = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_K = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_V = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_O = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(hidden_units, dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, enc, mask):\n",
    "        \"\"\"\n",
    "        enc : (batch_size, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \n",
    "        \"\"\"\n",
    "        residual = enc # residual connection을 위해 residual 부분을 저장\n",
    "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
    "\n",
    "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
    "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units // self.num_heads) # (batch_size, max_len, num_heads, hidden_units)\n",
    "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units // self.num_heads) # (batch_size, max_len, num_heads, hidden_units)\n",
    "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units // self.num_heads) # (batch_size, max_len, num_heads, hidden_units)\n",
    "\n",
    "        # Head별로 각기 다른 attention이 가능하도록 Transpose 후 각각 attention에 통과시킴\n",
    "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2) # (batch_size, num_heads, max_len, hidden_units)\n",
    "        output, attn_dist = self.attention(Q, K, V, mask) # output : (batch_size, num_heads, max_len, hidden_units) / attn_dist : (batch_size, num_heads, max_len, max_len)\n",
    "\n",
    "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n",
    "        output = output.transpose(1, 2).contiguous() # (batch_size, max_len, num_heads, hidden_units) / contiguous() : 가변적 메모리 할당\n",
    "        output = output.view(batch_size, seqlen, -1) # (batch_size, max_len, hidden_units * num_heads)\n",
    "\n",
    "        # Linear Projection, Dropout, Residual sum, and Layer Normalization\n",
    "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual) # (batch_size, max_len, hidden_units)\n",
    "        return output, attn_dist\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "\n",
    "        self.W_1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.W_2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = self.W_2(F.relu(self.dropout(self.W_1(x))))\n",
    "        output = self.layerNorm(self.dropout(output) + residual)\n",
    "        return output\n",
    "\n",
    "\n",
    "class SASRecBlock(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(SASRecBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads, hidden_units, dropout_rate)\n",
    "        self.pointwise_feedforward = PositionwiseFeedForward(hidden_units, dropout_rate)\n",
    "\n",
    "    def forward(self, input_enc, mask):\n",
    "        \"\"\"\n",
    "        input_enc : (batch_size, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \"\"\"\n",
    "        output_enc, attn_dist = self.attention(input_enc, mask)\n",
    "        output_enc = self.pointwise_feedforward(output_enc)\n",
    "        return output_enc, attn_dist\n",
    "\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_assessmentItemID, \n",
    "        num_testId,\n",
    "        num_KnowledgeTag,\n",
    "        num_large_paper_number,\n",
    "        num_hour,\n",
    "        num_dayofweek,\n",
    "        num_week_number,\n",
    "        num_cols,\n",
    "        cat_cols,\n",
    "        emb_size,\n",
    "        hidden_units,\n",
    "        num_heads, \n",
    "        num_layers, \n",
    "        dropout_rate, \n",
    "        device):\n",
    "        super(SASRec, self).__init__()\n",
    "\n",
    "        self.cat_cols = cat_cols\n",
    "        self.num_cols = num_cols\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "\n",
    "        # past\n",
    "        past_emb = {}\n",
    "        for cat_col in self.cat_cols:\n",
    "            if cat_col == 'assessmentItemID2idx':\n",
    "                past_emb[cat_col] = nn.Embedding(num_assessmentItemID + 1, emb_size, padding_idx = 0) # 문항에 대한 정보\n",
    "            elif cat_col == 'testId2idx':\n",
    "                past_emb[cat_col] = nn.Embedding(num_testId + 1, emb_size, padding_idx = 0) # 시험지에 대한 정보\n",
    "            elif cat_col == 'KnowledgeTag2idx':\n",
    "                past_emb[cat_col] = nn.Embedding(num_KnowledgeTag + 1, emb_size, padding_idx = 0) # 지식 태그에 대한 정보\n",
    "            elif cat_col == 'large_paper_number2idx':\n",
    "                past_emb[cat_col] = nn.Embedding(num_large_paper_number + 1, emb_size, padding_idx = 0) # 학년에 대한 정보\n",
    "            elif cat_col == 'hour':\n",
    "                past_emb[cat_col] = nn.Embedding(num_hour + 1, emb_size, padding_idx = 0) # 문제 풀이 시간에 대한 정보\n",
    "            elif cat_col == 'dayofweek':\n",
    "                past_emb[cat_col] = nn.Embedding(num_dayofweek + 1, emb_size, padding_idx = 0) # 문제 풀이 요일에 대항 정보\n",
    "            elif cat_col == 'week_number':\n",
    "                past_emb[cat_col] = nn.Embedding(num_week_number + 1, emb_size, padding_idx = 0) # 문제 풀이 주에 대항 정보\n",
    "\n",
    "        self.past_emb_dict = nn.ModuleDict(past_emb)\n",
    "\n",
    "        self.past_answerCode_emb = nn.Embedding(3, hidden_units, padding_idx = 0) # 문제 정답 여부에 대한 정보\n",
    "\n",
    "        self.past_cat_emb = nn.Sequential(\n",
    "            nn.Linear(len(cat_cols) * emb_size, hidden_units // 2),\n",
    "            nn.LayerNorm(hidden_units // 2, eps=1e-6)\n",
    "        )\n",
    "\n",
    "        self.past_num_emb = nn.Sequential(\n",
    "            nn.Linear(len(num_cols), hidden_units // 2),\n",
    "            nn.LayerNorm(hidden_units // 2, eps=1e-6)\n",
    "        )\n",
    "\n",
    "        self.emb_layernorm = nn.LayerNorm(hidden_units, eps=1e-6)\n",
    "\n",
    "        self.past_lstm = nn.LSTM(\n",
    "            input_size = hidden_units,\n",
    "            hidden_size = hidden_units,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            bidirectional = False,\n",
    "            dropout = dropout_rate,\n",
    "            )\n",
    "\n",
    "        self.past_blocks = nn.ModuleList([SASRecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
    "\n",
    "        # now\n",
    "\n",
    "        now_emb = {}\n",
    "        for cat_col in self.cat_cols:\n",
    "            if cat_col == 'assessmentItemID2idx':\n",
    "                now_emb[cat_col] = nn.Embedding(num_assessmentItemID + 1, emb_size, padding_idx = 0) # 문항에 대한 정보\n",
    "            elif cat_col == 'testId2idx':\n",
    "                now_emb[cat_col] = nn.Embedding(num_testId + 1, emb_size, padding_idx = 0) # 시험지에 대한 정보\n",
    "            elif cat_col == 'KnowledgeTag2idx':\n",
    "                now_emb[cat_col] = nn.Embedding(num_KnowledgeTag + 1, emb_size, padding_idx = 0) # 지식 태그에 대한 정보\n",
    "            elif cat_col == 'large_paper_number2idx':\n",
    "                now_emb[cat_col] = nn.Embedding(num_large_paper_number + 1, emb_size, padding_idx = 0) # 학년에 대한 정보\n",
    "            elif cat_col == 'hour':\n",
    "                now_emb[cat_col] = nn.Embedding(num_hour + 1, emb_size, padding_idx = 0) # 문제 풀이 시간에 대한 정보\n",
    "            elif cat_col == 'dayofweek':\n",
    "                now_emb[cat_col] = nn.Embedding(num_dayofweek + 1, emb_size, padding_idx = 0) # 문제 풀이 요일에 대항 정보\n",
    "            elif cat_col == 'week_number':\n",
    "                now_emb[cat_col] = nn.Embedding(num_week_number + 1, emb_size, padding_idx = 0) # 문제 풀이 주에 대항 정보\n",
    "\n",
    "        self.now_emb_dict = nn.ModuleDict(now_emb)\n",
    "\n",
    "        self.now_cat_emb = nn.Sequential(\n",
    "            nn.Linear(len(cat_cols) * emb_size, hidden_units // 2),\n",
    "            nn.LayerNorm(hidden_units // 2, eps=1e-6)\n",
    "        )\n",
    "\n",
    "        self.now_num_emb = nn.Sequential(\n",
    "            nn.Linear(len(num_cols), hidden_units // 2),\n",
    "            nn.LayerNorm(hidden_units // 2, eps=1e-6)\n",
    "        )\n",
    "\n",
    "        self.now_lstm = nn.LSTM(\n",
    "            input_size = hidden_units,\n",
    "            hidden_size = hidden_units,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            bidirectional = False,\n",
    "            dropout = dropout_rate,\n",
    "            )\n",
    "\n",
    "        self.now_blocks = nn.ModuleList([SASRecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
    "\n",
    "        # predict\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.predict_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_units * 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature):\n",
    "        \"\"\"\n",
    "        past_cat_feature : (batch_size, max_len, cat_cols)\n",
    "        past_num_feature : (batch_size, max_len, num_cols)\n",
    "        past_answerCode : (batch_size, max_len)\n",
    "\n",
    "        now_cat_feature : (batch_size, max_len, cat_cols)\n",
    "        now_num_feature : (batch_size, max_len, num_cols)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        past_cat_emb_list = []\n",
    "        for idx, cat_col in enumerate(self.cat_cols):\n",
    "            past_cat_emb_list.append(self.past_emb_dict[cat_col](past_cat_feature[:, :, idx]))\n",
    "\n",
    "        past_cat_emb = torch.concat(past_cat_emb_list, dim = -1)\n",
    "        past_cat_emb = self.past_cat_emb(past_cat_emb)\n",
    "        past_num_emb = self.past_num_emb(past_num_feature)\n",
    "\n",
    "        past_emb = torch.concat([past_cat_emb, past_num_emb], dim = -1)\n",
    "        past_emb += self.past_answerCode_emb(past_answerCode.to(self.device))\n",
    "        past_emb = self.emb_layernorm(past_emb) # LayerNorm\n",
    "\n",
    "        # masking \n",
    "        mask_pad = torch.BoolTensor(past_answerCode > 0).unsqueeze(1).unsqueeze(1) # (batch_size, 1, 1, max_len)\n",
    "        mask_time = (1 - torch.triu(torch.ones((1, 1, past_answerCode.size(1), past_answerCode.size(1))), diagonal=1)).bool() # (batch_size, 1, max_len, max_len)\n",
    "        mask = (mask_pad & mask_time).to(self.device) # (batch_size, 1, max_len, max_len)\n",
    "        for block in self.past_blocks:\n",
    "            past_emb, attn_dist = block(past_emb, mask)\n",
    "\n",
    "        past_emb, _ = self.past_lstm(past_emb)\n",
    "\n",
    "        now_cat_emb_list = []\n",
    "        for idx, cat_col in enumerate(self.cat_cols):\n",
    "            now_cat_emb_list.append(self.now_emb_dict[cat_col](now_cat_feature[:, :, idx]))\n",
    "\n",
    "        now_cat_emb = torch.concat(now_cat_emb_list, dim = -1)\n",
    "        now_cat_emb = self.now_cat_emb(now_cat_emb)\n",
    "        now_num_emb = self.now_num_emb(now_num_feature)\n",
    "\n",
    "        now_emb = torch.concat([now_cat_emb, now_num_emb], dim = -1)\n",
    "\n",
    "        for block in self.now_blocks:\n",
    "            now_emb, attn_dist = block(now_emb, mask)\n",
    "\n",
    "        now_emb, _ = self.now_lstm(now_emb)\n",
    "\n",
    "        emb = torch.concat([past_emb, now_emb], dim = -1)\n",
    "        \n",
    "        output = self.predict_layer(self.dropout(emb))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, data_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "\n",
    "    for past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature, now_answerCode in data_loader:\n",
    "\n",
    "        past_cat_feature, past_num_feature, past_answerCode = past_cat_feature.to(device), past_num_feature.to(device), past_answerCode\n",
    "        now_cat_feature, now_num_feature, now_answerCode = now_cat_feature.to(device), now_num_feature.to(device), now_answerCode.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature).squeeze(2)\n",
    "        loss = criterion(output[now_answerCode != -1], now_answerCode[now_answerCode != -1])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val += loss.item()\n",
    "\n",
    "    loss_val /= len(data_loader)\n",
    "\n",
    "    return loss_val\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    target = []\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature, now_answerCode in data_loader:\n",
    "            past_cat_feature, past_num_feature, past_answerCode = past_cat_feature.to(device), past_num_feature.to(device), past_answerCode\n",
    "            now_cat_feature, now_num_feature, now_answerCode = now_cat_feature.to(device), now_num_feature.to(device), now_answerCode.to(device)\n",
    "            \n",
    "            output = model(past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature).squeeze(2)\n",
    "\n",
    "            target.extend(now_answerCode[:, -1].cpu().numpy().tolist())\n",
    "            pred.extend(output[:, -1].cpu().numpy().tolist())\n",
    "\n",
    "    roc_auc = roc_auc_score(target, pred)\n",
    "\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature, now_answerCode in data_loader:\n",
    "            past_cat_feature, past_num_feature, past_answerCode = past_cat_feature.to(device), past_num_feature.to(device), past_answerCode\n",
    "            now_cat_feature, now_num_feature = now_cat_feature.to(device), now_num_feature.to(device)\n",
    "            \n",
    "            output = model(past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature).squeeze(2)\n",
    "            pred.extend(output[:, -1].cpu().numpy().tolist())\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "emb_size = 64\n",
    "hidden_units = 128\n",
    "num_heads = 2 # 2,4,8,16,32\n",
    "num_layers = 1\n",
    "dropout_rate = 0.5\n",
    "num_workers = 8\n",
    "\n",
    "DATA_PATH = '/opt/ml/input/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dataset = MakeDataset(DATA_PATH = DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  feature-sellection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, product\n",
    "\n",
    "cat_cols = [['week_number'], ['hour'], ['dayofweek'],]\n",
    "\n",
    "num_cols = [['now_week'],\n",
    "            ['num_week_number'],\n",
    "            ['sin_num_week_number', 'cos_num_week_number'],\n",
    "            ['num_hour'],\n",
    "            ['sin_num_hour', 'cos_num_hour'],\n",
    "            ['num_dayofweek'],\n",
    "            ['sin_num_dayofweek', 'cos_num_dayofweek'],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_combinations_list = [['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']]\n",
    "for cnt in range(1, len(cat_cols) + 1):\n",
    "    for i in combinations(cat_cols, cnt):\n",
    "        cat_cols_combinations = ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
    "        for j in i:\n",
    "            if isinstance(j, list): cat_cols_combinations.extend(j)\n",
    "            else : cat_cols_combinations.append(j)\n",
    "            \n",
    "        cat_cols_combinations_list += [cat_cols_combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_combinations_list = [['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode']]\n",
    "for cnt in range(1, len(num_cols) + 1):\n",
    "    for i in combinations(num_cols, cnt):\n",
    "        num_cols_combinations = ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode']\n",
    "        for j in i:\n",
    "            if isinstance(j, list): num_cols_combinations.extend(j)\n",
    "            else : num_cols_combinations.append(j)\n",
    "        num_cols_combinations_list += [num_cols_combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_and_num_cols_combinations_list = list(product(cat_cols_combinations_list, num_cols_combinations_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.50756| roc_auc: 0.82033: 100%|██████████| 1/1 [01:54<00:00, 114.45s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46816| roc_auc: 0.84098: 100%|██████████| 1/1 [01:53<00:00, 113.82s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45429| roc_auc: 0.84726: 100%|██████████| 1/1 [01:53<00:00, 113.22s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44831| roc_auc: 0.85348: 100%|██████████| 1/1 [01:51<00:00, 111.42s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44370| roc_auc: 0.85476: 100%|██████████| 1/1 [01:54<00:00, 114.70s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44123| roc_auc: 0.85441: 100%|██████████| 1/1 [01:53<00:00, 113.71s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43887| roc_auc: 0.85293: 100%|██████████| 1/1 [01:53<00:00, 113.09s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43690| roc_auc: 0.85497: 100%|██████████| 1/1 [01:53<00:00, 113.36s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43538| roc_auc: 0.85426: 100%|██████████| 1/1 [01:52<00:00, 112.19s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43365| roc_auc: 0.85652: 100%|██████████| 1/1 [01:53<00:00, 113.03s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "cat_cols : ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
      "num_cols : ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode']\n",
      "BEST OOF-0| Epoch:  10| Train loss: 0.43365| roc_auc: 0.85652\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.50721| roc_auc: 0.82651: 100%|██████████| 1/1 [01:53<00:00, 113.11s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46710| roc_auc: 0.84505: 100%|██████████| 1/1 [01:52<00:00, 112.31s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45339| roc_auc: 0.85121: 100%|██████████| 1/1 [01:53<00:00, 113.01s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44703| roc_auc: 0.85472: 100%|██████████| 1/1 [01:52<00:00, 112.67s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44336| roc_auc: 0.85278: 100%|██████████| 1/1 [01:53<00:00, 113.98s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44041| roc_auc: 0.85523: 100%|██████████| 1/1 [01:53<00:00, 113.26s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43808| roc_auc: 0.85616: 100%|██████████| 1/1 [01:52<00:00, 112.80s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43625| roc_auc: 0.85838: 100%|██████████| 1/1 [01:51<00:00, 111.86s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43487| roc_auc: 0.85862: 100%|██████████| 1/1 [01:54<00:00, 114.01s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43316| roc_auc: 0.86180: 100%|██████████| 1/1 [01:53<00:00, 113.59s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "cat_cols : ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
      "num_cols : ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode', 'now_week']\n",
      "BEST OOF-0| Epoch:  10| Train loss: 0.43316| roc_auc: 0.86180\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.50721| roc_auc: 0.82711: 100%|██████████| 1/1 [01:52<00:00, 112.15s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46717| roc_auc: 0.84565: 100%|██████████| 1/1 [01:53<00:00, 113.30s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45356| roc_auc: 0.85185: 100%|██████████| 1/1 [01:52<00:00, 112.89s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44704| roc_auc: 0.85547: 100%|██████████| 1/1 [01:53<00:00, 113.24s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44341| roc_auc: 0.85355: 100%|██████████| 1/1 [01:52<00:00, 112.39s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44055| roc_auc: 0.85531: 100%|██████████| 1/1 [01:52<00:00, 112.66s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43821| roc_auc: 0.85750: 100%|██████████| 1/1 [01:53<00:00, 113.33s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43626| roc_auc: 0.85960: 100%|██████████| 1/1 [01:52<00:00, 112.60s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43473| roc_auc: 0.85882: 100%|██████████| 1/1 [01:53<00:00, 113.40s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43311| roc_auc: 0.86007: 100%|██████████| 1/1 [01:53<00:00, 113.65s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "cat_cols : ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
      "num_cols : ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode', 'num_week_number']\n",
      "BEST OOF-0| Epoch:  10| Train loss: 0.43311| roc_auc: 0.86007\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.51049| roc_auc: 0.82731: 100%|██████████| 1/1 [01:53<00:00, 113.69s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46779| roc_auc: 0.84595: 100%|██████████| 1/1 [01:53<00:00, 113.21s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45344| roc_auc: 0.85166: 100%|██████████| 1/1 [01:52<00:00, 112.65s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44724| roc_auc: 0.85427: 100%|██████████| 1/1 [01:53<00:00, 113.26s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44358| roc_auc: 0.85250: 100%|██████████| 1/1 [01:52<00:00, 112.40s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44074| roc_auc: 0.85444: 100%|██████████| 1/1 [01:53<00:00, 113.17s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43840| roc_auc: 0.85535: 100%|██████████| 1/1 [01:51<00:00, 111.66s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43642| roc_auc: 0.85580: 100%|██████████| 1/1 [01:52<00:00, 112.60s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43481| roc_auc: 0.85577: 100%|██████████| 1/1 [01:54<00:00, 114.12s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43315| roc_auc: 0.85590: 100%|██████████| 1/1 [01:53<00:00, 113.85s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "cat_cols : ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
      "num_cols : ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode', 'sin_num_week_number', 'cos_num_week_number']\n",
      "BEST OOF-0| Epoch:  10| Train loss: 0.43315| roc_auc: 0.85590\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.50740| roc_auc: 0.82733: 100%|██████████| 1/1 [01:53<00:00, 113.58s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46739| roc_auc: 0.84597: 100%|██████████| 1/1 [01:53<00:00, 113.89s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45361| roc_auc: 0.85239: 100%|██████████| 1/1 [01:53<00:00, 113.65s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44708| roc_auc: 0.85626: 100%|██████████| 1/1 [01:53<00:00, 113.90s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44348| roc_auc: 0.85424: 100%|██████████| 1/1 [01:53<00:00, 113.63s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44052| roc_auc: 0.85638: 100%|██████████| 1/1 [01:53<00:00, 113.47s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43818| roc_auc: 0.85892: 100%|██████████| 1/1 [01:53<00:00, 113.55s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43630| roc_auc: 0.86102: 100%|██████████| 1/1 [01:52<00:00, 112.54s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43474| roc_auc: 0.85977: 100%|██████████| 1/1 [01:53<00:00, 113.96s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43323| roc_auc: 0.86142: 100%|██████████| 1/1 [01:54<00:00, 114.36s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "cat_cols : ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
      "num_cols : ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode', 'num_hour']\n",
      "BEST OOF-0| Epoch:  10| Train loss: 0.43323| roc_auc: 0.86142\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.51044| roc_auc: 0.82618: 100%|██████████| 1/1 [01:53<00:00, 113.23s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46776| roc_auc: 0.84604: 100%|██████████| 1/1 [01:54<00:00, 114.91s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45338| roc_auc: 0.85160: 100%|██████████| 1/1 [01:53<00:00, 113.23s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44717| roc_auc: 0.85459: 100%|██████████| 1/1 [01:55<00:00, 115.14s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44351| roc_auc: 0.85294: 100%|██████████| 1/1 [01:52<00:00, 112.98s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44065| roc_auc: 0.85542: 100%|██████████| 1/1 [01:54<00:00, 114.13s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43825| roc_auc: 0.85601: 100%|██████████| 1/1 [01:54<00:00, 114.09s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43660| roc_auc: 0.85690: 100%|██████████| 1/1 [01:54<00:00, 114.66s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43492| roc_auc: 0.85728: 100%|██████████| 1/1 [01:53<00:00, 113.93s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43322| roc_auc: 0.85639: 100%|██████████| 1/1 [01:54<00:00, 114.77s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "cat_cols : ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
      "num_cols : ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode', 'sin_num_hour', 'cos_num_hour']\n",
      "BEST OOF-0| Epoch:   9| Train loss: 0.43492| roc_auc: 0.85728\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.50730| roc_auc: 0.82767: 100%|██████████| 1/1 [01:52<00:00, 112.56s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46731| roc_auc: 0.84598: 100%|██████████| 1/1 [01:53<00:00, 113.25s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45365| roc_auc: 0.85263: 100%|██████████| 1/1 [01:53<00:00, 113.19s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44710| roc_auc: 0.85634: 100%|██████████| 1/1 [01:52<00:00, 112.09s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44346| roc_auc: 0.85409: 100%|██████████| 1/1 [01:52<00:00, 112.43s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44059| roc_auc: 0.85579: 100%|██████████| 1/1 [01:52<00:00, 112.43s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43824| roc_auc: 0.85841: 100%|██████████| 1/1 [01:52<00:00, 112.68s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43631| roc_auc: 0.85917: 100%|██████████| 1/1 [01:52<00:00, 112.59s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43471| roc_auc: 0.85954: 100%|██████████| 1/1 [01:55<00:00, 115.22s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43329| roc_auc: 0.86117: 100%|██████████| 1/1 [01:52<00:00, 112.20s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "cat_cols : ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
      "num_cols : ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode', 'num_dayofweek']\n",
      "BEST OOF-0| Epoch:  10| Train loss: 0.43329| roc_auc: 0.86117\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.51053| roc_auc: 0.82704: 100%|██████████| 1/1 [01:52<00:00, 112.19s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46769| roc_auc: 0.84575: 100%|██████████| 1/1 [01:52<00:00, 112.21s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45333| roc_auc: 0.85095: 100%|██████████| 1/1 [01:52<00:00, 112.87s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44712| roc_auc: 0.85464: 100%|██████████| 1/1 [01:52<00:00, 112.80s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44417| roc_auc: 0.85255: 100%|██████████| 1/1 [01:52<00:00, 112.50s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44113| roc_auc: 0.85510: 100%|██████████| 1/1 [01:53<00:00, 113.55s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43853| roc_auc: 0.85406: 100%|██████████| 1/1 [01:53<00:00, 113.47s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43677| roc_auc: 0.85571: 100%|██████████| 1/1 [01:53<00:00, 113.41s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43522| roc_auc: 0.85622: 100%|██████████| 1/1 [01:53<00:00, 113.81s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43355| roc_auc: 0.85573: 100%|██████████| 1/1 [01:55<00:00, 115.05s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "cat_cols : ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
      "num_cols : ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode', 'sin_num_dayofweek', 'cos_num_dayofweek']\n",
      "BEST OOF-0| Epoch:   9| Train loss: 0.43522| roc_auc: 0.85622\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.50988| roc_auc: 0.82829: 100%|██████████| 1/1 [01:54<00:00, 114.83s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46736| roc_auc: 0.84665: 100%|██████████| 1/1 [01:55<00:00, 115.43s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45303| roc_auc: 0.85112: 100%|██████████| 1/1 [01:54<00:00, 114.32s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44689| roc_auc: 0.85350: 100%|██████████| 1/1 [01:55<00:00, 115.76s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44325| roc_auc: 0.85176: 100%|██████████| 1/1 [01:54<00:00, 114.52s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44042| roc_auc: 0.85414: 100%|██████████| 1/1 [01:54<00:00, 114.30s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43795| roc_auc: 0.85566: 100%|██████████| 1/1 [01:53<00:00, 113.83s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43594| roc_auc: 0.85587: 100%|██████████| 1/1 [01:55<00:00, 115.10s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43437| roc_auc: 0.85624: 100%|██████████| 1/1 [01:54<00:00, 114.12s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43272| roc_auc: 0.85567: 100%|██████████| 1/1 [01:55<00:00, 115.36s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "cat_cols : ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
      "num_cols : ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode', 'now_week', 'num_week_number']\n",
      "BEST OOF-0| Epoch:   9| Train loss: 0.43437| roc_auc: 0.85624\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.50705| roc_auc: 0.82931: 100%|██████████| 1/1 [01:54<00:00, 114.84s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46612| roc_auc: 0.84492: 100%|██████████| 1/1 [01:54<00:00, 114.99s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45268| roc_auc: 0.84942: 100%|██████████| 1/1 [01:54<00:00, 114.90s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44649| roc_auc: 0.85225: 100%|██████████| 1/1 [01:54<00:00, 114.25s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44292| roc_auc: 0.84912: 100%|██████████| 1/1 [01:55<00:00, 115.72s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44006| roc_auc: 0.85127: 100%|██████████| 1/1 [01:55<00:00, 115.24s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43793| roc_auc: 0.85362: 100%|██████████| 1/1 [01:54<00:00, 114.43s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43615| roc_auc: 0.85516: 100%|██████████| 1/1 [01:55<00:00, 115.05s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43420| roc_auc: 0.85559: 100%|██████████| 1/1 [01:53<00:00, 113.16s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43330| roc_auc: 0.85687: 100%|██████████| 1/1 [01:55<00:00, 115.27s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "cat_cols : ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
      "num_cols : ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode', 'now_week', 'sin_num_week_number', 'cos_num_week_number']\n",
      "BEST OOF-0| Epoch:  10| Train loss: 0.43330| roc_auc: 0.85687\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.50998| roc_auc: 0.82821: 100%|██████████| 1/1 [01:53<00:00, 113.71s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46752| roc_auc: 0.84677: 100%|██████████| 1/1 [01:54<00:00, 114.20s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45318| roc_auc: 0.85151: 100%|██████████| 1/1 [01:54<00:00, 114.11s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44720| roc_auc: 0.85332: 100%|██████████| 1/1 [01:54<00:00, 114.33s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44334| roc_auc: 0.85211: 100%|██████████| 1/1 [01:53<00:00, 113.69s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44051| roc_auc: 0.85432: 100%|██████████| 1/1 [01:54<00:00, 114.79s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43806| roc_auc: 0.85590: 100%|██████████| 1/1 [01:54<00:00, 114.96s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43613| roc_auc: 0.85635: 100%|██████████| 1/1 [01:54<00:00, 114.82s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43444| roc_auc: 0.85581: 100%|██████████| 1/1 [01:54<00:00, 114.62s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43280| roc_auc: 0.85641: 100%|██████████| 1/1 [01:52<00:00, 112.87s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "cat_cols : ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
      "num_cols : ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode', 'now_week', 'num_hour']\n",
      "BEST OOF-0| Epoch:  10| Train loss: 0.43280| roc_auc: 0.85641\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.50705| roc_auc: 0.82993: 100%|██████████| 1/1 [01:54<00:00, 114.99s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46603| roc_auc: 0.84546: 100%|██████████| 1/1 [01:53<00:00, 113.83s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45263| roc_auc: 0.84905: 100%|██████████| 1/1 [01:54<00:00, 114.87s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44652| roc_auc: 0.85236: 100%|██████████| 1/1 [01:55<00:00, 115.25s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44290| roc_auc: 0.84958: 100%|██████████| 1/1 [01:55<00:00, 115.41s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44007| roc_auc: 0.85230: 100%|██████████| 1/1 [01:55<00:00, 115.17s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43794| roc_auc: 0.85297: 100%|██████████| 1/1 [01:55<00:00, 115.22s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43636| roc_auc: 0.85515: 100%|██████████| 1/1 [01:55<00:00, 115.16s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43417| roc_auc: 0.85541: 100%|██████████| 1/1 [01:54<00:00, 114.05s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43314| roc_auc: 0.85638: 100%|██████████| 1/1 [01:54<00:00, 114.47s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "cat_cols : ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
      "num_cols : ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode', 'now_week', 'sin_num_hour', 'cos_num_hour']\n",
      "BEST OOF-0| Epoch:  10| Train loss: 0.43314| roc_auc: 0.85638\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.50989| roc_auc: 0.82822: 100%|██████████| 1/1 [01:52<00:00, 112.90s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46746| roc_auc: 0.84722: 100%|██████████| 1/1 [01:53<00:00, 113.73s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45324| roc_auc: 0.85104: 100%|██████████| 1/1 [01:53<00:00, 113.42s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44696| roc_auc: 0.85280: 100%|██████████| 1/1 [01:53<00:00, 113.16s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44328| roc_auc: 0.85196: 100%|██████████| 1/1 [01:53<00:00, 113.04s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44041| roc_auc: 0.85445: 100%|██████████| 1/1 [01:52<00:00, 112.87s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43794| roc_auc: 0.85579: 100%|██████████| 1/1 [01:53<00:00, 113.25s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43590| roc_auc: 0.85544: 100%|██████████| 1/1 [01:55<00:00, 115.37s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43462| roc_auc: 0.85591: 100%|██████████| 1/1 [01:52<00:00, 112.87s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43296| roc_auc: 0.85625: 100%|██████████| 1/1 [01:53<00:00, 113.83s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "cat_cols : ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
      "num_cols : ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode', 'now_week', 'num_dayofweek']\n",
      "BEST OOF-0| Epoch:  10| Train loss: 0.43296| roc_auc: 0.85625\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.50703| roc_auc: 0.82931: 100%|██████████| 1/1 [01:55<00:00, 115.05s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46610| roc_auc: 0.84545: 100%|██████████| 1/1 [01:53<00:00, 113.61s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45275| roc_auc: 0.84935: 100%|██████████| 1/1 [01:53<00:00, 113.72s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44663| roc_auc: 0.85247: 100%|██████████| 1/1 [01:54<00:00, 114.36s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44293| roc_auc: 0.84970: 100%|██████████| 1/1 [01:54<00:00, 114.16s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44015| roc_auc: 0.85271: 100%|██████████| 1/1 [01:54<00:00, 114.45s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43808| roc_auc: 0.85394: 100%|██████████| 1/1 [01:53<00:00, 113.44s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43625| roc_auc: 0.85561: 100%|██████████| 1/1 [01:54<00:00, 114.48s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43412| roc_auc: 0.85571: 100%|██████████| 1/1 [01:53<00:00, 113.95s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43312| roc_auc: 0.85715: 100%|██████████| 1/1 [01:54<00:00, 114.82s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "cat_cols : ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
      "num_cols : ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode', 'now_week', 'sin_num_dayofweek', 'cos_num_dayofweek']\n",
      "BEST OOF-0| Epoch:  10| Train loss: 0.43312| roc_auc: 0.85715\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.50727| roc_auc: 0.83067: 100%|██████████| 1/1 [01:54<00:00, 114.26s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46606| roc_auc: 0.84680: 100%|██████████| 1/1 [01:54<00:00, 114.91s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45274| roc_auc: 0.85030: 100%|██████████| 1/1 [01:53<00:00, 113.47s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44686| roc_auc: 0.85368: 100%|██████████| 1/1 [01:53<00:00, 113.12s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44312| roc_auc: 0.85108: 100%|██████████| 1/1 [01:55<00:00, 115.01s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44027| roc_auc: 0.85420: 100%|██████████| 1/1 [01:54<00:00, 114.21s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43805| roc_auc: 0.85558: 100%|██████████| 1/1 [01:53<00:00, 113.29s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43643| roc_auc: 0.85660: 100%|██████████| 1/1 [01:53<00:00, 113.49s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43464| roc_auc: 0.85658: 100%|██████████| 1/1 [01:54<00:00, 114.22s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43345| roc_auc: 0.85818: 100%|██████████| 1/1 [01:54<00:00, 114.40s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "cat_cols : ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
      "num_cols : ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode', 'num_week_number', 'sin_num_week_number', 'cos_num_week_number']\n",
      "BEST OOF-0| Epoch:  10| Train loss: 0.43345| roc_auc: 0.85818\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.51026| roc_auc: 0.82815: 100%|██████████| 1/1 [01:53<00:00, 113.53s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46753| roc_auc: 0.84668: 100%|██████████| 1/1 [01:53<00:00, 113.47s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45326| roc_auc: 0.85196: 100%|██████████| 1/1 [01:54<00:00, 114.29s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44710| roc_auc: 0.85439: 100%|██████████| 1/1 [01:54<00:00, 114.66s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44347| roc_auc: 0.85302: 100%|██████████| 1/1 [01:53<00:00, 113.33s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44066| roc_auc: 0.85487: 100%|██████████| 1/1 [01:54<00:00, 114.02s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43827| roc_auc: 0.85697: 100%|██████████| 1/1 [01:54<00:00, 114.19s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43639| roc_auc: 0.85753: 100%|██████████| 1/1 [01:54<00:00, 114.31s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43477| roc_auc: 0.85710: 100%|██████████| 1/1 [01:54<00:00, 114.16s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43304| roc_auc: 0.85602: 100%|██████████| 1/1 [01:53<00:00, 113.88s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "cat_cols : ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
      "num_cols : ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode', 'num_week_number', 'num_hour']\n",
      "BEST OOF-0| Epoch:   8| Train loss: 0.43639| roc_auc: 0.85753\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.50726| roc_auc: 0.83077: 100%|██████████| 1/1 [01:53<00:00, 113.90s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46607| roc_auc: 0.84588: 100%|██████████| 1/1 [01:54<00:00, 114.00s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45274| roc_auc: 0.84892: 100%|██████████| 1/1 [01:54<00:00, 114.58s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44674| roc_auc: 0.85336: 100%|██████████| 1/1 [01:54<00:00, 114.22s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44309| roc_auc: 0.85057: 100%|██████████| 1/1 [01:54<00:00, 114.84s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44019| roc_auc: 0.85377: 100%|██████████| 1/1 [01:55<00:00, 115.24s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43801| roc_auc: 0.85453: 100%|██████████| 1/1 [01:53<00:00, 113.73s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43617| roc_auc: 0.85517: 100%|██████████| 1/1 [01:54<00:00, 114.94s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43436| roc_auc: 0.85663: 100%|██████████| 1/1 [01:55<00:00, 115.20s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43324| roc_auc: 0.85694: 100%|██████████| 1/1 [01:55<00:00, 115.12s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "cat_cols : ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
      "num_cols : ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode', 'num_week_number', 'sin_num_hour', 'cos_num_hour']\n",
      "BEST OOF-0| Epoch:  10| Train loss: 0.43324| roc_auc: 0.85694\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.51025| roc_auc: 0.82880: 100%|██████████| 1/1 [01:55<00:00, 115.01s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46759| roc_auc: 0.84755: 100%|██████████| 1/1 [01:58<00:00, 118.88s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45349| roc_auc: 0.85173: 100%|██████████| 1/1 [01:57<00:00, 117.75s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44728| roc_auc: 0.85399: 100%|██████████| 1/1 [01:58<00:00, 118.51s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44361| roc_auc: 0.85330: 100%|██████████| 1/1 [01:57<00:00, 117.38s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44070| roc_auc: 0.85510: 100%|██████████| 1/1 [01:58<00:00, 118.06s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43830| roc_auc: 0.85673: 100%|██████████| 1/1 [01:58<00:00, 118.33s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43652| roc_auc: 0.85756: 100%|██████████| 1/1 [01:57<00:00, 117.10s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43480| roc_auc: 0.85634: 100%|██████████| 1/1 [01:57<00:00, 117.88s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43317| roc_auc: 0.85562: 100%|██████████| 1/1 [01:55<00:00, 115.56s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "cat_cols : ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx']\n",
      "num_cols : ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode', 'num_week_number', 'num_dayofweek']\n",
      "BEST OOF-0| Epoch:   8| Train loss: 0.43652| roc_auc: 0.85756\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.50726| roc_auc: 0.83075: 100%|██████████| 1/1 [01:57<00:00, 117.37s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "oof = 0\n",
    "\n",
    "train_df, valid_df = make_dataset.get_oof_data(oof)\n",
    "\n",
    "for cat_cols, num_cols in cat_cols_and_num_cols_combinations_list:\n",
    "\n",
    "    seed_everything(22 + oof)\n",
    "\n",
    "    train_dataset = CustomDataset(df = train_df, cat_cols = cat_cols, num_cols = num_cols)\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size = batch_size, \n",
    "        shuffle = True, \n",
    "        drop_last = False,\n",
    "        collate_fn = train_make_batch,\n",
    "        num_workers = num_workers)\n",
    "\n",
    "    valid_dataset = CustomDataset(df = valid_df, cat_cols = cat_cols, num_cols = num_cols)\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = 1, \n",
    "        shuffle = False, \n",
    "        drop_last = False,\n",
    "        collate_fn = train_make_batch,\n",
    "        num_workers = num_workers)\n",
    "\n",
    "    model = SASRec(\n",
    "        num_assessmentItemID = make_dataset.num_assessmentItemID, \n",
    "        num_testId = make_dataset.num_testId,\n",
    "        num_KnowledgeTag = make_dataset.num_KnowledgeTag,\n",
    "        num_large_paper_number = make_dataset.num_large_paper_number,\n",
    "        num_hour = make_dataset.num_hour,\n",
    "        num_dayofweek = make_dataset.num_dayofweek,\n",
    "        num_week_number = make_dataset.num_week_number,\n",
    "        num_cols = train_dataset.num_cols,\n",
    "        cat_cols = train_dataset.cat_cols,\n",
    "        emb_size = emb_size,\n",
    "        hidden_units = hidden_units,\n",
    "        num_heads = num_heads,\n",
    "        num_layers = num_layers,\n",
    "        dropout_rate = dropout_rate,\n",
    "        device = device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    best_epoch = 0\n",
    "    best_train_loss = 0\n",
    "    best_roc_auc = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tbar = tqdm(range(1))\n",
    "        for _ in tbar:\n",
    "            train_loss = train(model = model, data_loader = train_data_loader, criterion = criterion, optimizer = optimizer)\n",
    "            roc_auc = evaluate(model = model, data_loader = valid_data_loader)\n",
    "            if best_roc_auc < roc_auc:\n",
    "                best_epoch = epoch\n",
    "                best_train_loss = train_loss\n",
    "                best_roc_auc = roc_auc\n",
    "\n",
    "            tbar.set_description(f'OOF-{oof}| Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| roc_auc: {roc_auc:.5f}')\n",
    "\n",
    "    print('-' * 20)\n",
    "    print('cat_cols :', cat_cols)\n",
    "    print('num_cols :', num_cols)\n",
    "    print(f'BEST OOF-{oof}| Epoch: {best_epoch:3d}| Train loss: {best_train_loss:.5f}| roc_auc: {best_roc_auc:.5f}')\n",
    "    print('-' * 20)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
