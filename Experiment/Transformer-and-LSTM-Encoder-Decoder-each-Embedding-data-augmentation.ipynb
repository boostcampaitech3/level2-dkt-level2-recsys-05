{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "torch.set_printoptions(sci_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDataset():\n",
    "\n",
    "    def __init__(self, DATA_PATH):\n",
    "        self.preporcessing(DATA_PATH)\n",
    "        self.oof_user_set = self.split_data()\n",
    "    \n",
    "    def split_data(self):\n",
    "        user_list = self.all_df['userID'].unique().tolist()\n",
    "        oof_user_set = {}\n",
    "        kf = KFold(n_splits = 5, random_state = 22, shuffle = True)\n",
    "        for idx, (train_user, valid_user) in enumerate(kf.split(user_list)):\n",
    "            oof_user_set[idx] = valid_user.tolist()\n",
    "        \n",
    "        return oof_user_set\n",
    "\n",
    "    def preporcessing(self, DATA_PATH):\n",
    "\n",
    "        dtype = {\n",
    "            'userID': 'int16',\n",
    "            'answerCode': 'int8',\n",
    "            'KnowledgeTag': 'int16'\n",
    "        }\n",
    "        \n",
    "        train_df = pd.read_csv(os.path.join(DATA_PATH, 'train_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "        train_df = train_df.sort_values(by=['userID', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "        test_df = pd.read_csv(os.path.join(DATA_PATH, 'test_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "        test_df = test_df.sort_values(by=['userID', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        def get_large_paper_number(x):\n",
    "            return x[1:4]\n",
    "        \n",
    "        train_df['large_paper_number'] = train_df['assessmentItemID'].apply(lambda x : get_large_paper_number(x))\n",
    "        test_df['large_paper_number'] = test_df['assessmentItemID'].apply(lambda x : get_large_paper_number(x))\n",
    "\n",
    "        # 문제 푸는데 걸린 시간\n",
    "        def get_now_elapsed(df):\n",
    "            \n",
    "            diff = df.loc[:, ['userID','Timestamp']].groupby('userID').diff().fillna(pd.Timedelta(seconds=0))\n",
    "            diff = diff.fillna(pd.Timedelta(seconds=0))\n",
    "            diff = diff['Timestamp'].apply(lambda x: x.total_seconds())\n",
    "            df['now_elapsed'] = diff\n",
    "            df['now_elapsed'] = df['now_elapsed'].apply(lambda x : x if x < 650 and x >=0 else 0)\n",
    "            df['now_elapsed'] = df['now_elapsed']\n",
    "\n",
    "            return df\n",
    "\n",
    "        train_df = get_now_elapsed(df = train_df)\n",
    "        test_df = get_now_elapsed(df = test_df)\n",
    "\n",
    "        all_df = pd.concat([train_df, test_df])\n",
    "        all_df = all_df[all_df['answerCode'] != -1].reset_index(drop = True)\n",
    "\n",
    "        # normalize_score\n",
    "        def get_normalize_score(df, all_df):\n",
    "            ret_df = []\n",
    "\n",
    "            group_df = df.groupby('userID')\n",
    "            mean_answerCode_df = all_df.groupby('testId').mean()['answerCode']\n",
    "            std_answerCode_df = all_df.groupby('testId').std()['answerCode']\n",
    "            for userID, get_df in group_df:\n",
    "                normalize_score_df = (get_df[get_df['answerCode'] != -1].groupby('testId').mean()['answerCode'] - mean_answerCode_df) / std_answerCode_df\n",
    "                get_df = get_df.copy().set_index('testId')\n",
    "                get_df['normalize_score'] = normalize_score_df\n",
    "                ret_df.append(get_df.reset_index(drop = False))\n",
    "\n",
    "            ret_df = pd.concat(ret_df).reset_index(drop = True)\n",
    "            \n",
    "            return ret_df\n",
    "        \n",
    "        # train_df = get_normalize_score(df = train_df, all_df = all_df)\n",
    "        # test_df = get_normalize_score(df = test_df, all_df = all_df)\n",
    "\n",
    "        # 문항별 정답률\n",
    "        train_df = train_df.set_index('assessmentItemID')\n",
    "        train_df['assessmentItemID_mean_answerCode'] = all_df.groupby('assessmentItemID').mean()['answerCode']\n",
    "        train_df = train_df.reset_index(drop = False)\n",
    "\n",
    "        test_df = test_df.set_index('assessmentItemID')\n",
    "        test_df['assessmentItemID_mean_answerCode'] = all_df.groupby('assessmentItemID').mean()['answerCode']\n",
    "        test_df = test_df.reset_index(drop = False)\n",
    "\n",
    "        # 문항별 정답률 표준편차\n",
    "        train_df = train_df.set_index('assessmentItemID')\n",
    "        train_df['assessmentItemID_std_answerCode'] = all_df.groupby('assessmentItemID').std()['answerCode']\n",
    "        train_df = train_df.reset_index(drop = False)\n",
    "\n",
    "        test_df = test_df.set_index('assessmentItemID')\n",
    "        test_df['assessmentItemID_std_answerCode'] = all_df.groupby('assessmentItemID').std()['answerCode']\n",
    "        test_df = test_df.reset_index(drop = False)\n",
    "\n",
    "        # 올바르게 푼 사람들의 문항별 풀이 시간 평균\n",
    "        train_df = train_df.set_index('assessmentItemID')\n",
    "        train_df['assessmentItemID_mean_now_elapsed'] = all_df[all_df['answerCode'] == 1].groupby('assessmentItemID').mean()['now_elapsed']\n",
    "        train_df = train_df.reset_index(drop = False)\n",
    "\n",
    "        test_df = test_df.set_index('assessmentItemID')\n",
    "        test_df['assessmentItemID_mean_now_elapsed'] = all_df[all_df['answerCode'] == 1].groupby('assessmentItemID').mean()['now_elapsed']\n",
    "        test_df = test_df.reset_index(drop = False)\n",
    "\n",
    "        # 올바르게 푼 사람들의 문항별 풀이 시간 표준 편차\n",
    "        train_df = train_df.set_index('assessmentItemID')\n",
    "        train_df['assessmentItemID_std_now_elapsed'] = all_df[all_df['answerCode'] == 1].groupby('assessmentItemID').std()['now_elapsed']\n",
    "        train_df = train_df.reset_index(drop = False)\n",
    "\n",
    "        test_df = test_df.set_index('assessmentItemID')\n",
    "        test_df['assessmentItemID_std_now_elapsed'] = all_df[all_df['answerCode'] == 1].groupby('assessmentItemID').std()['now_elapsed']\n",
    "        test_df = test_df.reset_index(drop = False)\n",
    "\n",
    "        # 문제 푼 시간\n",
    "        train_df['hour'] = train_df['Timestamp'].dt.hour\n",
    "        test_df['hour'] = test_df['Timestamp'].dt.hour\n",
    "\n",
    "        # 문제 푼 요일\n",
    "        train_df['dayofweek'] = train_df['Timestamp'].dt.dayofweek\n",
    "        test_df['dayofweek'] = test_df['Timestamp'].dt.dayofweek\n",
    "\n",
    "        # index 로 변환\n",
    "\n",
    "        def get_val2idx(val_list : list) -> dict:\n",
    "            val2idx = {}\n",
    "            for idx, val in enumerate(val_list):\n",
    "                val2idx[val] = idx\n",
    "            \n",
    "            return val2idx\n",
    "\n",
    "        assessmentItemID2idx = get_val2idx(all_df['assessmentItemID'].unique().tolist())\n",
    "        testId2idx = get_val2idx(all_df['testId'].unique().tolist())\n",
    "        KnowledgeTag2idx = get_val2idx(all_df['KnowledgeTag'].unique().tolist())\n",
    "        large_paper_number2idx = get_val2idx(all_df['large_paper_number'].unique().tolist())\n",
    "\n",
    "        train_df['assessmentItemID2idx'] = train_df['assessmentItemID'].apply(lambda x : assessmentItemID2idx[x])\n",
    "        train_df['testId2idx'] = train_df['testId'].apply(lambda x : testId2idx[x])\n",
    "        train_df['KnowledgeTag2idx'] = train_df['KnowledgeTag'].apply(lambda x : KnowledgeTag2idx[x])\n",
    "        train_df['large_paper_number2idx'] = train_df['large_paper_number'].apply(lambda x : large_paper_number2idx[x])\n",
    "\n",
    "        test_df['assessmentItemID2idx'] = test_df['assessmentItemID'].apply(lambda x : assessmentItemID2idx[x])\n",
    "        test_df['testId2idx'] = test_df['testId'].apply(lambda x : testId2idx[x])\n",
    "        test_df['KnowledgeTag2idx'] = test_df['KnowledgeTag'].apply(lambda x : KnowledgeTag2idx[x])\n",
    "        test_df['large_paper_number2idx'] = test_df['large_paper_number'].apply(lambda x : large_paper_number2idx[x])\n",
    "\n",
    "        self.assessmentItemID2idx = assessmentItemID2idx\n",
    "        self.train_df, self.test_df = train_df, test_df\n",
    "        self.all_df = pd.concat([train_df, test_df[test_df['answerCode'] != -1]]).reset_index(drop=True)\n",
    "        self.num_assessmentItemID = len(assessmentItemID2idx)\n",
    "        self.num_testId = len(testId2idx)\n",
    "        self.num_KnowledgeTag = len(KnowledgeTag2idx)\n",
    "        self.num_large_paper_number = len(large_paper_number2idx)\n",
    "        self.num_hour = 24\n",
    "        self.num_dayofweek = 7\n",
    "\n",
    "    def get_oof_data(self, oof):\n",
    "\n",
    "        val_user_list = self.oof_user_set[oof]\n",
    "\n",
    "        train = []\n",
    "        valid = []\n",
    "\n",
    "        group_df = self.all_df.groupby('userID')\n",
    "\n",
    "        for userID, df in group_df:\n",
    "            if userID in val_user_list:\n",
    "                trn_df = df.iloc[:-1, :]\n",
    "                val_df = df.copy()\n",
    "                train.append(trn_df)\n",
    "                valid.append(val_df)\n",
    "            else:\n",
    "                train.append(df)\n",
    "\n",
    "        # normalize_score\n",
    "        def get_normalize_score(df, all_df, vailid = False):\n",
    "            ret_df = []\n",
    "\n",
    "            group_df = df.groupby('userID')\n",
    "            mean_answerCode_df = all_df.groupby('testId').mean()['answerCode']\n",
    "            std_answerCode_df = all_df.groupby('testId').std()['answerCode']\n",
    "            for userID, get_df in group_df:\n",
    "                if vailid:\n",
    "                    normalize_score_df = (get_df.iloc[:-1, :].groupby('testId').mean()['answerCode'] - mean_answerCode_df) / std_answerCode_df\n",
    "                else:\n",
    "                    normalize_score_df = (get_df.groupby('testId').mean()['answerCode'] - mean_answerCode_df) / std_answerCode_df\n",
    "                    \n",
    "                get_df = get_df.copy().set_index('testId')\n",
    "                get_df['normalize_score'] = normalize_score_df\n",
    "                ret_df.append(get_df.reset_index(drop = False))\n",
    "\n",
    "            ret_df = pd.concat(ret_df).reset_index(drop = True)\n",
    "            \n",
    "            return ret_df\n",
    "\n",
    "        train = pd.concat(train).reset_index(drop = True)\n",
    "        valid = pd.concat(valid).reset_index(drop = True)\n",
    "\n",
    "        # train = get_normalize_score(df = train, all_df = train)\n",
    "        # valid = get_normalize_score(df = valid, all_df = train, vailid = True)\n",
    "        \n",
    "        return train, valid\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        return self.test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        df,\n",
    "        cat_cols = ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx', 'hour', 'dayofweek'],\n",
    "        num_cols = ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode'],\n",
    "        max_len = None,\n",
    "        window = None,\n",
    "        data_augmentation = False,\n",
    "        ):\n",
    "\n",
    "        self.cat_cols = cat_cols\n",
    "        self.num_cols = num_cols\n",
    "        self.get_df = df.groupby('userID')\n",
    "        self.user_list = df['userID'].unique().tolist()\n",
    "        self.max_len = max_len\n",
    "        self.window = window\n",
    "        self.data_augmentation = data_augmentation\n",
    "        if self.data_augmentation:\n",
    "            self.cat_feature_list, self.num_feature_list, self.answerCode_list = self._data_augmentation()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.data_augmentation:\n",
    "            return len(self.cat_feature_list)\n",
    "        return len(self.user_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.data_augmentation:\n",
    "            cat_feature = self.cat_feature_list[idx]\n",
    "            num_feature = self.num_feature_list[idx]\n",
    "            answerCode = self.answerCode_list[idx]\n",
    "\n",
    "            now_cat_feature = cat_feature[1:, :]\n",
    "            now_num_feature = num_feature[1:, :]\n",
    "            now_answerCode = answerCode[1:]\n",
    "            \n",
    "            past_cat_feature = cat_feature[:-1, :]\n",
    "            past_num_feature = num_feature[:-1, :]\n",
    "            past_answerCode = answerCode[:-1]\n",
    "            \n",
    "        else:\n",
    "            user = self.user_list[idx]\n",
    "            if self.max_len:\n",
    "                get_df = self.get_df.get_group(user).iloc[-self.max_len:, :]\n",
    "            else:\n",
    "                get_df = self.get_df.get_group(user)\n",
    "\n",
    "            now_df = get_df.iloc[1:, :]\n",
    "            now_cat_feature = now_df[self.cat_cols].values\n",
    "            now_num_feature = now_df[self.num_cols].values\n",
    "            now_answerCode = now_df['answerCode'].values\n",
    "\n",
    "            past_df = get_df.iloc[:-1, :]\n",
    "            past_cat_feature = past_df[self.cat_cols].values\n",
    "            past_num_feature = past_df[self.num_cols].values\n",
    "            past_answerCode = past_df['answerCode'].values\n",
    "\n",
    "        return {\n",
    "            'past_cat_feature' : past_cat_feature, \n",
    "            'past_num_feature' : past_num_feature, \n",
    "            'past_answerCode' : past_answerCode, \n",
    "            'now_cat_feature' : now_cat_feature, \n",
    "            'now_num_feature' : now_num_feature, \n",
    "            'now_answerCode' : now_answerCode\n",
    "            }\n",
    "    \n",
    "\n",
    "    def _data_augmentation(self):\n",
    "        cat_feature_list = []\n",
    "        num_feature_list = []\n",
    "        answerCode_list = []\n",
    "        for userID, get_df in tqdm(self.get_df):\n",
    "            cat_feature = get_df[self.cat_cols].values[::-1]\n",
    "            num_feature = get_df[self.num_cols].values[::-1]\n",
    "            answerCode = get_df['answerCode'].values[::-1]\n",
    "\n",
    "            start_idx = 0\n",
    "\n",
    "            if len(get_df) <= self.max_len:\n",
    "                cat_feature_list.append(cat_feature[::-1])\n",
    "                num_feature_list.append(num_feature[::-1])\n",
    "                answerCode_list.append(answerCode[::-1])\n",
    "            else:\n",
    "                while True:\n",
    "                    if len(cat_feature[start_idx: start_idx + self.max_len, :]) < self.max_len:\n",
    "                        cat_feature_list.append(cat_feature[start_idx: start_idx + self.max_len, :][::-1])\n",
    "                        num_feature_list.append(num_feature[start_idx: start_idx + self.max_len, :][::-1])\n",
    "                        answerCode_list.append(answerCode[start_idx: start_idx + self.max_len][::-1])\n",
    "                        break\n",
    "                    cat_feature_list.append(cat_feature[start_idx: start_idx + self.max_len, :][::-1])\n",
    "                    num_feature_list.append(num_feature[start_idx: start_idx + self.max_len, :][::-1])\n",
    "                    answerCode_list.append(answerCode[start_idx: start_idx + self.max_len][::-1])\n",
    "                    start_idx += self.window\n",
    "            \n",
    "        return cat_feature_list, num_feature_list, answerCode_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(seq, max_len, padding_value = 0):\n",
    "    try:\n",
    "        seq_len, col = seq.shape\n",
    "        padding = np.zeros((max_len - seq_len, col)) + padding_value\n",
    "    except:\n",
    "        seq_len = seq.shape[0]\n",
    "        padding = np.zeros((max_len - seq_len, )) + padding_value\n",
    "\n",
    "    padding_seq = np.concatenate([padding, seq])\n",
    "\n",
    "    return padding_seq\n",
    "\n",
    "def train_make_batch(samples):\n",
    "    max_len = 0\n",
    "    for sample in samples:\n",
    "        seq_len, col = sample['past_cat_feature'].shape\n",
    "        if max_len < seq_len:\n",
    "            max_len = seq_len\n",
    "    \n",
    "    past_cat_feature = []\n",
    "    past_num_feature = []\n",
    "    past_answerCode = []\n",
    "    now_cat_feature = []\n",
    "    now_num_feature = []\n",
    "    now_answerCode = []\n",
    "\n",
    "    for sample in samples:\n",
    "        past_cat_feature += [pad_sequence(sample['past_cat_feature'] + 1, max_len = max_len, padding_value = 0)]\n",
    "        past_num_feature += [pad_sequence(sample['past_num_feature'], max_len = max_len, padding_value = 0)]\n",
    "        past_answerCode += [pad_sequence(sample['past_answerCode'] + 1, max_len = max_len, padding_value = 0)]\n",
    "        now_cat_feature += [pad_sequence(sample['now_cat_feature'] + 1, max_len = max_len, padding_value = 0)]\n",
    "        now_num_feature += [pad_sequence(sample['now_num_feature'], max_len = max_len, padding_value = 0)]\n",
    "        now_answerCode += [pad_sequence(sample['now_answerCode'], max_len = max_len, padding_value = -1)]\n",
    "\n",
    "    return torch.tensor(past_cat_feature, dtype = torch.long), torch.tensor(past_num_feature, dtype = torch.float32), torch.tensor(past_answerCode, dtype = torch.long), torch.tensor(now_cat_feature, dtype = torch.long), torch.tensor(now_num_feature, dtype = torch.float32), torch.tensor(now_answerCode, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        \"\"\"\n",
    "        Q, K, V : (batch_size, num_heads, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \"\"\"\n",
    "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units) # (batch_size, num_heads, max_len, max_len)\n",
    "        attn_score = attn_score.masked_fill(mask == 0, -1e9)  # 유사도가 0인 지점은 -infinity로 보내 softmax 결과가 0이 되도록 함\n",
    "        attn_dist = self.dropout(F.softmax(attn_score, dim=-1))  # attention distribution\n",
    "        output = torch.matmul(attn_dist, V)  # (batch_size, num_heads, max_len, hidden_units) / # dim of output : batchSize x num_head x seqLen x hidden_units\n",
    "        return output, attn_dist\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads # head의 수\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        # query, key, value, output 생성을 위해 Linear 모델 생성\n",
    "        self.W_Q = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_K = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_V = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_O = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(hidden_units, dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, enc, mask):\n",
    "        \"\"\"\n",
    "        enc : (batch_size, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \n",
    "        \"\"\"\n",
    "        residual = enc # residual connection을 위해 residual 부분을 저장\n",
    "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
    "\n",
    "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
    "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units // self.num_heads) # (batch_size, max_len, num_heads, hidden_units)\n",
    "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units // self.num_heads) # (batch_size, max_len, num_heads, hidden_units)\n",
    "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units // self.num_heads) # (batch_size, max_len, num_heads, hidden_units)\n",
    "\n",
    "        # Head별로 각기 다른 attention이 가능하도록 Transpose 후 각각 attention에 통과시킴\n",
    "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2) # (batch_size, num_heads, max_len, hidden_units)\n",
    "        output, attn_dist = self.attention(Q, K, V, mask) # output : (batch_size, num_heads, max_len, hidden_units) / attn_dist : (batch_size, num_heads, max_len, max_len)\n",
    "\n",
    "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n",
    "        output = output.transpose(1, 2).contiguous() # (batch_size, max_len, num_heads, hidden_units) / contiguous() : 가변적 메모리 할당\n",
    "        output = output.view(batch_size, seqlen, -1) # (batch_size, max_len, hidden_units * num_heads)\n",
    "\n",
    "        # Linear Projection, Dropout, Residual sum, and Layer Normalization\n",
    "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual) # (batch_size, max_len, hidden_units)\n",
    "        return output, attn_dist\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "\n",
    "        self.W_1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.W_2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = self.W_2(F.relu(self.dropout(self.W_1(x))))\n",
    "        output = self.layerNorm(self.dropout(output) + residual)\n",
    "        return output\n",
    "\n",
    "\n",
    "class SASRecBlock(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(SASRecBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads, hidden_units, dropout_rate)\n",
    "        self.pointwise_feedforward = PositionwiseFeedForward(hidden_units, dropout_rate)\n",
    "\n",
    "    def forward(self, input_enc, mask):\n",
    "        \"\"\"\n",
    "        input_enc : (batch_size, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \"\"\"\n",
    "        output_enc, attn_dist = self.attention(input_enc, mask)\n",
    "        output_enc = self.pointwise_feedforward(output_enc)\n",
    "        return output_enc, attn_dist\n",
    "\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_assessmentItemID, \n",
    "        num_testId,\n",
    "        num_KnowledgeTag,\n",
    "        num_large_paper_number,\n",
    "        num_hour,\n",
    "        num_dayofweek,\n",
    "        num_cols,\n",
    "        cat_cols,\n",
    "        emb_size,\n",
    "        hidden_units,\n",
    "        num_heads, \n",
    "        num_layers, \n",
    "        dropout_rate, \n",
    "        device):\n",
    "        super(SASRec, self).__init__()\n",
    "\n",
    "        # past\n",
    "        self.past_assessmentItemID_emb = nn.Embedding(num_assessmentItemID + 1, emb_size, padding_idx = 0) # 문항에 대한 정보\n",
    "        self.past_testId_emb = nn.Embedding(num_testId + 1, emb_size, padding_idx = 0) # 시험지에 대한 정보\n",
    "        self.past_KnowledgeTag_emb = nn.Embedding(num_KnowledgeTag + 1, emb_size, padding_idx = 0) # 지식 태그에 대한 정보\n",
    "        self.past_large_paper_number_emb = nn.Embedding(num_large_paper_number + 1, emb_size, padding_idx = 0) # 핫년에 대한 정보\n",
    "        self.past_hour_emb = nn.Embedding(num_hour + 1, emb_size, padding_idx = 0) # 문제 풀이 시간에 대한 정보\n",
    "        self.past_dayofweek_emb = nn.Embedding(num_dayofweek + 1, emb_size, padding_idx = 0) # 문제 풀이 요일에 대항 정보\n",
    "        self.past_answerCode_emb = nn.Embedding(3, hidden_units, padding_idx = 0) # 문제 정답 여부에 대한 정보\n",
    "\n",
    "        self.past_cat_emb = nn.Sequential(\n",
    "            nn.Linear(len(cat_cols) * emb_size, hidden_units // 2),\n",
    "            nn.LayerNorm(hidden_units // 2, eps=1e-6)\n",
    "        )\n",
    "\n",
    "        self.past_num_emb = nn.Sequential(\n",
    "            nn.Linear(len(num_cols), hidden_units // 2),\n",
    "            nn.LayerNorm(hidden_units // 2, eps=1e-6)\n",
    "        )\n",
    "\n",
    "        self.emb_layernorm = nn.LayerNorm(hidden_units, eps=1e-6)\n",
    "\n",
    "        self.past_lstm = nn.LSTM(\n",
    "            input_size = hidden_units,\n",
    "            hidden_size = hidden_units,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            bidirectional = False,\n",
    "            dropout = dropout_rate,\n",
    "            )\n",
    "\n",
    "        self.past_blocks = nn.ModuleList([SASRecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
    "\n",
    "        # now\n",
    "        self.now_assessmentItemID_emb = nn.Embedding(num_assessmentItemID + 1, emb_size, padding_idx = 0) # 문항에 대한 정보\n",
    "        self.now_testId_emb = nn.Embedding(num_testId + 1, emb_size, padding_idx = 0) # 시험지에 대한 정보\n",
    "        self.now_KnowledgeTag_emb = nn.Embedding(num_KnowledgeTag + 1, emb_size, padding_idx = 0) # 지식 태그에 대한 정보\n",
    "        self.now_large_paper_number_emb = nn.Embedding(num_large_paper_number + 1, emb_size, padding_idx = 0) # 핫년에 대한 정보\n",
    "        self.now_hour_emb = nn.Embedding(num_hour + 1, emb_size, padding_idx = 0) # 문제 풀이 시간에 대한 정보\n",
    "        self.now_dayofweek_emb = nn.Embedding(num_dayofweek + 1, emb_size, padding_idx = 0) # 문제 풀이 요일에 대항 정보\n",
    "\n",
    "        self.now_cat_emb = nn.Sequential(\n",
    "            nn.Linear(len(cat_cols) * emb_size, hidden_units // 2),\n",
    "            nn.LayerNorm(hidden_units // 2, eps=1e-6)\n",
    "        )\n",
    "\n",
    "        self.now_num_emb = nn.Sequential(\n",
    "            nn.Linear(len(num_cols), hidden_units // 2),\n",
    "            nn.LayerNorm(hidden_units // 2, eps=1e-6)\n",
    "        )\n",
    "\n",
    "        self.now_lstm = nn.LSTM(\n",
    "            input_size = hidden_units,\n",
    "            hidden_size = hidden_units,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            bidirectional = False,\n",
    "            dropout = dropout_rate,\n",
    "            )\n",
    "\n",
    "        self.now_blocks = nn.ModuleList([SASRecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
    "\n",
    "        # predict\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.predict_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_units * 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.cat_cols = cat_cols\n",
    "        self.num_cols = num_cols\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "    \n",
    "    \n",
    "    def forward(self, past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature):\n",
    "        \"\"\"\n",
    "        past_cat_feature : (batch_size, max_len, cat_cols)\n",
    "        past_num_feature : (batch_size, max_len, num_cols)\n",
    "        past_answerCode : (batch_size, max_len)\n",
    "\n",
    "        now_cat_feature : (batch_size, max_len, cat_cols)\n",
    "        now_num_feature : (batch_size, max_len, num_cols)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        past_cat_emb_list = []\n",
    "        for idx in range(len(self.cat_cols)):\n",
    "            if self.cat_cols[idx] == 'assessmentItemID2idx':\n",
    "                past_cat_emb_list.append(self.past_assessmentItemID_emb(past_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'testId2idx':\n",
    "                past_cat_emb_list.append(self.past_testId_emb(past_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'KnowledgeTag2idx':\n",
    "                past_cat_emb_list.append(self.past_KnowledgeTag_emb(past_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'large_paper_number2idx':\n",
    "                past_cat_emb_list.append(self.past_large_paper_number_emb(past_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'hour':\n",
    "                past_cat_emb_list.append(self.past_hour_emb(past_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'dayofweek':\n",
    "                past_cat_emb_list.append(self.past_dayofweek_emb(past_cat_feature[:, :, idx]))\n",
    "\n",
    "        past_cat_emb = torch.concat(past_cat_emb_list, dim = -1)\n",
    "        past_cat_emb = self.past_cat_emb(past_cat_emb)\n",
    "        past_num_emb = self.past_num_emb(past_num_feature)\n",
    "\n",
    "        past_emb = torch.concat([past_cat_emb, past_num_emb], dim = -1)\n",
    "        past_emb += self.past_answerCode_emb(past_answerCode.to(self.device))\n",
    "        past_emb = self.emb_layernorm(past_emb) # LayerNorm\n",
    "\n",
    "        # masking \n",
    "        mask_pad = torch.BoolTensor(past_answerCode > 0).unsqueeze(1).unsqueeze(1) # (batch_size, 1, 1, max_len)\n",
    "        mask_time = (1 - torch.triu(torch.ones((1, 1, past_answerCode.size(1), past_answerCode.size(1))), diagonal=1)).bool() # (batch_size, 1, max_len, max_len)\n",
    "        mask = (mask_pad & mask_time).to(self.device) # (batch_size, 1, max_len, max_len)\n",
    "        for block in self.past_blocks:\n",
    "            past_emb, attn_dist = block(past_emb, mask)\n",
    "\n",
    "        past_emb, _ = self.past_lstm(past_emb)\n",
    "\n",
    "        now_cat_emb_list = []\n",
    "        for idx in range(len(self.cat_cols)):\n",
    "            if self.cat_cols[idx] == 'assessmentItemID2idx':\n",
    "                now_cat_emb_list.append(self.now_assessmentItemID_emb(now_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'testId2idx':\n",
    "                now_cat_emb_list.append(self.now_testId_emb(now_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'KnowledgeTag2idx':\n",
    "                now_cat_emb_list.append(self.now_KnowledgeTag_emb(now_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'large_paper_number2idx':\n",
    "                now_cat_emb_list.append(self.now_large_paper_number_emb(now_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'hour':\n",
    "                now_cat_emb_list.append(self.now_hour_emb(now_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'dayofweek':\n",
    "                now_cat_emb_list.append(self.now_dayofweek_emb(now_cat_feature[:, :, idx]))\n",
    "\n",
    "        now_cat_emb = torch.concat(now_cat_emb_list, dim = -1)\n",
    "        now_cat_emb = self.now_cat_emb(now_cat_emb)\n",
    "        now_num_emb = self.now_num_emb(now_num_feature)\n",
    "\n",
    "        now_emb = torch.concat([now_cat_emb, now_num_emb], dim = -1)\n",
    "\n",
    "        for block in self.now_blocks:\n",
    "            now_emb, attn_dist = block(now_emb, mask)\n",
    "\n",
    "        now_emb, _ = self.now_lstm(now_emb)\n",
    "\n",
    "        emb = torch.concat([past_emb, now_emb], dim = -1)\n",
    "        \n",
    "        output = self.predict_layer(self.dropout(emb))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, data_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "\n",
    "    for past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature, now_answerCode in data_loader:\n",
    "\n",
    "        past_cat_feature, past_num_feature, past_answerCode = past_cat_feature.to(device), past_num_feature.to(device), past_answerCode\n",
    "        now_cat_feature, now_num_feature, now_answerCode = now_cat_feature.to(device), now_num_feature.to(device), now_answerCode.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature).squeeze(2)\n",
    "        loss = criterion(output[:, -1], now_answerCode[:, -1])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val += loss.item()\n",
    "\n",
    "    loss_val /= len(data_loader)\n",
    "\n",
    "    return loss_val\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    target = []\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature, now_answerCode in data_loader:\n",
    "            past_cat_feature, past_num_feature, past_answerCode = past_cat_feature.to(device), past_num_feature.to(device), past_answerCode\n",
    "            now_cat_feature, now_num_feature, now_answerCode = now_cat_feature.to(device), now_num_feature.to(device), now_answerCode.to(device)\n",
    "            \n",
    "            output = model(past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature).squeeze(2)\n",
    "\n",
    "            target.extend(now_answerCode[:, -1].cpu().numpy().tolist())\n",
    "            pred.extend(output[:, -1].cpu().numpy().tolist())\n",
    "\n",
    "    roc_auc = roc_auc_score(target, pred)\n",
    "\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature, now_answerCode in data_loader:\n",
    "            past_cat_feature, past_num_feature, past_answerCode = past_cat_feature.to(device), past_num_feature.to(device), past_answerCode\n",
    "            now_cat_feature, now_num_feature = now_cat_feature.to(device), now_num_feature.to(device)\n",
    "            \n",
    "            output = model(past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature).squeeze(2)\n",
    "            pred.extend(output[:, -1].cpu().numpy().tolist())\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 20\n",
    "lr = 0.001\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "emb_size = 64\n",
    "hidden_units = 128\n",
    "num_heads = 2 # 2,4,8,16,32\n",
    "num_layers = 1\n",
    "dropout_rate = 0.5\n",
    "num_workers = 8\n",
    "\n",
    "max_len = 50\n",
    "window = 10\n",
    "data_augmentation = True\n",
    "\n",
    "DATA_PATH = '/opt/ml/input/data'\n",
    "MODEL_PATH = '/opt/ml/model'\n",
    "SUBMISSION_PATH = '/opt/ml/submission'\n",
    "\n",
    "model_name = 'Transformer-and-LSTM-Encoder-Decoder-each-Embedding-num_heads-2-data-aug.pt'\n",
    "submission_name = 'Transformer-and-LSTM-Encoder-Decoder-each-Embedding-num_heads-2-data-aug.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(MODEL_PATH):\n",
    "    os.mkdir(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(SUBMISSION_PATH):\n",
    "    os.mkdir(SUBMISSION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dataset = MakeDataset(DATA_PATH = DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOF Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7442/7442 [00:16<00:00, 446.33it/s]\n",
      "OOF-0| Epoch:   1| Train loss: 0.51428| roc_auc: 0.81783: 100%|██████████| 1/1 [02:44<00:00, 164.47s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.48639| roc_auc: 0.83350: 100%|██████████| 1/1 [02:45<00:00, 165.57s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.46524| roc_auc: 0.83709: 100%|██████████| 1/1 [02:43<00:00, 163.81s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.45043| roc_auc: 0.83694: 100%|██████████| 1/1 [02:43<00:00, 163.07s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44030| roc_auc: 0.83533: 100%|██████████| 1/1 [02:47<00:00, 167.34s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.43078| roc_auc: 0.83323: 100%|██████████| 1/1 [02:48<00:00, 168.92s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.42315| roc_auc: 0.82941: 100%|██████████| 1/1 [02:42<00:00, 162.22s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.41616| roc_auc: 0.82981: 100%|██████████| 1/1 [02:40<00:00, 160.42s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.40766| roc_auc: 0.82895: 100%|██████████| 1/1 [02:46<00:00, 166.71s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.40018| roc_auc: 0.82450: 100%|██████████| 1/1 [02:48<00:00, 168.44s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "oof_roc_auc = 0\n",
    "\n",
    "for oof in make_dataset.oof_user_set.keys():\n",
    "    train_df, valid_df = make_dataset.get_oof_data(oof)\n",
    "    \n",
    "    seed_everything(22 + oof)\n",
    "    \n",
    "    train_dataset = CustomDataset(df = train_df, max_len = max_len, window = window, data_augmentation = data_augmentation)\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size = batch_size, \n",
    "        shuffle = True, \n",
    "        drop_last = False,\n",
    "        collate_fn = train_make_batch,\n",
    "        num_workers = num_workers)\n",
    "\n",
    "    valid_dataset = CustomDataset(df = valid_df, max_len = max_len)\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = 1, \n",
    "        shuffle = False, \n",
    "        drop_last = False,\n",
    "        collate_fn = train_make_batch,\n",
    "        num_workers = num_workers)\n",
    "\n",
    "    model = SASRec(\n",
    "        num_assessmentItemID = make_dataset.num_assessmentItemID, \n",
    "        num_testId = make_dataset.num_testId,\n",
    "        num_KnowledgeTag = make_dataset.num_KnowledgeTag,\n",
    "        num_large_paper_number = make_dataset.num_large_paper_number,\n",
    "        num_hour = make_dataset.num_hour,\n",
    "        num_dayofweek = make_dataset.num_dayofweek,\n",
    "        num_cols = train_dataset.num_cols,\n",
    "        cat_cols = train_dataset.cat_cols,\n",
    "        emb_size = emb_size,\n",
    "        hidden_units = hidden_units,\n",
    "        num_heads = num_heads,\n",
    "        num_layers = num_layers,\n",
    "        dropout_rate = dropout_rate,\n",
    "        device = device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # pre_emb = Word2Vec.load(os.path.join(MODEL_PATH, 'Word2Vec_Embedding_Model_window_50.model'))\n",
    "\n",
    "    # assessmentItemID_li = make_dataset.assessmentItemID2idx.keys()\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     for assessmentItemID in assessmentItemID_li:\n",
    "    #         idx = make_dataset.assessmentItemID2idx[assessmentItemID]\n",
    "    #         model.assessmentItemID_emb.weight[idx + 1] = torch.tensor(pre_emb.wv[assessmentItemID]).to(device)\n",
    "\n",
    "    best_epoch = 0\n",
    "    best_train_loss = 0\n",
    "    best_roc_auc = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tbar = tqdm(range(1))\n",
    "        for _ in tbar:\n",
    "            train_loss = train(model = model, data_loader = train_data_loader, criterion = criterion, optimizer = optimizer)\n",
    "            roc_auc = evaluate(model = model, data_loader = valid_data_loader)\n",
    "            if best_roc_auc < roc_auc:\n",
    "                best_epoch = epoch\n",
    "                best_train_loss = train_loss\n",
    "                best_roc_auc = roc_auc\n",
    "                torch.save(model.state_dict(), os.path.join(MODEL_PATH, f'oof_{oof}_' + model_name))\n",
    "\n",
    "            tbar.set_description(f'OOF-{oof}| Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| roc_auc: {roc_auc:.5f}')\n",
    "    \n",
    "    print(f'BEST OOF-{oof}| Epoch: {best_epoch:3d}| Train loss: {best_train_loss:.5f}| roc_auc: {best_roc_auc:.5f}')\n",
    "\n",
    "    oof_roc_auc += best_roc_auc\n",
    "\n",
    "print(f'Total roc_auc: {oof_roc_auc / len(make_dataset.oof_user_set.keys()):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = make_dataset.get_test_data()\n",
    "test_dataset = CustomDataset(df = test_df, max_len = max_len)\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 1, \n",
    "    shuffle = False, \n",
    "    drop_last = False,\n",
    "    collate_fn = train_make_batch,\n",
    "    num_workers = num_workers)\n",
    "\n",
    "pred_list = []\n",
    "\n",
    "model = SASRec(\n",
    "    num_assessmentItemID = make_dataset.num_assessmentItemID, \n",
    "    num_testId = make_dataset.num_testId,\n",
    "    num_KnowledgeTag = make_dataset.num_KnowledgeTag,\n",
    "    num_large_paper_number = make_dataset.num_large_paper_number,\n",
    "    num_hour = make_dataset.num_hour,\n",
    "    num_dayofweek = make_dataset.num_dayofweek,\n",
    "    num_cols = train_dataset.num_cols,\n",
    "    cat_cols = train_dataset.cat_cols,\n",
    "    emb_size = emb_size, \n",
    "    hidden_units = hidden_units, \n",
    "    num_heads = num_heads, \n",
    "    num_layers = num_layers, \n",
    "    dropout_rate = dropout_rate, \n",
    "    device = device).to(device)\n",
    "\n",
    "for oof in make_dataset.oof_user_set.keys():\n",
    "    model.load_state_dict(torch.load(os.path.join(MODEL_PATH, f'oof_{oof}_' + model_name)))\n",
    "    pred = predict(model = model, data_loader = test_data_loader)\n",
    "    pred_list.append(pred)\n",
    "\n",
    "pred_list = np.array(pred_list).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data = np.array(pred_list), columns = ['prediction'])\n",
    "submission['id'] = submission.index\n",
    "submission = submission[['id', 'prediction']]\n",
    "submission.to_csv(os.path.join(SUBMISSION_PATH, 'OOF-Ensemble-' + submission_name), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
