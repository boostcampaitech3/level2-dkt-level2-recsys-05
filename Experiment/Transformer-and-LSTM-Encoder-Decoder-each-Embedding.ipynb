{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "torch.set_printoptions(sci_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDataset():\n",
    "\n",
    "    def __init__(self, DATA_PATH):\n",
    "        self.preporcessing(DATA_PATH)\n",
    "        self.oof_user_set = self.split_data()\n",
    "    \n",
    "    def split_data(self):\n",
    "        user_list = self.all_df['userID'].unique().tolist()\n",
    "        oof_user_set = {}\n",
    "        kf = KFold(n_splits = 5, random_state = 22, shuffle = True)\n",
    "        for idx, (train_user, valid_user) in enumerate(kf.split(user_list)):\n",
    "            oof_user_set[idx] = valid_user.tolist()\n",
    "        \n",
    "        return oof_user_set\n",
    "\n",
    "    def preporcessing(self, DATA_PATH):\n",
    "\n",
    "        dtype = {\n",
    "            'userID': 'int16',\n",
    "            'answerCode': 'int8',\n",
    "            'KnowledgeTag': 'int16'\n",
    "        }\n",
    "        \n",
    "        train_df = pd.read_csv(os.path.join(DATA_PATH, 'train_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "        train_df = train_df.sort_values(by=['userID', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "        test_df = pd.read_csv(os.path.join(DATA_PATH, 'test_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "        test_df = test_df.sort_values(by=['userID', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        def get_large_paper_number(x):\n",
    "            return x[1:4]\n",
    "        \n",
    "        train_df['large_paper_number'] = train_df['assessmentItemID'].apply(lambda x : get_large_paper_number(x))\n",
    "        test_df['large_paper_number'] = test_df['assessmentItemID'].apply(lambda x : get_large_paper_number(x))\n",
    "\n",
    "        # 문제 푸는데 걸린 시간\n",
    "        def get_now_elapsed(df):\n",
    "            \n",
    "            diff = df.loc[:, ['userID','Timestamp']].groupby('userID').diff().fillna(pd.Timedelta(seconds=0))\n",
    "            diff = diff.fillna(pd.Timedelta(seconds=0))\n",
    "            diff = diff['Timestamp'].apply(lambda x: x.total_seconds())\n",
    "            df['now_elapsed'] = diff\n",
    "            df['now_elapsed'] = df['now_elapsed'].apply(lambda x : x if x < 650 and x >=0 else 0)\n",
    "            df['now_elapsed'] = df['now_elapsed']\n",
    "\n",
    "            return df\n",
    "\n",
    "        train_df = get_now_elapsed(df = train_df)\n",
    "        test_df = get_now_elapsed(df = test_df)\n",
    "\n",
    "        all_df = pd.concat([train_df, test_df])\n",
    "        all_df = all_df[all_df['answerCode'] != -1].reset_index(drop = True)\n",
    "\n",
    "        # normalize_score\n",
    "        def get_normalize_score(df, all_df):\n",
    "            ret_df = []\n",
    "\n",
    "            group_df = df.groupby('userID')\n",
    "            mean_answerCode_df = all_df.groupby('testId').mean()['answerCode']\n",
    "            std_answerCode_df = all_df.groupby('testId').std()['answerCode']\n",
    "            for userID, get_df in group_df:\n",
    "                normalize_score_df = (get_df[get_df['answerCode'] != -1].groupby('testId').mean()['answerCode'] - mean_answerCode_df) / std_answerCode_df\n",
    "                get_df = get_df.copy().set_index('testId')\n",
    "                get_df['normalize_score'] = normalize_score_df\n",
    "                ret_df.append(get_df.reset_index(drop = False))\n",
    "\n",
    "            ret_df = pd.concat(ret_df).reset_index(drop = True)\n",
    "            \n",
    "            return ret_df\n",
    "        \n",
    "        # train_df = get_normalize_score(df = train_df, all_df = all_df)\n",
    "        # test_df = get_normalize_score(df = test_df, all_df = all_df)\n",
    "\n",
    "        # 문항별 정답률\n",
    "        train_df = train_df.set_index('assessmentItemID')\n",
    "        train_df['assessmentItemID_mean_answerCode'] = all_df.groupby('assessmentItemID').mean()['answerCode']\n",
    "        train_df = train_df.reset_index(drop = False)\n",
    "\n",
    "        test_df = test_df.set_index('assessmentItemID')\n",
    "        test_df['assessmentItemID_mean_answerCode'] = all_df.groupby('assessmentItemID').mean()['answerCode']\n",
    "        test_df = test_df.reset_index(drop = False)\n",
    "\n",
    "        # 문항별 정답률 표준편차\n",
    "        train_df = train_df.set_index('assessmentItemID')\n",
    "        train_df['assessmentItemID_std_answerCode'] = all_df.groupby('assessmentItemID').std()['answerCode']\n",
    "        train_df = train_df.reset_index(drop = False)\n",
    "\n",
    "        test_df = test_df.set_index('assessmentItemID')\n",
    "        test_df['assessmentItemID_std_answerCode'] = all_df.groupby('assessmentItemID').std()['answerCode']\n",
    "        test_df = test_df.reset_index(drop = False)\n",
    "\n",
    "        # 올바르게 푼 사람들의 문항별 풀이 시간 평균\n",
    "        train_df = train_df.set_index('assessmentItemID')\n",
    "        train_df['assessmentItemID_mean_now_elapsed'] = all_df[all_df['answerCode'] == 1].groupby('assessmentItemID').mean()['now_elapsed']\n",
    "        train_df = train_df.reset_index(drop = False)\n",
    "\n",
    "        test_df = test_df.set_index('assessmentItemID')\n",
    "        test_df['assessmentItemID_mean_now_elapsed'] = all_df[all_df['answerCode'] == 1].groupby('assessmentItemID').mean()['now_elapsed']\n",
    "        test_df = test_df.reset_index(drop = False)\n",
    "\n",
    "        # 올바르게 푼 사람들의 문항별 풀이 시간 표준 편차\n",
    "        train_df = train_df.set_index('assessmentItemID')\n",
    "        train_df['assessmentItemID_std_now_elapsed'] = all_df[all_df['answerCode'] == 1].groupby('assessmentItemID').std()['now_elapsed']\n",
    "        train_df = train_df.reset_index(drop = False)\n",
    "\n",
    "        test_df = test_df.set_index('assessmentItemID')\n",
    "        test_df['assessmentItemID_std_now_elapsed'] = all_df[all_df['answerCode'] == 1].groupby('assessmentItemID').std()['now_elapsed']\n",
    "        test_df = test_df.reset_index(drop = False)\n",
    "\n",
    "        # 문제 푼 시간\n",
    "        train_df['hour'] = train_df['Timestamp'].dt.hour\n",
    "        test_df['hour'] = test_df['Timestamp'].dt.hour\n",
    "\n",
    "        # 문제 푼 요일\n",
    "        train_df['dayofweek'] = train_df['Timestamp'].dt.dayofweek\n",
    "        test_df['dayofweek'] = test_df['Timestamp'].dt.dayofweek\n",
    "\n",
    "        # index 로 변환\n",
    "\n",
    "        def get_val2idx(val_list : list) -> dict:\n",
    "            val2idx = {}\n",
    "            for idx, val in enumerate(val_list):\n",
    "                val2idx[val] = idx\n",
    "            \n",
    "            return val2idx\n",
    "\n",
    "        assessmentItemID2idx = get_val2idx(all_df['assessmentItemID'].unique().tolist())\n",
    "        testId2idx = get_val2idx(all_df['testId'].unique().tolist())\n",
    "        KnowledgeTag2idx = get_val2idx(all_df['KnowledgeTag'].unique().tolist())\n",
    "        large_paper_number2idx = get_val2idx(all_df['large_paper_number'].unique().tolist())\n",
    "\n",
    "        train_df['assessmentItemID2idx'] = train_df['assessmentItemID'].apply(lambda x : assessmentItemID2idx[x])\n",
    "        train_df['testId2idx'] = train_df['testId'].apply(lambda x : testId2idx[x])\n",
    "        train_df['KnowledgeTag2idx'] = train_df['KnowledgeTag'].apply(lambda x : KnowledgeTag2idx[x])\n",
    "        train_df['large_paper_number2idx'] = train_df['large_paper_number'].apply(lambda x : large_paper_number2idx[x])\n",
    "\n",
    "        test_df['assessmentItemID2idx'] = test_df['assessmentItemID'].apply(lambda x : assessmentItemID2idx[x])\n",
    "        test_df['testId2idx'] = test_df['testId'].apply(lambda x : testId2idx[x])\n",
    "        test_df['KnowledgeTag2idx'] = test_df['KnowledgeTag'].apply(lambda x : KnowledgeTag2idx[x])\n",
    "        test_df['large_paper_number2idx'] = test_df['large_paper_number'].apply(lambda x : large_paper_number2idx[x])\n",
    "\n",
    "        self.assessmentItemID2idx = assessmentItemID2idx\n",
    "        self.train_df, self.test_df = train_df, test_df\n",
    "        self.all_df = pd.concat([train_df, test_df[test_df['answerCode'] != -1]]).reset_index(drop=True)\n",
    "        self.num_assessmentItemID = len(assessmentItemID2idx)\n",
    "        self.num_testId = len(testId2idx)\n",
    "        self.num_KnowledgeTag = len(KnowledgeTag2idx)\n",
    "        self.num_large_paper_number = len(large_paper_number2idx)\n",
    "        self.num_hour = 24\n",
    "        self.num_dayofweek = 7\n",
    "\n",
    "    def get_oof_data(self, oof):\n",
    "\n",
    "        val_user_list = self.oof_user_set[oof]\n",
    "\n",
    "        train = []\n",
    "        valid = []\n",
    "\n",
    "        group_df = self.all_df.groupby('userID')\n",
    "\n",
    "        for userID, df in group_df:\n",
    "            if userID in val_user_list:\n",
    "                trn_df = df.iloc[:-1, :]\n",
    "                val_df = df.copy()\n",
    "                train.append(trn_df)\n",
    "                valid.append(val_df)\n",
    "            else:\n",
    "                train.append(df)\n",
    "\n",
    "        # normalize_score\n",
    "        def get_normalize_score(df, all_df, vailid = False):\n",
    "            ret_df = []\n",
    "\n",
    "            group_df = df.groupby('userID')\n",
    "            mean_answerCode_df = all_df.groupby('testId').mean()['answerCode']\n",
    "            std_answerCode_df = all_df.groupby('testId').std()['answerCode']\n",
    "            for userID, get_df in group_df:\n",
    "                if vailid:\n",
    "                    normalize_score_df = (get_df.iloc[:-1, :].groupby('testId').mean()['answerCode'] - mean_answerCode_df) / std_answerCode_df\n",
    "                else:\n",
    "                    normalize_score_df = (get_df.groupby('testId').mean()['answerCode'] - mean_answerCode_df) / std_answerCode_df\n",
    "                    \n",
    "                get_df = get_df.copy().set_index('testId')\n",
    "                get_df['normalize_score'] = normalize_score_df\n",
    "                ret_df.append(get_df.reset_index(drop = False))\n",
    "\n",
    "            ret_df = pd.concat(ret_df).reset_index(drop = True)\n",
    "            \n",
    "            return ret_df\n",
    "\n",
    "        train = pd.concat(train).reset_index(drop = True)\n",
    "        valid = pd.concat(valid).reset_index(drop = True)\n",
    "\n",
    "        # train = get_normalize_score(df = train, all_df = train)\n",
    "        # valid = get_normalize_score(df = valid, all_df = train, vailid = True)\n",
    "        \n",
    "        return train, valid\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        return self.test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        df,\n",
    "        cat_cols = ['assessmentItemID2idx', 'testId2idx', 'KnowledgeTag2idx', 'large_paper_number2idx', 'hour', 'dayofweek'],\n",
    "        num_cols = ['now_elapsed', 'assessmentItemID_mean_now_elapsed', 'assessmentItemID_std_now_elapsed', 'assessmentItemID_mean_answerCode', 'assessmentItemID_std_answerCode'],\n",
    "        max_len = None,\n",
    "        window = None,\n",
    "        data_augmentation = False,\n",
    "        ):\n",
    "\n",
    "        self.cat_cols = cat_cols\n",
    "        self.num_cols = num_cols\n",
    "        self.get_df = df.groupby('userID')\n",
    "        self.user_list = df['userID'].unique().tolist()\n",
    "        self.max_len = max_len\n",
    "        self.window = window\n",
    "        self.data_augmentation = data_augmentation\n",
    "        if self.data_augmentation:\n",
    "            self.cat_feature_list, self.num_feature_list, self.answerCode_list = self._data_augmentation()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.data_augmentation:\n",
    "            return len(self.cat_feature_list)\n",
    "        return len(self.user_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.data_augmentation:\n",
    "            cat_feature = self.cat_feature_list[idx]\n",
    "            num_feature = self.num_feature_list[idx]\n",
    "            answerCode = self.answerCode_list[idx]\n",
    "\n",
    "            now_cat_feature = cat_feature[1:, :]\n",
    "            now_num_feature = num_feature[1:, :]\n",
    "            now_answerCode = answerCode[1:]\n",
    "            \n",
    "            past_cat_feature = cat_feature[:-1, :]\n",
    "            past_num_feature = num_feature[:-1, :]\n",
    "            past_answerCode = answerCode[:-1]\n",
    "            \n",
    "        else:\n",
    "            user = self.user_list[idx]\n",
    "            if self.max_len:\n",
    "                get_df = self.get_df.get_group(user).iloc[-self.max_len:, :]\n",
    "            else:\n",
    "                get_df = self.get_df.get_group(user)\n",
    "\n",
    "            now_df = get_df.iloc[1:, :]\n",
    "            now_cat_feature = now_df[self.cat_cols].values\n",
    "            now_num_feature = now_df[self.num_cols].values\n",
    "            now_answerCode = now_df['answerCode'].values\n",
    "\n",
    "            past_df = get_df.iloc[:-1, :]\n",
    "            past_cat_feature = past_df[self.cat_cols].values\n",
    "            past_num_feature = past_df[self.num_cols].values\n",
    "            past_answerCode = past_df['answerCode'].values\n",
    "\n",
    "        return {\n",
    "            'past_cat_feature' : past_cat_feature, \n",
    "            'past_num_feature' : past_num_feature, \n",
    "            'past_answerCode' : past_answerCode, \n",
    "            'now_cat_feature' : now_cat_feature, \n",
    "            'now_num_feature' : now_num_feature, \n",
    "            'now_answerCode' : now_answerCode\n",
    "            }\n",
    "    \n",
    "\n",
    "    def _data_augmentation(self):\n",
    "        cat_feature_list = []\n",
    "        num_feature_list = []\n",
    "        answerCode_list = []\n",
    "        for userID, get_df in tqdm(self.get_df):\n",
    "            cat_feature = get_df[self.cat_cols].values[::-1]\n",
    "            num_feature = get_df[self.num_cols].values[::-1]\n",
    "            answerCode = get_df['answerCode'].values[::-1]\n",
    "\n",
    "            start_idx = 0\n",
    "\n",
    "            if len(get_df) <= self.max_len:\n",
    "                cat_feature_list.append(cat_feature[::-1])\n",
    "                num_feature_list.append(num_feature[::-1])\n",
    "                answerCode_list.append(answerCode[::-1])\n",
    "            else:\n",
    "                while True:\n",
    "                    if len(cat_feature[start_idx: start_idx + self.max_len, :]) < self.max_len:\n",
    "                        cat_feature_list.append(cat_feature[start_idx: start_idx + self.max_len, :][::-1])\n",
    "                        num_feature_list.append(num_feature[start_idx: start_idx + self.max_len, :][::-1])\n",
    "                        answerCode_list.append(answerCode[start_idx: start_idx + self.max_len][::-1])\n",
    "                        break\n",
    "                    cat_feature_list.append(cat_feature[start_idx: start_idx + self.max_len, :][::-1])\n",
    "                    num_feature_list.append(num_feature[start_idx: start_idx + self.max_len, :][::-1])\n",
    "                    answerCode_list.append(answerCode[start_idx: start_idx + self.max_len][::-1])\n",
    "                    start_idx += self.window\n",
    "            \n",
    "        return cat_feature_list, num_feature_list, answerCode_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(seq, max_len, padding_value = 0):\n",
    "    try:\n",
    "        seq_len, col = seq.shape\n",
    "        padding = np.zeros((max_len - seq_len, col)) + padding_value\n",
    "    except:\n",
    "        seq_len = seq.shape[0]\n",
    "        padding = np.zeros((max_len - seq_len, )) + padding_value\n",
    "\n",
    "    padding_seq = np.concatenate([padding, seq])\n",
    "\n",
    "    return padding_seq\n",
    "\n",
    "def train_make_batch(samples):\n",
    "    max_len = 0\n",
    "    for sample in samples:\n",
    "        seq_len, col = sample['past_cat_feature'].shape\n",
    "        if max_len < seq_len:\n",
    "            max_len = seq_len\n",
    "    \n",
    "    past_cat_feature = []\n",
    "    past_num_feature = []\n",
    "    past_answerCode = []\n",
    "    now_cat_feature = []\n",
    "    now_num_feature = []\n",
    "    now_answerCode = []\n",
    "\n",
    "    for sample in samples:\n",
    "        past_cat_feature += [pad_sequence(sample['past_cat_feature'] + 1, max_len = max_len, padding_value = 0)]\n",
    "        past_num_feature += [pad_sequence(sample['past_num_feature'], max_len = max_len, padding_value = 0)]\n",
    "        past_answerCode += [pad_sequence(sample['past_answerCode'] + 1, max_len = max_len, padding_value = 0)]\n",
    "        now_cat_feature += [pad_sequence(sample['now_cat_feature'] + 1, max_len = max_len, padding_value = 0)]\n",
    "        now_num_feature += [pad_sequence(sample['now_num_feature'], max_len = max_len, padding_value = 0)]\n",
    "        now_answerCode += [pad_sequence(sample['now_answerCode'], max_len = max_len, padding_value = -1)]\n",
    "\n",
    "    return torch.tensor(past_cat_feature, dtype = torch.long), torch.tensor(past_num_feature, dtype = torch.float32), torch.tensor(past_answerCode, dtype = torch.long), torch.tensor(now_cat_feature, dtype = torch.long), torch.tensor(now_num_feature, dtype = torch.float32), torch.tensor(now_answerCode, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        \"\"\"\n",
    "        Q, K, V : (batch_size, num_heads, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \"\"\"\n",
    "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units) # (batch_size, num_heads, max_len, max_len)\n",
    "        attn_score = attn_score.masked_fill(mask == 0, -1e9)  # 유사도가 0인 지점은 -infinity로 보내 softmax 결과가 0이 되도록 함\n",
    "        attn_dist = self.dropout(F.softmax(attn_score, dim=-1))  # attention distribution\n",
    "        output = torch.matmul(attn_dist, V)  # (batch_size, num_heads, max_len, hidden_units) / # dim of output : batchSize x num_head x seqLen x hidden_units\n",
    "        return output, attn_dist\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads # head의 수\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        # query, key, value, output 생성을 위해 Linear 모델 생성\n",
    "        self.W_Q = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_K = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_V = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_O = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(hidden_units, dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, enc, mask):\n",
    "        \"\"\"\n",
    "        enc : (batch_size, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \n",
    "        \"\"\"\n",
    "        residual = enc # residual connection을 위해 residual 부분을 저장\n",
    "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
    "\n",
    "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
    "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units // self.num_heads) # (batch_size, max_len, num_heads, hidden_units)\n",
    "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units // self.num_heads) # (batch_size, max_len, num_heads, hidden_units)\n",
    "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units // self.num_heads) # (batch_size, max_len, num_heads, hidden_units)\n",
    "\n",
    "        # Head별로 각기 다른 attention이 가능하도록 Transpose 후 각각 attention에 통과시킴\n",
    "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2) # (batch_size, num_heads, max_len, hidden_units)\n",
    "        output, attn_dist = self.attention(Q, K, V, mask) # output : (batch_size, num_heads, max_len, hidden_units) / attn_dist : (batch_size, num_heads, max_len, max_len)\n",
    "\n",
    "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n",
    "        output = output.transpose(1, 2).contiguous() # (batch_size, max_len, num_heads, hidden_units) / contiguous() : 가변적 메모리 할당\n",
    "        output = output.view(batch_size, seqlen, -1) # (batch_size, max_len, hidden_units * num_heads)\n",
    "\n",
    "        # Linear Projection, Dropout, Residual sum, and Layer Normalization\n",
    "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual) # (batch_size, max_len, hidden_units)\n",
    "        return output, attn_dist\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "\n",
    "        self.W_1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.W_2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = self.W_2(F.relu(self.dropout(self.W_1(x))))\n",
    "        output = self.layerNorm(self.dropout(output) + residual)\n",
    "        return output\n",
    "\n",
    "\n",
    "class SASRecBlock(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(SASRecBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads, hidden_units, dropout_rate)\n",
    "        self.pointwise_feedforward = PositionwiseFeedForward(hidden_units, dropout_rate)\n",
    "\n",
    "    def forward(self, input_enc, mask):\n",
    "        \"\"\"\n",
    "        input_enc : (batch_size, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \"\"\"\n",
    "        output_enc, attn_dist = self.attention(input_enc, mask)\n",
    "        output_enc = self.pointwise_feedforward(output_enc)\n",
    "        return output_enc, attn_dist\n",
    "\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_assessmentItemID, \n",
    "        num_testId,\n",
    "        num_KnowledgeTag,\n",
    "        num_large_paper_number,\n",
    "        num_hour,\n",
    "        num_dayofweek,\n",
    "        num_cols,\n",
    "        cat_cols,\n",
    "        emb_size,\n",
    "        hidden_units,\n",
    "        num_heads, \n",
    "        num_layers, \n",
    "        dropout_rate, \n",
    "        device):\n",
    "        super(SASRec, self).__init__()\n",
    "\n",
    "        # past\n",
    "        self.past_assessmentItemID_emb = nn.Embedding(num_assessmentItemID + 1, emb_size, padding_idx = 0) # 문항에 대한 정보\n",
    "        self.past_testId_emb = nn.Embedding(num_testId + 1, emb_size, padding_idx = 0) # 시험지에 대한 정보\n",
    "        self.past_KnowledgeTag_emb = nn.Embedding(num_KnowledgeTag + 1, emb_size, padding_idx = 0) # 지식 태그에 대한 정보\n",
    "        self.past_large_paper_number_emb = nn.Embedding(num_large_paper_number + 1, emb_size, padding_idx = 0) # 핫년에 대한 정보\n",
    "        self.past_hour_emb = nn.Embedding(num_hour + 1, emb_size, padding_idx = 0) # 문제 풀이 시간에 대한 정보\n",
    "        self.past_dayofweek_emb = nn.Embedding(num_dayofweek + 1, emb_size, padding_idx = 0) # 문제 풀이 요일에 대항 정보\n",
    "        self.past_answerCode_emb = nn.Embedding(3, hidden_units, padding_idx = 0) # 문제 정답 여부에 대한 정보\n",
    "\n",
    "        self.past_cat_emb = nn.Sequential(\n",
    "            nn.Linear(len(cat_cols) * emb_size, hidden_units // 2),\n",
    "            nn.LayerNorm(hidden_units // 2, eps=1e-6)\n",
    "        )\n",
    "\n",
    "        self.past_num_emb = nn.Sequential(\n",
    "            nn.Linear(len(num_cols), hidden_units // 2),\n",
    "            nn.LayerNorm(hidden_units // 2, eps=1e-6)\n",
    "        )\n",
    "\n",
    "        self.emb_layernorm = nn.LayerNorm(hidden_units, eps=1e-6)\n",
    "\n",
    "        self.past_lstm = nn.LSTM(\n",
    "            input_size = hidden_units,\n",
    "            hidden_size = hidden_units,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            bidirectional = False,\n",
    "            dropout = dropout_rate,\n",
    "            )\n",
    "\n",
    "        self.past_blocks = nn.ModuleList([SASRecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
    "\n",
    "        # now\n",
    "        self.now_assessmentItemID_emb = nn.Embedding(num_assessmentItemID + 1, emb_size, padding_idx = 0) # 문항에 대한 정보\n",
    "        self.now_testId_emb = nn.Embedding(num_testId + 1, emb_size, padding_idx = 0) # 시험지에 대한 정보\n",
    "        self.now_KnowledgeTag_emb = nn.Embedding(num_KnowledgeTag + 1, emb_size, padding_idx = 0) # 지식 태그에 대한 정보\n",
    "        self.now_large_paper_number_emb = nn.Embedding(num_large_paper_number + 1, emb_size, padding_idx = 0) # 핫년에 대한 정보\n",
    "        self.now_hour_emb = nn.Embedding(num_hour + 1, emb_size, padding_idx = 0) # 문제 풀이 시간에 대한 정보\n",
    "        self.now_dayofweek_emb = nn.Embedding(num_dayofweek + 1, emb_size, padding_idx = 0) # 문제 풀이 요일에 대항 정보\n",
    "\n",
    "        self.now_cat_emb = nn.Sequential(\n",
    "            nn.Linear(len(cat_cols) * emb_size, hidden_units // 2),\n",
    "            nn.LayerNorm(hidden_units // 2, eps=1e-6)\n",
    "        )\n",
    "\n",
    "        self.now_num_emb = nn.Sequential(\n",
    "            nn.Linear(len(num_cols), hidden_units // 2),\n",
    "            nn.LayerNorm(hidden_units // 2, eps=1e-6)\n",
    "        )\n",
    "\n",
    "        self.now_lstm = nn.LSTM(\n",
    "            input_size = hidden_units,\n",
    "            hidden_size = hidden_units,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            bidirectional = False,\n",
    "            dropout = dropout_rate,\n",
    "            )\n",
    "\n",
    "        self.now_blocks = nn.ModuleList([SASRecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
    "\n",
    "        # predict\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.predict_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_units * 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.cat_cols = cat_cols\n",
    "        self.num_cols = num_cols\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "    \n",
    "    \n",
    "    def forward(self, past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature):\n",
    "        \"\"\"\n",
    "        past_cat_feature : (batch_size, max_len, cat_cols)\n",
    "        past_num_feature : (batch_size, max_len, num_cols)\n",
    "        past_answerCode : (batch_size, max_len)\n",
    "\n",
    "        now_cat_feature : (batch_size, max_len, cat_cols)\n",
    "        now_num_feature : (batch_size, max_len, num_cols)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        past_cat_emb_list = []\n",
    "        for idx in range(len(self.cat_cols)):\n",
    "            if self.cat_cols[idx] == 'assessmentItemID2idx':\n",
    "                past_cat_emb_list.append(self.past_assessmentItemID_emb(past_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'testId2idx':\n",
    "                past_cat_emb_list.append(self.past_testId_emb(past_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'KnowledgeTag2idx':\n",
    "                past_cat_emb_list.append(self.past_KnowledgeTag_emb(past_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'large_paper_number2idx':\n",
    "                past_cat_emb_list.append(self.past_large_paper_number_emb(past_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'hour':\n",
    "                past_cat_emb_list.append(self.past_hour_emb(past_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'dayofweek':\n",
    "                past_cat_emb_list.append(self.past_dayofweek_emb(past_cat_feature[:, :, idx]))\n",
    "\n",
    "        past_cat_emb = torch.concat(past_cat_emb_list, dim = -1)\n",
    "        past_cat_emb = self.past_cat_emb(past_cat_emb)\n",
    "        past_num_emb = self.past_num_emb(past_num_feature)\n",
    "\n",
    "        past_emb = torch.concat([past_cat_emb, past_num_emb], dim = -1)\n",
    "        past_emb += self.past_answerCode_emb(past_answerCode.to(self.device))\n",
    "        past_emb = self.emb_layernorm(past_emb) # LayerNorm\n",
    "\n",
    "        # masking \n",
    "        mask_pad = torch.BoolTensor(past_answerCode > 0).unsqueeze(1).unsqueeze(1) # (batch_size, 1, 1, max_len)\n",
    "        mask_time = (1 - torch.triu(torch.ones((1, 1, past_answerCode.size(1), past_answerCode.size(1))), diagonal=1)).bool() # (batch_size, 1, max_len, max_len)\n",
    "        mask = (mask_pad & mask_time).to(self.device) # (batch_size, 1, max_len, max_len)\n",
    "        for block in self.past_blocks:\n",
    "            past_emb, attn_dist = block(past_emb, mask)\n",
    "\n",
    "        past_emb, _ = self.past_lstm(past_emb)\n",
    "\n",
    "        now_cat_emb_list = []\n",
    "        for idx in range(len(self.cat_cols)):\n",
    "            if self.cat_cols[idx] == 'assessmentItemID2idx':\n",
    "                now_cat_emb_list.append(self.now_assessmentItemID_emb(now_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'testId2idx':\n",
    "                now_cat_emb_list.append(self.now_testId_emb(now_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'KnowledgeTag2idx':\n",
    "                now_cat_emb_list.append(self.now_KnowledgeTag_emb(now_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'large_paper_number2idx':\n",
    "                now_cat_emb_list.append(self.now_large_paper_number_emb(now_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'hour':\n",
    "                now_cat_emb_list.append(self.now_hour_emb(now_cat_feature[:, :, idx]))\n",
    "            elif self.cat_cols[idx] == 'dayofweek':\n",
    "                now_cat_emb_list.append(self.now_dayofweek_emb(now_cat_feature[:, :, idx]))\n",
    "\n",
    "        now_cat_emb = torch.concat(now_cat_emb_list, dim = -1)\n",
    "        now_cat_emb = self.now_cat_emb(now_cat_emb)\n",
    "        now_num_emb = self.now_num_emb(now_num_feature)\n",
    "\n",
    "        now_emb = torch.concat([now_cat_emb, now_num_emb], dim = -1)\n",
    "\n",
    "        for block in self.now_blocks:\n",
    "            now_emb, attn_dist = block(now_emb, mask)\n",
    "\n",
    "        now_emb, _ = self.now_lstm(now_emb)\n",
    "\n",
    "        emb = torch.concat([past_emb, now_emb], dim = -1)\n",
    "        \n",
    "        output = self.predict_layer(self.dropout(emb))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, data_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "\n",
    "    for past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature, now_answerCode in data_loader:\n",
    "\n",
    "        past_cat_feature, past_num_feature, past_answerCode = past_cat_feature.to(device), past_num_feature.to(device), past_answerCode\n",
    "        now_cat_feature, now_num_feature, now_answerCode = now_cat_feature.to(device), now_num_feature.to(device), now_answerCode.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature).squeeze(2)\n",
    "        loss = criterion(output[now_answerCode != -1], now_answerCode[now_answerCode != -1])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val += loss.item()\n",
    "\n",
    "    loss_val /= len(data_loader)\n",
    "\n",
    "    return loss_val\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    target = []\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature, now_answerCode in data_loader:\n",
    "            past_cat_feature, past_num_feature, past_answerCode = past_cat_feature.to(device), past_num_feature.to(device), past_answerCode\n",
    "            now_cat_feature, now_num_feature, now_answerCode = now_cat_feature.to(device), now_num_feature.to(device), now_answerCode.to(device)\n",
    "            \n",
    "            output = model(past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature).squeeze(2)\n",
    "\n",
    "            target.extend(now_answerCode[:, -1].cpu().numpy().tolist())\n",
    "            pred.extend(output[:, -1].cpu().numpy().tolist())\n",
    "\n",
    "    roc_auc = roc_auc_score(target, pred)\n",
    "\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature, now_answerCode in data_loader:\n",
    "            past_cat_feature, past_num_feature, past_answerCode = past_cat_feature.to(device), past_num_feature.to(device), past_answerCode\n",
    "            now_cat_feature, now_num_feature = now_cat_feature.to(device), now_num_feature.to(device)\n",
    "            \n",
    "            output = model(past_cat_feature, past_num_feature, past_answerCode, now_cat_feature, now_num_feature).squeeze(2)\n",
    "            pred.extend(output[:, -1].cpu().numpy().tolist())\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "lr = 0.001\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "emb_size = 64\n",
    "hidden_units = 128\n",
    "num_heads = 2 # 2,4,8,16,32\n",
    "num_layers = 1\n",
    "dropout_rate = 0.5\n",
    "num_workers = 8\n",
    "\n",
    "max_len = 50\n",
    "window = 10\n",
    "data_augmentation = False\n",
    "\n",
    "DATA_PATH = '/opt/ml/input/data'\n",
    "MODEL_PATH = '/opt/ml/model'\n",
    "SUBMISSION_PATH = '/opt/ml/submission'\n",
    "\n",
    "model_name = 'Transformer-and-LSTM-Encoder-Decoder-each-Embedding-add-feature.pt'\n",
    "submission_name = 'Transformer-and-LSTM-Encoder-Decoder-each-Embedding-add-feature.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(MODEL_PATH):\n",
    "    os.mkdir(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(SUBMISSION_PATH):\n",
    "    os.mkdir(SUBMISSION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dataset = MakeDataset(DATA_PATH = DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOF Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.51096| roc_auc: 0.82697: 100%|██████████| 1/1 [02:05<00:00, 125.52s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46838| roc_auc: 0.84584: 100%|██████████| 1/1 [02:03<00:00, 123.60s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45416| roc_auc: 0.84911: 100%|██████████| 1/1 [02:04<00:00, 124.01s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44686| roc_auc: 0.85202: 100%|██████████| 1/1 [02:03<00:00, 123.37s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44319| roc_auc: 0.85429: 100%|██████████| 1/1 [02:03<00:00, 123.33s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.44022| roc_auc: 0.85450: 100%|██████████| 1/1 [02:02<00:00, 122.78s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.43847| roc_auc: 0.85417: 100%|██████████| 1/1 [02:03<00:00, 123.18s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.43620| roc_auc: 0.85411: 100%|██████████| 1/1 [02:02<00:00, 122.66s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.43414| roc_auc: 0.85365: 100%|██████████| 1/1 [02:05<00:00, 125.45s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.43314| roc_auc: 0.85443: 100%|██████████| 1/1 [02:09<00:00, 129.19s/it]\n",
      "OOF-0| Epoch:  11| Train loss: 0.43157| roc_auc: 0.85739: 100%|██████████| 1/1 [02:11<00:00, 131.29s/it]\n",
      "OOF-0| Epoch:  12| Train loss: 0.43010| roc_auc: 0.85597: 100%|██████████| 1/1 [02:06<00:00, 126.74s/it]\n",
      "OOF-0| Epoch:  13| Train loss: 0.42896| roc_auc: 0.85432: 100%|██████████| 1/1 [02:06<00:00, 126.54s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "oof_roc_auc = 0\n",
    "\n",
    "for oof in make_dataset.oof_user_set.keys():\n",
    "    train_df, valid_df = make_dataset.get_oof_data(oof)\n",
    "    \n",
    "    seed_everything(22 + oof)\n",
    "    \n",
    "    train_dataset = CustomDataset(df = train_df,)\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size = batch_size, \n",
    "        shuffle = True, \n",
    "        drop_last = False,\n",
    "        collate_fn = train_make_batch,\n",
    "        num_workers = num_workers)\n",
    "\n",
    "    valid_dataset = CustomDataset(df = valid_df,)\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = 1, \n",
    "        shuffle = False, \n",
    "        drop_last = False,\n",
    "        collate_fn = train_make_batch,\n",
    "        num_workers = num_workers)\n",
    "\n",
    "    model = SASRec(\n",
    "        num_assessmentItemID = make_dataset.num_assessmentItemID, \n",
    "        num_testId = make_dataset.num_testId,\n",
    "        num_KnowledgeTag = make_dataset.num_KnowledgeTag,\n",
    "        num_large_paper_number = make_dataset.num_large_paper_number,\n",
    "        num_hour = make_dataset.num_hour,\n",
    "        num_dayofweek = make_dataset.num_dayofweek,\n",
    "        num_cols = train_dataset.num_cols,\n",
    "        cat_cols = train_dataset.cat_cols,\n",
    "        emb_size = emb_size,\n",
    "        hidden_units = hidden_units,\n",
    "        num_heads = num_heads,\n",
    "        num_layers = num_layers,\n",
    "        dropout_rate = dropout_rate,\n",
    "        device = device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # pre_emb = Word2Vec.load(os.path.join(MODEL_PATH, 'Word2Vec_Embedding_Model_window_50.model'))\n",
    "\n",
    "    # assessmentItemID_li = make_dataset.assessmentItemID2idx.keys()\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     for assessmentItemID in assessmentItemID_li:\n",
    "    #         idx = make_dataset.assessmentItemID2idx[assessmentItemID]\n",
    "    #         model.assessmentItemID_emb.weight[idx + 1] = torch.tensor(pre_emb.wv[assessmentItemID]).to(device)\n",
    "\n",
    "    best_epoch = 0\n",
    "    best_train_loss = 0\n",
    "    best_roc_auc = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tbar = tqdm(range(1))\n",
    "        for _ in tbar:\n",
    "            train_loss = train(model = model, data_loader = train_data_loader, criterion = criterion, optimizer = optimizer)\n",
    "            roc_auc = evaluate(model = model, data_loader = valid_data_loader)\n",
    "            if best_roc_auc < roc_auc:\n",
    "                best_epoch = epoch\n",
    "                best_train_loss = train_loss\n",
    "                best_roc_auc = roc_auc\n",
    "                torch.save(model.state_dict(), os.path.join(MODEL_PATH, f'oof_{oof}_' + model_name))\n",
    "\n",
    "            tbar.set_description(f'OOF-{oof}| Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| roc_auc: {roc_auc:.5f}')\n",
    "    \n",
    "    print(f'BEST OOF-{oof}| Epoch: {best_epoch:3d}| Train loss: {best_train_loss:.5f}| roc_auc: {best_roc_auc:.5f}')\n",
    "\n",
    "    oof_roc_auc += best_roc_auc\n",
    "\n",
    "print(f'Total roc_auc: {oof_roc_auc / len(make_dataset.oof_user_set.keys()):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "OOF-0| Epoch:   1| Train loss: 0.52036| roc_auc: 0.81918: 100%|██████████| 1/1 [01:54<00:00, 114.87s/it]\n",
    "OOF-0| Epoch:   2| Train loss: 0.47700| roc_auc: 0.84003: 100%|██████████| 1/1 [01:57<00:00, 117.49s/it]\n",
    "OOF-0| Epoch:   3| Train loss: 0.45824| roc_auc: 0.84969: 100%|██████████| 1/1 [01:58<00:00, 118.67s/it]\n",
    "OOF-0| Epoch:   4| Train loss: 0.45016| roc_auc: 0.85025: 100%|██████████| 1/1 [01:57<00:00, 117.37s/it]\n",
    "OOF-0| Epoch:   5| Train loss: 0.44466| roc_auc: 0.85088: 100%|██████████| 1/1 [01:57<00:00, 117.79s/it]\n",
    "OOF-0| Epoch:   6| Train loss: 0.44167| roc_auc: 0.85585: 100%|██████████| 1/1 [01:55<00:00, 115.66s/it]\n",
    "OOF-0| Epoch:   7| Train loss: 0.43878| roc_auc: 0.85691: 100%|██████████| 1/1 [01:56<00:00, 116.95s/it]\n",
    "OOF-0| Epoch:   8| Train loss: 0.43657| roc_auc: 0.85590: 100%|██████████| 1/1 [01:55<00:00, 115.28s/it]\n",
    "OOF-0| Epoch:   9| Train loss: 0.43426| roc_auc: 0.85459: 100%|██████████| 1/1 [01:57<00:00, 117.32s/it]\n",
    "OOF-0| Epoch:  10| Train loss: 0.43261| roc_auc: 0.85731: 100%|██████████| 1/1 [01:54<00:00, 114.69s/it]\n",
    "OOF-0| Epoch:  11| Train loss: 0.43107| roc_auc: 0.85737: 100%|██████████| 1/1 [01:56<00:00, 116.13s/it]\n",
    "OOF-0| Epoch:  12| Train loss: 0.42911| roc_auc: 0.85611: 100%|██████████| 1/1 [01:57<00:00, 117.01s/it]\n",
    "OOF-0| Epoch:  13| Train loss: 0.42757| roc_auc: 0.85523: 100%|██████████| 1/1 [01:55<00:00, 115.94s/it]\n",
    "OOF-0| Epoch:  14| Train loss: 0.42626| roc_auc: 0.85689: 100%|██████████| 1/1 [01:56<00:00, 116.23s/it]\n",
    "OOF-0| Epoch:  15| Train loss: 0.42450| roc_auc: 0.85636: 100%|██████████| 1/1 [01:54<00:00, 114.13s/it]\n",
    "OOF-0| Epoch:  16| Train loss: 0.42297| roc_auc: 0.85745: 100%|██████████| 1/1 [01:55<00:00, 115.89s/it]\n",
    "OOF-0| Epoch:  17| Train loss: 0.42085| roc_auc: 0.85434: 100%|██████████| 1/1 [01:56<00:00, 116.49s/it]\n",
    "OOF-0| Epoch:  18| Train loss: 0.41954| roc_auc: 0.85886: 100%|██████████| 1/1 [01:58<00:00, 118.36s/it]\n",
    "OOF-0| Epoch:  19| Train loss: 0.41753| roc_auc: 0.85614: 100%|██████████| 1/1 [01:57<00:00, 117.64s/it]\n",
    "OOF-0| Epoch:  20| Train loss: 0.41564| roc_auc: 0.85646: 100%|██████████| 1/1 [01:58<00:00, 118.19s/it]\n",
    "BEST OOF-0| Epoch:  18| Train loss: 0.41954| roc_auc: 0.85886\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "num-head-8\n",
    "\n",
    "OOF-0| Epoch:   1| Train loss: 0.50920| roc_auc: 0.82475: 100%|██████████| 1/1 [02:19<00:00, 139.90s/it]\n",
    "OOF-0| Epoch:   2| Train loss: 0.46884| roc_auc: 0.84497: 100%|██████████| 1/1 [02:18<00:00, 138.22s/it]\n",
    "OOF-0| Epoch:   3| Train loss: 0.45432| roc_auc: 0.85040: 100%|██████████| 1/1 [02:19<00:00, 139.15s/it]\n",
    "OOF-0| Epoch:   4| Train loss: 0.44692| roc_auc: 0.85332: 100%|██████████| 1/1 [02:16<00:00, 136.10s/it]\n",
    "OOF-0| Epoch:   5| Train loss: 0.44264| roc_auc: 0.85380: 100%|██████████| 1/1 [02:16<00:00, 136.87s/it]\n",
    "OOF-0| Epoch:   6| Train loss: 0.44024| roc_auc: 0.85650: 100%|██████████| 1/1 [02:17<00:00, 137.41s/it]\n",
    "OOF-0| Epoch:   7| Train loss: 0.43706| roc_auc: 0.85545: 100%|██████████| 1/1 [02:15<00:00, 135.47s/it]\n",
    "OOF-0| Epoch:   8| Train loss: 0.43546| roc_auc: 0.85398: 100%|██████████| 1/1 [02:18<00:00, 138.32s/it]\n",
    "OOF-0| Epoch:   9| Train loss: 0.43349| roc_auc: 0.85546: 100%|██████████| 1/1 [02:16<00:00, 136.43s/it]\n",
    "OOF-0| Epoch:  10| Train loss: 0.43202| roc_auc: 0.85714: 100%|██████████| 1/1 [02:17<00:00, 137.22s/it]\n",
    "OOF-0| Epoch:  11| Train loss: 0.43065| roc_auc: 0.85590: 100%|██████████| 1/1 [02:18<00:00, 138.73s/it]\n",
    "OOF-0| Epoch:  12| Train loss: 0.42957| roc_auc: 0.85310: 100%|██████████| 1/1 [02:16<00:00, 136.90s/it]\n",
    "OOF-0| Epoch:  13| Train loss: 0.42805| roc_auc: 0.85498: 100%|██████████| 1/1 [02:16<00:00, 136.56s/it]\n",
    "OOF-0| Epoch:  14| Train loss: 0.42686| roc_auc: 0.85525: 100%|██████████| 1/1 [02:17<00:00, 137.80s/it]\n",
    "OOF-0| Epoch:  15| Train loss: 0.42572| roc_auc: 0.85763: 100%|██████████| 1/1 [02:16<00:00, 136.77s/it]\n",
    "OOF-0| Epoch:  16| Train loss: 0.42424| roc_auc: 0.85718: 100%|██████████| 1/1 [02:18<00:00, 138.39s/it]\n",
    "OOF-0| Epoch:  17| Train loss: 0.42323| roc_auc: 0.85417: 100%|██████████| 1/1 [02:18<00:00, 138.44s/it]\n",
    "OOF-0| Epoch:  18| Train loss: 0.42205| roc_auc: 0.85348: 100%|██████████| 1/1 [02:23<00:00, 143.78s/it]\n",
    "OOF-0| Epoch:  19| Train loss: 0.42115| roc_auc: 0.85546: 100%|██████████| 1/1 [02:17<00:00, 137.05s/it]\n",
    "OOF-0| Epoch:  20| Train loss: 0.41956| roc_auc: 0.85327: 100%|██████████| 1/1 [02:18<00:00, 138.90s/it]\n",
    "BEST OOF-0| Epoch:  15| Train loss: 0.42572| roc_auc: 0.85763\n",
    "\n",
    "OOF-1| Epoch:   1| Train loss: 0.51279| roc_auc: 0.81536: 100%|██████████| 1/1 [02:22<00:00, 142.50s/it]\n",
    "OOF-1| Epoch:   2| Train loss: 0.46936| roc_auc: 0.83921: 100%|██████████| 1/1 [02:22<00:00, 142.29s/it]\n",
    "OOF-1| Epoch:   3| Train loss: 0.45532| roc_auc: 0.84452: 100%|██████████| 1/1 [02:21<00:00, 141.83s/it]\n",
    "OOF-1| Epoch:   4| Train loss: 0.44919| roc_auc: 0.84952: 100%|██████████| 1/1 [02:22<00:00, 142.93s/it]\n",
    "OOF-1| Epoch:   5| Train loss: 0.44409| roc_auc: 0.85425: 100%|██████████| 1/1 [02:36<00:00, 156.87s/it]\n",
    "OOF-1| Epoch:   6| Train loss: 0.44063| roc_auc: 0.85290: 100%|██████████| 1/1 [02:21<00:00, 141.98s/it]\n",
    "OOF-1| Epoch:   7| Train loss: 0.43809| roc_auc: 0.85419: 100%|██████████| 1/1 [02:20<00:00, 140.42s/it]\n",
    "OOF-1| Epoch:   8| Train loss: 0.43596| roc_auc: 0.85666: 100%|██████████| 1/1 [02:21<00:00, 141.32s/it]\n",
    "OOF-1| Epoch:   9| Train loss: 0.43374| roc_auc: 0.85673: 100%|██████████| 1/1 [02:21<00:00, 141.56s/it]\n",
    "OOF-1| Epoch:  10| Train loss: 0.43214| roc_auc: 0.85754: 100%|██████████| 1/1 [02:20<00:00, 140.37s/it]\n",
    "OOF-1| Epoch:  11| Train loss: 0.43081| roc_auc: 0.85575: 100%|██████████| 1/1 [02:19<00:00, 139.63s/it]\n",
    "OOF-1| Epoch:  12| Train loss: 0.42971| roc_auc: 0.85489: 100%|██████████| 1/1 [02:21<00:00, 141.79s/it]\n",
    "OOF-1| Epoch:  13| Train loss: 0.42812| roc_auc: 0.85803: 100%|██████████| 1/1 [02:21<00:00, 141.54s/it]\n",
    "OOF-1| Epoch:  14| Train loss: 0.42707| roc_auc: 0.85766: 100%|██████████| 1/1 [02:20<00:00, 140.56s/it]\n",
    "OOF-1| Epoch:  15| Train loss: 0.42542| roc_auc: 0.85789: 100%|██████████| 1/1 [02:21<00:00, 141.67s/it]\n",
    "OOF-1| Epoch:  16| Train loss: 0.42444| roc_auc: 0.85779: 100%|██████████| 1/1 [02:20<00:00, 140.40s/it]\n",
    "OOF-1| Epoch:  17| Train loss: 0.42325| roc_auc: 0.85840: 100%|██████████| 1/1 [02:20<00:00, 140.55s/it]\n",
    "OOF-1| Epoch:  18| Train loss: 0.42214| roc_auc: 0.85964: 100%|██████████| 1/1 [02:24<00:00, 144.61s/it]\n",
    "OOF-1| Epoch:  19| Train loss: 0.42051| roc_auc: 0.85680: 100%|██████████| 1/1 [02:21<00:00, 141.18s/it]\n",
    "OOF-1| Epoch:  20| Train loss: 0.41990| roc_auc: 0.85859: 100%|██████████| 1/1 [02:18<00:00, 138.96s/it]\n",
    "BEST OOF-1| Epoch:  18| Train loss: 0.42214| roc_auc: 0.85964\n",
    "\n",
    "OOF-2| Epoch:   1| Train loss: 0.51173| roc_auc: 0.79473: 100%|██████████| 1/1 [02:17<00:00, 137.77s/it]\n",
    "OOF-2| Epoch:   2| Train loss: 0.46982| roc_auc: 0.81169: 100%|██████████| 1/1 [02:17<00:00, 137.17s/it]\n",
    "OOF-2| Epoch:   3| Train loss: 0.45566| roc_auc: 0.82065: 100%|██████████| 1/1 [02:20<00:00, 140.65s/it]\n",
    "OOF-2| Epoch:   4| Train loss: 0.44883| roc_auc: 0.82155: 100%|██████████| 1/1 [02:30<00:00, 150.64s/it]\n",
    "OOF-2| Epoch:   5| Train loss: 0.44482| roc_auc: 0.82134: 100%|██████████| 1/1 [02:17<00:00, 137.73s/it]\n",
    "OOF-2| Epoch:   6| Train loss: 0.44070| roc_auc: 0.82342: 100%|██████████| 1/1 [02:22<00:00, 142.38s/it]\n",
    "OOF-2| Epoch:   7| Train loss: 0.43813| roc_auc: 0.82699: 100%|██████████| 1/1 [02:28<00:00, 148.92s/it]\n",
    "OOF-2| Epoch:   8| Train loss: 0.43610| roc_auc: 0.82629: 100%|██████████| 1/1 [02:26<00:00, 146.50s/it]\n",
    "OOF-2| Epoch:   9| Train loss: 0.43438| roc_auc: 0.82808: 100%|██████████| 1/1 [02:34<00:00, 154.04s/it]\n",
    "OOF-2| Epoch:  10| Train loss: 0.43267| roc_auc: 0.82730: 100%|██████████| 1/1 [02:27<00:00, 147.95s/it]\n",
    "OOF-2| Epoch:  11| Train loss: 0.43127| roc_auc: 0.82876: 100%|██████████| 1/1 [02:22<00:00, 142.05s/it]\n",
    "OOF-2| Epoch:  12| Train loss: 0.43009| roc_auc: 0.82857: 100%|██████████| 1/1 [02:17<00:00, 137.14s/it]\n",
    "OOF-2| Epoch:  13| Train loss: 0.42863| roc_auc: 0.82909: 100%|██████████| 1/1 [02:21<00:00, 141.41s/it]\n",
    "OOF-2| Epoch:  14| Train loss: 0.42723| roc_auc: 0.83026: 100%|██████████| 1/1 [02:22<00:00, 142.32s/it]\n",
    "OOF-2| Epoch:  15| Train loss: 0.42572| roc_auc: 0.82854: 100%|██████████| 1/1 [02:20<00:00, 140.03s/it]\n",
    "OOF-2| Epoch:  16| Train loss: 0.42461| roc_auc: 0.82929: 100%|██████████| 1/1 [02:21<00:00, 141.04s/it]\n",
    "OOF-2| Epoch:  17| Train loss: 0.42370| roc_auc: 0.83052: 100%|██████████| 1/1 [02:27<00:00, 147.07s/it]\n",
    "OOF-2| Epoch:  18| Train loss: 0.42196| roc_auc: 0.82952: 100%|██████████| 1/1 [02:20<00:00, 140.20s/it]\n",
    "OOF-2| Epoch:  19| Train loss: 0.42149| roc_auc: 0.83075: 100%|██████████| 1/1 [02:20<00:00, 140.22s/it]\n",
    "OOF-2| Epoch:  20| Train loss: 0.41978| roc_auc: 0.82729: 100%|██████████| 1/1 [02:21<00:00, 141.82s/it]\n",
    "BEST OOF-2| Epoch:  19| Train loss: 0.42149| roc_auc: 0.83075\n",
    "\n",
    "OOF-3| Epoch:   1| Train loss: 0.50995| roc_auc: 0.79665: 100%|██████████| 1/1 [02:37<00:00, 157.60s/it]\n",
    "OOF-3| Epoch:   2| Train loss: 0.46852| roc_auc: 0.81843: 100%|██████████| 1/1 [02:38<00:00, 158.77s/it]\n",
    "OOF-3| Epoch:   3| Train loss: 0.45517| roc_auc: 0.82834: 100%|██████████| 1/1 [02:37<00:00, 157.10s/it]\n",
    "OOF-3| Epoch:   4| Train loss: 0.44760| roc_auc: 0.82956: 100%|██████████| 1/1 [02:31<00:00, 151.33s/it]\n",
    "OOF-3| Epoch:   5| Train loss: 0.44386| roc_auc: 0.82926: 100%|██████████| 1/1 [02:28<00:00, 148.99s/it]\n",
    "OOF-3| Epoch:   6| Train loss: 0.44080| roc_auc: 0.83329: 100%|██████████| 1/1 [02:25<00:00, 145.31s/it]\n",
    "OOF-3| Epoch:   7| Train loss: 0.43836| roc_auc: 0.83230: 100%|██████████| 1/1 [02:24<00:00, 144.73s/it]\n",
    "OOF-3| Epoch:   8| Train loss: 0.43664| roc_auc: 0.83361: 100%|██████████| 1/1 [02:27<00:00, 147.86s/it]\n",
    "OOF-3| Epoch:   9| Train loss: 0.43412| roc_auc: 0.83277: 100%|██████████| 1/1 [02:25<00:00, 145.65s/it]\n",
    "OOF-3| Epoch:  10| Train loss: 0.43257| roc_auc: 0.83396: 100%|██████████| 1/1 [02:29<00:00, 149.01s/it]\n",
    "OOF-3| Epoch:  11| Train loss: 0.43092| roc_auc: 0.83664: 100%|██████████| 1/1 [02:25<00:00, 145.97s/it]\n",
    "OOF-3| Epoch:  12| Train loss: 0.42942| roc_auc: 0.83432: 100%|██████████| 1/1 [02:24<00:00, 144.50s/it]\n",
    "OOF-3| Epoch:  13| Train loss: 0.42819| roc_auc: 0.83605: 100%|██████████| 1/1 [02:23<00:00, 143.15s/it]\n",
    "OOF-3| Epoch:  14| Train loss: 0.42680| roc_auc: 0.83396: 100%|██████████| 1/1 [02:32<00:00, 152.35s/it]\n",
    "OOF-3| Epoch:  15| Train loss: 0.42528| roc_auc: 0.83877: 100%|██████████| 1/1 [02:25<00:00, 145.03s/it]\n",
    "OOF-3| Epoch:  16| Train loss: 0.42438| roc_auc: 0.83804: 100%|██████████| 1/1 [02:22<00:00, 142.87s/it]\n",
    "OOF-3| Epoch:  17| Train loss: 0.42302| roc_auc: 0.83676: 100%|██████████| 1/1 [02:24<00:00, 144.06s/it]\n",
    "OOF-3| Epoch:  18| Train loss: 0.42179| roc_auc: 0.83798: 100%|██████████| 1/1 [02:27<00:00, 147.40s/it]\n",
    "OOF-3| Epoch:  19| Train loss: 0.42055| roc_auc: 0.83865: 100%|██████████| 1/1 [02:23<00:00, 143.90s/it]\n",
    "OOF-3| Epoch:  20| Train loss: 0.41964| roc_auc: 0.83802: 100%|██████████| 1/1 [02:30<00:00, 150.22s/it]\n",
    "BEST OOF-3| Epoch:  15| Train loss: 0.42528| roc_auc: 0.83877\n",
    "\n",
    "OOF-4| Epoch:   1| Train loss: 0.50810| roc_auc: 0.80734: 100%|██████████| 1/1 [02:26<00:00, 146.30s/it]\n",
    "OOF-4| Epoch:   2| Train loss: 0.46918| roc_auc: 0.82926: 100%|██████████| 1/1 [02:23<00:00, 143.44s/it]\n",
    "OOF-4| Epoch:   3| Train loss: 0.45513| roc_auc: 0.83609: 100%|██████████| 1/1 [02:23<00:00, 143.62s/it]\n",
    "OOF-4| Epoch:   4| Train loss: 0.44781| roc_auc: 0.83864: 100%|██████████| 1/1 [02:24<00:00, 144.23s/it]\n",
    "OOF-4| Epoch:   5| Train loss: 0.44328| roc_auc: 0.83966: 100%|██████████| 1/1 [02:24<00:00, 144.25s/it]\n",
    "OOF-4| Epoch:   6| Train loss: 0.44016| roc_auc: 0.84340: 100%|██████████| 1/1 [02:22<00:00, 142.59s/it]\n",
    "OOF-4| Epoch:   7| Train loss: 0.43716| roc_auc: 0.84532: 100%|██████████| 1/1 [02:30<00:00, 150.12s/it]\n",
    "OOF-4| Epoch:   8| Train loss: 0.43563| roc_auc: 0.84293: 100%|██████████| 1/1 [02:24<00:00, 144.37s/it]\n",
    "OOF-4| Epoch:   9| Train loss: 0.43350| roc_auc: 0.84527: 100%|██████████| 1/1 [02:25<00:00, 145.76s/it]\n",
    "OOF-4| Epoch:  10| Train loss: 0.43198| roc_auc: 0.84752: 100%|██████████| 1/1 [02:24<00:00, 144.53s/it]\n",
    "OOF-4| Epoch:  11| Train loss: 0.43080| roc_auc: 0.84394: 100%|██████████| 1/1 [02:24<00:00, 144.84s/it]\n",
    "OOF-4| Epoch:  12| Train loss: 0.42884| roc_auc: 0.84691: 100%|██████████| 1/1 [02:24<00:00, 144.77s/it]\n",
    "OOF-4| Epoch:  13| Train loss: 0.42803| roc_auc: 0.84576: 100%|██████████| 1/1 [02:23<00:00, 143.13s/it]\n",
    "OOF-4| Epoch:  14| Train loss: 0.42662| roc_auc: 0.84762: 100%|██████████| 1/1 [02:25<00:00, 145.26s/it]\n",
    "OOF-4| Epoch:  15| Train loss: 0.42544| roc_auc: 0.84574: 100%|██████████| 1/1 [02:27<00:00, 147.13s/it]\n",
    "OOF-4| Epoch:  16| Train loss: 0.42445| roc_auc: 0.84528: 100%|██████████| 1/1 [02:26<00:00, 146.96s/it]\n",
    "OOF-4| Epoch:  17| Train loss: 0.42323| roc_auc: 0.84549: 100%|██████████| 1/1 [02:25<00:00, 145.53s/it]\n",
    "OOF-4| Epoch:  18| Train loss: 0.42183| roc_auc: 0.84524: 100%|██████████| 1/1 [02:23<00:00, 143.60s/it]\n",
    "OOF-4| Epoch:  19| Train loss: 0.42043| roc_auc: 0.84609: 100%|██████████| 1/1 [02:24<00:00, 144.84s/it]\n",
    "OOF-4| Epoch:  20| Train loss: 0.41954| roc_auc: 0.84506: 100%|██████████| 1/1 [02:26<00:00, 146.87s/it]\n",
    "BEST OOF-4| Epoch:  14| Train loss: 0.42662| roc_auc: 0.84762\n",
    "\n",
    "Total roc_auc: 0.84688\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "num-head-4\n",
    "OOF-0| Epoch:   1| Train loss: 0.50902| roc_auc: 0.82562: 100%|██████████| 1/1 [01:57<00:00, 117.15s/it]\n",
    "OOF-0| Epoch:   2| Train loss: 0.46809| roc_auc: 0.84612: 100%|██████████| 1/1 [01:55<00:00, 115.90s/it]\n",
    "OOF-0| Epoch:   3| Train loss: 0.45299| roc_auc: 0.85097: 100%|██████████| 1/1 [01:56<00:00, 116.11s/it]\n",
    "OOF-0| Epoch:   4| Train loss: 0.44614| roc_auc: 0.85433: 100%|██████████| 1/1 [01:54<00:00, 114.93s/it]\n",
    "OOF-0| Epoch:   5| Train loss: 0.44228| roc_auc: 0.85423: 100%|██████████| 1/1 [01:56<00:00, 116.62s/it]\n",
    "OOF-0| Epoch:   6| Train loss: 0.43995| roc_auc: 0.85500: 100%|██████████| 1/1 [01:56<00:00, 116.18s/it]\n",
    "OOF-0| Epoch:   7| Train loss: 0.43697| roc_auc: 0.85591: 100%|██████████| 1/1 [01:55<00:00, 115.39s/it]\n",
    "OOF-0| Epoch:   8| Train loss: 0.43548| roc_auc: 0.85338: 100%|██████████| 1/1 [01:55<00:00, 115.17s/it]\n",
    "OOF-0| Epoch:   9| Train loss: 0.43343| roc_auc: 0.85503: 100%|██████████| 1/1 [01:55<00:00, 115.27s/it]\n",
    "OOF-0| Epoch:  10| Train loss: 0.43202| roc_auc: 0.85662: 100%|██████████| 1/1 [01:55<00:00, 115.12s/it]\n",
    "OOF-0| Epoch:  11| Train loss: 0.43063| roc_auc: 0.85552: 100%|██████████| 1/1 [01:55<00:00, 115.99s/it]\n",
    "OOF-0| Epoch:  12| Train loss: 0.42975| roc_auc: 0.85550: 100%|██████████| 1/1 [01:53<00:00, 113.77s/it]\n",
    "OOF-0| Epoch:  13| Train loss: 0.42828| roc_auc: 0.85552: 100%|██████████| 1/1 [01:55<00:00, 115.69s/it]\n",
    "OOF-0| Epoch:  14| Train loss: 0.42702| roc_auc: 0.85437: 100%|██████████| 1/1 [01:56<00:00, 116.17s/it]\n",
    "OOF-0| Epoch:  15| Train loss: 0.42586| roc_auc: 0.85719: 100%|██████████| 1/1 [01:54<00:00, 114.93s/it]\n",
    "OOF-0| Epoch:  16| Train loss: 0.42442| roc_auc: 0.85702: 100%|██████████| 1/1 [01:53<00:00, 113.31s/it]\n",
    "OOF-0| Epoch:  17| Train loss: 0.42344| roc_auc: 0.85581: 100%|██████████| 1/1 [01:55<00:00, 115.23s/it]\n",
    "OOF-0| Epoch:  18| Train loss: 0.42239| roc_auc: 0.85651: 100%|██████████| 1/1 [01:54<00:00, 114.78s/it]\n",
    "OOF-0| Epoch:  19| Train loss: 0.42130| roc_auc: 0.85805: 100%|██████████| 1/1 [01:53<00:00, 113.14s/it]\n",
    "OOF-0| Epoch:  20| Train loss: 0.41978| roc_auc: 0.85687: 100%|██████████| 1/1 [01:54<00:00, 114.15s/it]\n",
    "BEST OOF-0| Epoch:  19| Train loss: 0.42130| roc_auc: 0.85805\n",
    "OOF-1| Epoch:   1| Train loss: 0.51285| roc_auc: 0.81700: 100%|██████████| 1/1 [01:55<00:00, 115.82s/it]\n",
    "OOF-1| Epoch:   2| Train loss: 0.46937| roc_auc: 0.83834: 100%|██████████| 1/1 [01:57<00:00, 117.06s/it]\n",
    "OOF-1| Epoch:   3| Train loss: 0.45446| roc_auc: 0.84686: 100%|██████████| 1/1 [01:56<00:00, 116.05s/it]\n",
    "OOF-1| Epoch:   4| Train loss: 0.44771| roc_auc: 0.85008: 100%|██████████| 1/1 [01:57<00:00, 117.28s/it]\n",
    "OOF-1| Epoch:   5| Train loss: 0.44301| roc_auc: 0.85404: 100%|██████████| 1/1 [01:58<00:00, 118.22s/it]\n",
    "OOF-1| Epoch:   6| Train loss: 0.44004| roc_auc: 0.85412: 100%|██████████| 1/1 [01:57<00:00, 117.40s/it]\n",
    "OOF-1| Epoch:   7| Train loss: 0.43781| roc_auc: 0.85416: 100%|██████████| 1/1 [01:56<00:00, 116.22s/it]\n",
    "OOF-1| Epoch:   8| Train loss: 0.43591| roc_auc: 0.85418: 100%|██████████| 1/1 [01:56<00:00, 116.29s/it]\n",
    "OOF-1| Epoch:   9| Train loss: 0.43387| roc_auc: 0.85624: 100%|██████████| 1/1 [01:57<00:00, 117.77s/it]\n",
    "OOF-1| Epoch:  10| Train loss: 0.43199| roc_auc: 0.85756: 100%|██████████| 1/1 [01:56<00:00, 116.91s/it]\n",
    "OOF-1| Epoch:  11| Train loss: 0.43090| roc_auc: 0.85780: 100%|██████████| 1/1 [01:57<00:00, 117.43s/it]\n",
    "OOF-1| Epoch:  12| Train loss: 0.42961| roc_auc: 0.85557: 100%|██████████| 1/1 [01:56<00:00, 116.16s/it]\n",
    "OOF-1| Epoch:  13| Train loss: 0.42827| roc_auc: 0.85748: 100%|██████████| 1/1 [01:56<00:00, 116.58s/it]\n",
    "OOF-1| Epoch:  14| Train loss: 0.42734| roc_auc: 0.85987: 100%|██████████| 1/1 [01:57<00:00, 117.85s/it]\n",
    "OOF-1| Epoch:  15| Train loss: 0.42588| roc_auc: 0.85912: 100%|██████████| 1/1 [01:55<00:00, 115.99s/it]\n",
    "OOF-1| Epoch:  16| Train loss: 0.42482| roc_auc: 0.86096: 100%|██████████| 1/1 [01:58<00:00, 118.42s/it]\n",
    "OOF-1| Epoch:  17| Train loss: 0.42367| roc_auc: 0.86259: 100%|██████████| 1/1 [01:55<00:00, 115.72s/it]\n",
    "OOF-1| Epoch:  18| Train loss: 0.42282| roc_auc: 0.86145: 100%|██████████| 1/1 [01:58<00:00, 118.47s/it]\n",
    "OOF-1| Epoch:  19| Train loss: 0.42118| roc_auc: 0.85960: 100%|██████████| 1/1 [01:58<00:00, 118.09s/it]\n",
    "OOF-1| Epoch:  20| Train loss: 0.42068| roc_auc: 0.86045: 100%|██████████| 1/1 [01:57<00:00, 117.78s/it]\n",
    "BEST OOF-1| Epoch:  17| Train loss: 0.42367| roc_auc: 0.86259\n",
    "OOF-2| Epoch:   1| Train loss: 0.51174| roc_auc: 0.79507: 100%|██████████| 1/1 [01:53<00:00, 113.93s/it]\n",
    "OOF-2| Epoch:   2| Train loss: 0.46915| roc_auc: 0.81242: 100%|██████████| 1/1 [01:56<00:00, 116.49s/it]\n",
    "OOF-2| Epoch:   3| Train loss: 0.45443| roc_auc: 0.82134: 100%|██████████| 1/1 [01:56<00:00, 116.67s/it]\n",
    "OOF-2| Epoch:   4| Train loss: 0.44754| roc_auc: 0.82415: 100%|██████████| 1/1 [01:56<00:00, 116.01s/it]\n",
    "OOF-2| Epoch:   5| Train loss: 0.44379| roc_auc: 0.82391: 100%|██████████| 1/1 [01:53<00:00, 113.19s/it]\n",
    "OOF-2| Epoch:   6| Train loss: 0.44011| roc_auc: 0.82470: 100%|██████████| 1/1 [01:56<00:00, 116.60s/it]\n",
    "OOF-2| Epoch:   7| Train loss: 0.43781| roc_auc: 0.82604: 100%|██████████| 1/1 [01:57<00:00, 117.48s/it]\n",
    "OOF-2| Epoch:   8| Train loss: 0.43609| roc_auc: 0.82529: 100%|██████████| 1/1 [01:56<00:00, 116.44s/it]\n",
    "OOF-2| Epoch:   9| Train loss: 0.43446| roc_auc: 0.82994: 100%|██████████| 1/1 [01:57<00:00, 117.08s/it]\n",
    "OOF-2| Epoch:  10| Train loss: 0.43298| roc_auc: 0.82598: 100%|██████████| 1/1 [01:53<00:00, 113.98s/it]\n",
    "OOF-2| Epoch:  11| Train loss: 0.43168| roc_auc: 0.83130: 100%|██████████| 1/1 [01:54<00:00, 114.59s/it]\n",
    "OOF-2| Epoch:  12| Train loss: 0.43063| roc_auc: 0.83011: 100%|██████████| 1/1 [01:55<00:00, 115.30s/it]\n",
    "OOF-2| Epoch:  13| Train loss: 0.42915| roc_auc: 0.83015: 100%|██████████| 1/1 [01:56<00:00, 116.12s/it]\n",
    "OOF-2| Epoch:  14| Train loss: 0.42755| roc_auc: 0.83018: 100%|██████████| 1/1 [01:56<00:00, 116.48s/it]\n",
    "OOF-2| Epoch:  15| Train loss: 0.42616| roc_auc: 0.82988: 100%|██████████| 1/1 [01:55<00:00, 115.58s/it]\n",
    "OOF-2| Epoch:  16| Train loss: 0.42525| roc_auc: 0.83054: 100%|██████████| 1/1 [01:55<00:00, 115.25s/it]\n",
    "OOF-2| Epoch:  17| Train loss: 0.42407| roc_auc: 0.83171: 100%|██████████| 1/1 [01:53<00:00, 113.34s/it]\n",
    "OOF-2| Epoch:  18| Train loss: 0.42251| roc_auc: 0.82987: 100%|██████████| 1/1 [01:56<00:00, 116.50s/it]\n",
    "OOF-2| Epoch:  19| Train loss: 0.42215| roc_auc: 0.83093: 100%|██████████| 1/1 [01:54<00:00, 114.01s/it]\n",
    "OOF-2| Epoch:  20| Train loss: 0.42035| roc_auc: 0.82769: 100%|██████████| 1/1 [01:55<00:00, 115.53s/it]\n",
    "BEST OOF-2| Epoch:  17| Train loss: 0.42407| roc_auc: 0.83171\n",
    "OOF-3| Epoch:   1| Train loss: 0.50986| roc_auc: 0.79456: 100%|██████████| 1/1 [01:56<00:00, 116.35s/it]\n",
    "OOF-3| Epoch:   2| Train loss: 0.46829| roc_auc: 0.81776: 100%|██████████| 1/1 [01:58<00:00, 118.48s/it]\n",
    "OOF-3| Epoch:   3| Train loss: 0.45429| roc_auc: 0.82726: 100%|██████████| 1/1 [01:55<00:00, 115.56s/it]\n",
    "OOF-3| Epoch:   4| Train loss: 0.44735| roc_auc: 0.82892: 100%|██████████| 1/1 [01:55<00:00, 115.90s/it]\n",
    "OOF-3| Epoch:   5| Train loss: 0.44347| roc_auc: 0.82818: 100%|██████████| 1/1 [01:58<00:00, 118.92s/it]\n",
    "OOF-3| Epoch:   6| Train loss: 0.44045| roc_auc: 0.83282: 100%|██████████| 1/1 [01:58<00:00, 118.88s/it]\n",
    "OOF-3| Epoch:   7| Train loss: 0.43825| roc_auc: 0.83213: 100%|██████████| 1/1 [01:56<00:00, 116.99s/it]\n",
    "OOF-3| Epoch:   8| Train loss: 0.43650| roc_auc: 0.83542: 100%|██████████| 1/1 [01:58<00:00, 118.83s/it]\n",
    "OOF-3| Epoch:   9| Train loss: 0.43390| roc_auc: 0.83386: 100%|██████████| 1/1 [01:57<00:00, 117.81s/it]\n",
    "OOF-3| Epoch:  10| Train loss: 0.43257| roc_auc: 0.83359: 100%|██████████| 1/1 [01:56<00:00, 116.30s/it]\n",
    "OOF-3| Epoch:  11| Train loss: 0.43083| roc_auc: 0.83521: 100%|██████████| 1/1 [01:56<00:00, 116.46s/it]\n",
    "OOF-3| Epoch:  12| Train loss: 0.42935| roc_auc: 0.83358: 100%|██████████| 1/1 [01:57<00:00, 117.09s/it]\n",
    "OOF-3| Epoch:  13| Train loss: 0.42800| roc_auc: 0.83720: 100%|██████████| 1/1 [01:57<00:00, 117.71s/it]\n",
    "OOF-3| Epoch:  14| Train loss: 0.42677| roc_auc: 0.83551: 100%|██████████| 1/1 [01:57<00:00, 117.13s/it]\n",
    "OOF-3| Epoch:  15| Train loss: 0.42529| roc_auc: 0.83762: 100%|██████████| 1/1 [01:57<00:00, 117.83s/it]\n",
    "OOF-3| Epoch:  16| Train loss: 0.42435| roc_auc: 0.83665: 100%|██████████| 1/1 [01:59<00:00, 119.27s/it]\n",
    "OOF-3| Epoch:  17| Train loss: 0.42324| roc_auc: 0.83612: 100%|██████████| 1/1 [01:57<00:00, 117.74s/it]\n",
    "OOF-3| Epoch:  18| Train loss: 0.42194| roc_auc: 0.83714: 100%|██████████| 1/1 [01:56<00:00, 116.28s/it]\n",
    "OOF-3| Epoch:  19| Train loss: 0.42063| roc_auc: 0.83718: 100%|██████████| 1/1 [01:57<00:00, 117.85s/it]\n",
    "OOF-3| Epoch:  20| Train loss: 0.41991| roc_auc: 0.83624: 100%|██████████| 1/1 [02:03<00:00, 123.29s/it]\n",
    "BEST OOF-3| Epoch:  15| Train loss: 0.42529| roc_auc: 0.83762\n",
    "OOF-4| Epoch:   1| Train loss: 0.50803| roc_auc: 0.80679: 100%|██████████| 1/1 [02:02<00:00, 122.05s/it]\n",
    "OOF-4| Epoch:   2| Train loss: 0.46878| roc_auc: 0.83093: 100%|██████████| 1/1 [01:57<00:00, 117.42s/it]\n",
    "OOF-4| Epoch:   3| Train loss: 0.45452| roc_auc: 0.83637: 100%|██████████| 1/1 [01:58<00:00, 118.88s/it]\n",
    "OOF-4| Epoch:   4| Train loss: 0.44752| roc_auc: 0.83825: 100%|██████████| 1/1 [02:02<00:00, 122.77s/it]\n",
    "OOF-4| Epoch:   5| Train loss: 0.44336| roc_auc: 0.84125: 100%|██████████| 1/1 [02:02<00:00, 122.48s/it]\n",
    "OOF-4| Epoch:   6| Train loss: 0.44035| roc_auc: 0.84286: 100%|██████████| 1/1 [02:01<00:00, 121.50s/it]\n",
    "OOF-4| Epoch:   7| Train loss: 0.43780| roc_auc: 0.84547: 100%|██████████| 1/1 [02:03<00:00, 123.44s/it]\n",
    "OOF-4| Epoch:   8| Train loss: 0.43600| roc_auc: 0.84276: 100%|██████████| 1/1 [02:03<00:00, 123.28s/it]\n",
    "OOF-4| Epoch:   9| Train loss: 0.43430| roc_auc: 0.84459: 100%|██████████| 1/1 [02:03<00:00, 123.39s/it]\n",
    "OOF-4| Epoch:  10| Train loss: 0.43291| roc_auc: 0.84620: 100%|██████████| 1/1 [02:02<00:00, 122.22s/it]\n",
    "OOF-4| Epoch:  11| Train loss: 0.43139| roc_auc: 0.84427: 100%|██████████| 1/1 [02:03<00:00, 123.91s/it]\n",
    "OOF-4| Epoch:  12| Train loss: 0.42975| roc_auc: 0.84544: 100%|██████████| 1/1 [02:00<00:00, 120.78s/it]\n",
    "OOF-4| Epoch:  13| Train loss: 0.42908| roc_auc: 0.84650: 100%|██████████| 1/1 [02:00<00:00, 120.85s/it]\n",
    "OOF-4| Epoch:  14| Train loss: 0.42732| roc_auc: 0.84850: 100%|██████████| 1/1 [02:01<00:00, 121.80s/it]\n",
    "OOF-4| Epoch:  15| Train loss: 0.42605| roc_auc: 0.84544: 100%|██████████| 1/1 [02:02<00:00, 122.84s/it]\n",
    "OOF-4| Epoch:  16| Train loss: 0.42498| roc_auc: 0.84496: 100%|██████████| 1/1 [02:01<00:00, 121.84s/it]\n",
    "OOF-4| Epoch:  17| Train loss: 0.42330| roc_auc: 0.84284: 100%|██████████| 1/1 [02:03<00:00, 123.95s/it]\n",
    "OOF-4| Epoch:  18| Train loss: 0.42218| roc_auc: 0.84292: 100%|██████████| 1/1 [02:01<00:00, 121.40s/it]\n",
    "OOF-4| Epoch:  19| Train loss: 0.42094| roc_auc: 0.84504: 100%|██████████| 1/1 [02:02<00:00, 122.80s/it]\n",
    "OOF-4| Epoch:  20| Train loss: 0.42022| roc_auc: 0.84301: 100%|██████████| 1/1 [02:03<00:00, 123.39s/it]\n",
    "BEST OOF-4| Epoch:  14| Train loss: 0.42732| roc_auc: 0.84850\n",
    "Total roc_auc: 0.84769\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "num-head-2\n",
    "\n",
    "OOF-0| Epoch:   1| Train loss: 0.50858| roc_auc: 0.82706: 100%|██████████| 1/1 [02:03<00:00, 123.25s/it]\n",
    "OOF-0| Epoch:   2| Train loss: 0.46791| roc_auc: 0.84448: 100%|██████████| 1/1 [02:02<00:00, 122.69s/it]\n",
    "OOF-0| Epoch:   3| Train loss: 0.45332| roc_auc: 0.84909: 100%|██████████| 1/1 [02:03<00:00, 123.32s/it]\n",
    "OOF-0| Epoch:   4| Train loss: 0.44674| roc_auc: 0.85422: 100%|██████████| 1/1 [02:02<00:00, 122.90s/it]\n",
    "OOF-0| Epoch:   5| Train loss: 0.44278| roc_auc: 0.85371: 100%|██████████| 1/1 [02:02<00:00, 122.54s/it]\n",
    "OOF-0| Epoch:   6| Train loss: 0.44034| roc_auc: 0.85612: 100%|██████████| 1/1 [02:01<00:00, 121.98s/it]\n",
    "OOF-0| Epoch:   7| Train loss: 0.43729| roc_auc: 0.85691: 100%|██████████| 1/1 [02:02<00:00, 122.18s/it]\n",
    "OOF-0| Epoch:   8| Train loss: 0.43586| roc_auc: 0.85380: 100%|██████████| 1/1 [02:03<00:00, 123.93s/it]\n",
    "OOF-0| Epoch:   9| Train loss: 0.43391| roc_auc: 0.85510: 100%|██████████| 1/1 [02:05<00:00, 125.92s/it]\n",
    "OOF-0| Epoch:  10| Train loss: 0.43245| roc_auc: 0.85718: 100%|██████████| 1/1 [02:02<00:00, 122.81s/it]\n",
    "OOF-0| Epoch:  11| Train loss: 0.43095| roc_auc: 0.85469: 100%|██████████| 1/1 [02:01<00:00, 121.42s/it]\n",
    "OOF-0| Epoch:  12| Train loss: 0.42998| roc_auc: 0.85344: 100%|██████████| 1/1 [02:03<00:00, 123.49s/it]\n",
    "OOF-0| Epoch:  13| Train loss: 0.42843| roc_auc: 0.85439: 100%|██████████| 1/1 [02:00<00:00, 120.31s/it]\n",
    "OOF-0| Epoch:  14| Train loss: 0.42698| roc_auc: 0.85367: 100%|██████████| 1/1 [02:00<00:00, 120.31s/it]\n",
    "OOF-0| Epoch:  15| Train loss: 0.42620| roc_auc: 0.85526: 100%|██████████| 1/1 [02:01<00:00, 121.08s/it]\n",
    "OOF-0| Epoch:  16| Train loss: 0.42500| roc_auc: 0.85509: 100%|██████████| 1/1 [01:58<00:00, 118.33s/it]\n",
    "OOF-0| Epoch:  17| Train loss: 0.42390| roc_auc: 0.85435: 100%|██████████| 1/1 [01:59<00:00, 119.80s/it]\n",
    "OOF-0| Epoch:  18| Train loss: 0.42260| roc_auc: 0.85455: 100%|██████████| 1/1 [01:57<00:00, 117.17s/it]\n",
    "OOF-0| Epoch:  19| Train loss: 0.42165| roc_auc: 0.85624: 100%|██████████| 1/1 [01:56<00:00, 116.83s/it]\n",
    "OOF-0| Epoch:  20| Train loss: 0.42004| roc_auc: 0.85612: 100%|██████████| 1/1 [01:56<00:00, 116.26s/it]\n",
    "BEST OOF-0| Epoch:  10| Train loss: 0.43245| roc_auc: 0.85718\n",
    "\n",
    "OOF-1| Epoch:   1| Train loss: 0.51246| roc_auc: 0.81910: 100%|██████████| 1/1 [02:01<00:00, 121.98s/it]\n",
    "OOF-1| Epoch:   2| Train loss: 0.46823| roc_auc: 0.84305: 100%|██████████| 1/1 [02:02<00:00, 122.80s/it]\n",
    "OOF-1| Epoch:   3| Train loss: 0.45340| roc_auc: 0.84986: 100%|██████████| 1/1 [02:01<00:00, 121.49s/it]\n",
    "OOF-1| Epoch:   4| Train loss: 0.44746| roc_auc: 0.85024: 100%|██████████| 1/1 [01:59<00:00, 119.15s/it]\n",
    "OOF-1| Epoch:   5| Train loss: 0.44332| roc_auc: 0.85493: 100%|██████████| 1/1 [01:57<00:00, 117.11s/it]\n",
    "OOF-1| Epoch:   6| Train loss: 0.44067| roc_auc: 0.85214: 100%|██████████| 1/1 [01:57<00:00, 117.35s/it]\n",
    "OOF-1| Epoch:   7| Train loss: 0.43816| roc_auc: 0.85739: 100%|██████████| 1/1 [01:55<00:00, 115.40s/it]\n",
    "OOF-1| Epoch:   8| Train loss: 0.43633| roc_auc: 0.85651: 100%|██████████| 1/1 [02:02<00:00, 122.66s/it]\n",
    "OOF-1| Epoch:   9| Train loss: 0.43419| roc_auc: 0.85588: 100%|██████████| 1/1 [01:59<00:00, 119.82s/it]\n",
    "OOF-1| Epoch:  10| Train loss: 0.43259| roc_auc: 0.85834: 100%|██████████| 1/1 [01:58<00:00, 118.95s/it]\n",
    "OOF-1| Epoch:  11| Train loss: 0.43123| roc_auc: 0.85700: 100%|██████████| 1/1 [01:58<00:00, 118.09s/it]\n",
    "OOF-1| Epoch:  12| Train loss: 0.43013| roc_auc: 0.85617: 100%|██████████| 1/1 [02:05<00:00, 125.98s/it]\n",
    "OOF-1| Epoch:  13| Train loss: 0.42880| roc_auc: 0.85912: 100%|██████████| 1/1 [01:59<00:00, 119.05s/it]\n",
    "OOF-1| Epoch:  14| Train loss: 0.42790| roc_auc: 0.85804: 100%|██████████| 1/1 [01:58<00:00, 118.77s/it]\n",
    "OOF-1| Epoch:  15| Train loss: 0.42638| roc_auc: 0.85811: 100%|██████████| 1/1 [02:01<00:00, 121.06s/it]\n",
    "OOF-1| Epoch:  16| Train loss: 0.42547| roc_auc: 0.85898: 100%|██████████| 1/1 [01:59<00:00, 119.59s/it]\n",
    "OOF-1| Epoch:  17| Train loss: 0.42433| roc_auc: 0.85918: 100%|██████████| 1/1 [02:01<00:00, 121.23s/it]\n",
    "OOF-1| Epoch:  18| Train loss: 0.42331| roc_auc: 0.85816: 100%|██████████| 1/1 [01:58<00:00, 118.67s/it]\n",
    "OOF-1| Epoch:  19| Train loss: 0.42187| roc_auc: 0.85730: 100%|██████████| 1/1 [02:02<00:00, 122.19s/it]\n",
    "OOF-1| Epoch:  20| Train loss: 0.42127| roc_auc: 0.85648: 100%|██████████| 1/1 [02:00<00:00, 120.38s/it]\n",
    "BEST OOF-1| Epoch:  17| Train loss: 0.42433| roc_auc: 0.85918\n",
    "\n",
    "OOF-2| Epoch:   1| Train loss: 0.51142| roc_auc: 0.79601: 100%|██████████| 1/1 [01:58<00:00, 118.50s/it]\n",
    "OOF-2| Epoch:   2| Train loss: 0.46885| roc_auc: 0.81396: 100%|██████████| 1/1 [01:59<00:00, 119.01s/it]\n",
    "OOF-2| Epoch:   3| Train loss: 0.45433| roc_auc: 0.82116: 100%|██████████| 1/1 [01:58<00:00, 118.16s/it]\n",
    "OOF-2| Epoch:   4| Train loss: 0.44758| roc_auc: 0.82253: 100%|██████████| 1/1 [01:55<00:00, 115.09s/it]\n",
    "OOF-2| Epoch:   5| Train loss: 0.44414| roc_auc: 0.82432: 100%|██████████| 1/1 [01:58<00:00, 118.77s/it]\n",
    "OOF-2| Epoch:   6| Train loss: 0.44046| roc_auc: 0.82371: 100%|██████████| 1/1 [02:00<00:00, 120.85s/it]\n",
    "OOF-2| Epoch:   7| Train loss: 0.43899| roc_auc: 0.82584: 100%|██████████| 1/1 [01:56<00:00, 116.90s/it]\n",
    "OOF-2| Epoch:   8| Train loss: 0.43688| roc_auc: 0.82684: 100%|██████████| 1/1 [01:57<00:00, 117.17s/it]\n",
    "OOF-2| Epoch:   9| Train loss: 0.43528| roc_auc: 0.82799: 100%|██████████| 1/1 [01:58<00:00, 118.52s/it]\n",
    "OOF-2| Epoch:  10| Train loss: 0.43384| roc_auc: 0.82661: 100%|██████████| 1/1 [02:00<00:00, 120.03s/it]\n",
    "OOF-2| Epoch:  11| Train loss: 0.43257| roc_auc: 0.83009: 100%|██████████| 1/1 [01:57<00:00, 117.98s/it]\n",
    "OOF-2| Epoch:  12| Train loss: 0.43140| roc_auc: 0.82843: 100%|██████████| 1/1 [01:59<00:00, 119.56s/it]\n",
    "OOF-2| Epoch:  13| Train loss: 0.43006| roc_auc: 0.82755: 100%|██████████| 1/1 [01:56<00:00, 116.50s/it]\n",
    "OOF-2| Epoch:  14| Train loss: 0.42855| roc_auc: 0.83049: 100%|██████████| 1/1 [01:57<00:00, 117.12s/it]\n",
    "OOF-2| Epoch:  15| Train loss: 0.42716| roc_auc: 0.82899: 100%|██████████| 1/1 [01:56<00:00, 116.98s/it]\n",
    "OOF-2| Epoch:  16| Train loss: 0.42623| roc_auc: 0.82861: 100%|██████████| 1/1 [01:58<00:00, 118.81s/it]\n",
    "OOF-2| Epoch:  17| Train loss: 0.42522| roc_auc: 0.82610: 100%|██████████| 1/1 [01:53<00:00, 113.30s/it]\n",
    "OOF-2| Epoch:  18| Train loss: 0.42364| roc_auc: 0.82639: 100%|██████████| 1/1 [01:55<00:00, 115.48s/it]\n",
    "OOF-2| Epoch:  19| Train loss: 0.42318| roc_auc: 0.82957: 100%|██████████| 1/1 [01:56<00:00, 116.82s/it]\n",
    "OOF-2| Epoch:  20| Train loss: 0.42144| roc_auc: 0.82631: 100%|██████████| 1/1 [01:59<00:00, 119.69s/it]\n",
    "BEST OOF-2| Epoch:  14| Train loss: 0.42855| roc_auc: 0.83049\n",
    "\n",
    "OOF-3| Epoch:   1| Train loss: 0.50971| roc_auc: 0.79570: 100%|██████████| 1/1 [02:02<00:00, 122.54s/it]\n",
    "OOF-3| Epoch:   2| Train loss: 0.46781| roc_auc: 0.82055: 100%|██████████| 1/1 [02:04<00:00, 124.66s/it]\n",
    "OOF-3| Epoch:   3| Train loss: 0.45411| roc_auc: 0.82631: 100%|██████████| 1/1 [02:02<00:00, 122.37s/it]\n",
    "OOF-3| Epoch:   4| Train loss: 0.44730| roc_auc: 0.82849: 100%|██████████| 1/1 [02:02<00:00, 122.98s/it]\n",
    "OOF-3| Epoch:   5| Train loss: 0.44350| roc_auc: 0.83021: 100%|██████████| 1/1 [02:03<00:00, 123.41s/it]\n",
    "OOF-3| Epoch:   6| Train loss: 0.44060| roc_auc: 0.83509: 100%|██████████| 1/1 [02:01<00:00, 121.74s/it]\n",
    "OOF-3| Epoch:   7| Train loss: 0.43862| roc_auc: 0.83220: 100%|██████████| 1/1 [02:01<00:00, 121.48s/it]\n",
    "OOF-3| Epoch:   8| Train loss: 0.43681| roc_auc: 0.83439: 100%|██████████| 1/1 [02:02<00:00, 122.34s/it]\n",
    "OOF-3| Epoch:   9| Train loss: 0.43458| roc_auc: 0.83282: 100%|██████████| 1/1 [02:02<00:00, 122.76s/it]\n",
    "OOF-3| Epoch:  10| Train loss: 0.43325| roc_auc: 0.83455: 100%|██████████| 1/1 [02:03<00:00, 123.84s/it]\n",
    "OOF-3| Epoch:  11| Train loss: 0.43151| roc_auc: 0.83559: 100%|██████████| 1/1 [02:02<00:00, 122.65s/it]\n",
    "OOF-3| Epoch:  12| Train loss: 0.43006| roc_auc: 0.83558: 100%|██████████| 1/1 [02:02<00:00, 122.41s/it]\n",
    "OOF-3| Epoch:  13| Train loss: 0.42861| roc_auc: 0.83806: 100%|██████████| 1/1 [01:59<00:00, 119.82s/it]\n",
    "OOF-3| Epoch:  14| Train loss: 0.42748| roc_auc: 0.83684: 100%|██████████| 1/1 [02:01<00:00, 121.69s/it]\n",
    "OOF-3| Epoch:  15| Train loss: 0.42606| roc_auc: 0.83885: 100%|██████████| 1/1 [02:01<00:00, 121.51s/it]\n",
    "OOF-3| Epoch:  16| Train loss: 0.42530| roc_auc: 0.83961: 100%|██████████| 1/1 [02:02<00:00, 122.61s/it]\n",
    "OOF-3| Epoch:  17| Train loss: 0.42400| roc_auc: 0.83737: 100%|██████████| 1/1 [02:00<00:00, 120.90s/it]\n",
    "OOF-3| Epoch:  18| Train loss: 0.42272| roc_auc: 0.83888: 100%|██████████| 1/1 [02:01<00:00, 121.95s/it]\n",
    "OOF-3| Epoch:  19| Train loss: 0.42148| roc_auc: 0.83861: 100%|██████████| 1/1 [02:01<00:00, 121.19s/it]\n",
    "OOF-3| Epoch:  20| Train loss: 0.42069| roc_auc: 0.83983: 100%|██████████| 1/1 [02:01<00:00, 121.66s/it]\n",
    "BEST OOF-3| Epoch:  20| Train loss: 0.42069| roc_auc: 0.83983\n",
    "\n",
    "OOF-4| Epoch:   1| Train loss: 0.50790| roc_auc: 0.80640: 100%|██████████| 1/1 [02:01<00:00, 121.78s/it]\n",
    "OOF-4| Epoch:   2| Train loss: 0.46806| roc_auc: 0.82975: 100%|██████████| 1/1 [02:01<00:00, 121.26s/it]\n",
    "OOF-4| Epoch:   3| Train loss: 0.45404| roc_auc: 0.83418: 100%|██████████| 1/1 [02:01<00:00, 121.38s/it]\n",
    "OOF-4| Epoch:   4| Train loss: 0.44747| roc_auc: 0.83818: 100%|██████████| 1/1 [02:02<00:00, 122.35s/it]\n",
    "OOF-4| Epoch:   5| Train loss: 0.44355| roc_auc: 0.83937: 100%|██████████| 1/1 [01:59<00:00, 119.69s/it]\n",
    "OOF-4| Epoch:   6| Train loss: 0.44079| roc_auc: 0.84145: 100%|██████████| 1/1 [02:00<00:00, 120.66s/it]\n",
    "OOF-4| Epoch:   7| Train loss: 0.43819| roc_auc: 0.84354: 100%|██████████| 1/1 [02:04<00:00, 124.13s/it]\n",
    "OOF-4| Epoch:   8| Train loss: 0.43658| roc_auc: 0.84237: 100%|██████████| 1/1 [02:01<00:00, 121.35s/it]\n",
    "OOF-4| Epoch:   9| Train loss: 0.43476| roc_auc: 0.84456: 100%|██████████| 1/1 [02:00<00:00, 120.28s/it]\n",
    "OOF-4| Epoch:  10| Train loss: 0.43347| roc_auc: 0.84327: 100%|██████████| 1/1 [02:02<00:00, 122.39s/it]\n",
    "OOF-4| Epoch:  11| Train loss: 0.43257| roc_auc: 0.84308: 100%|██████████| 1/1 [02:00<00:00, 120.90s/it]\n",
    "OOF-4| Epoch:  12| Train loss: 0.43087| roc_auc: 0.84594: 100%|██████████| 1/1 [01:59<00:00, 119.75s/it]\n",
    "OOF-4| Epoch:  13| Train loss: 0.43005| roc_auc: 0.84592: 100%|██████████| 1/1 [02:01<00:00, 121.57s/it]\n",
    "OOF-4| Epoch:  14| Train loss: 0.42850| roc_auc: 0.84861: 100%|██████████| 1/1 [02:00<00:00, 120.06s/it]\n",
    "OOF-4| Epoch:  15| Train loss: 0.42722| roc_auc: 0.84901: 100%|██████████| 1/1 [02:00<00:00, 120.92s/it]\n",
    "OOF-4| Epoch:  16| Train loss: 0.42609| roc_auc: 0.84590: 100%|██████████| 1/1 [02:00<00:00, 120.62s/it]\n",
    "OOF-4| Epoch:  17| Train loss: 0.42477| roc_auc: 0.84602: 100%|██████████| 1/1 [02:02<00:00, 122.59s/it]\n",
    "OOF-4| Epoch:  18| Train loss: 0.42326| roc_auc: 0.84428: 100%|██████████| 1/1 [02:00<00:00, 120.75s/it]\n",
    "OOF-4| Epoch:  19| Train loss: 0.42234| roc_auc: 0.84595: 100%|██████████| 1/1 [02:01<00:00, 121.15s/it]\n",
    "OOF-4| Epoch:  20| Train loss: 0.42147| roc_auc: 0.84370: 100%|██████████| 1/1 [01:59<00:00, 119.21s/it]\n",
    "BEST OOF-4| Epoch:  15| Train loss: 0.42722| roc_auc: 0.84901\n",
    "\n",
    "Total roc_auc: 0.84714\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = make_dataset.get_test_data()\n",
    "test_dataset = CustomDataset(df = test_df,)\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 1, \n",
    "    shuffle = False, \n",
    "    drop_last = False,\n",
    "    collate_fn = train_make_batch,\n",
    "    num_workers = num_workers)\n",
    "\n",
    "pred_list = []\n",
    "\n",
    "model = SASRec(\n",
    "    num_assessmentItemID = make_dataset.num_assessmentItemID, \n",
    "    num_testId = make_dataset.num_testId,\n",
    "    num_KnowledgeTag = make_dataset.num_KnowledgeTag,\n",
    "    num_large_paper_number = make_dataset.num_large_paper_number,\n",
    "    num_cols = train_dataset.num_cols,\n",
    "    cat_cols = train_dataset.cat_cols,\n",
    "    emb_size = emb_size, \n",
    "    hidden_units = hidden_units, \n",
    "    num_heads = num_heads, \n",
    "    num_layers = num_layers, \n",
    "    dropout_rate = dropout_rate, \n",
    "    device = device).to(device)\n",
    "\n",
    "for oof in make_dataset.oof_user_set.keys():\n",
    "    model.load_state_dict(torch.load(os.path.join(MODEL_PATH, f'oof_{oof}_' + model_name)))\n",
    "    pred = predict(model = model, data_loader = test_data_loader)\n",
    "    pred_list.append(pred)\n",
    "\n",
    "pred_list = np.array(pred_list).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data = np.array(pred_list), columns = ['prediction'])\n",
    "submission['id'] = submission.index\n",
    "submission = submission[['id', 'prediction']]\n",
    "submission.to_csv(os.path.join(SUBMISSION_PATH, 'OOF-Ensemble-' + submission_name), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
